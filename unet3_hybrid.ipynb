{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-03T19:16:05.104908Z","iopub.status.busy":"2023-04-03T19:16:05.104391Z","iopub.status.idle":"2023-04-03T19:16:46.010334Z","shell.execute_reply":"2023-04-03T19:16:46.009132Z","shell.execute_reply.started":"2023-04-03T19:16:05.104878Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/davej23/image-segmentation-keras.git\n","  Cloning https://github.com/davej23/image-segmentation-keras.git to /tmp/pip-req-build-s9h1a0s0\n","  Running command git clone --filter=blob:none --quiet https://github.com/davej23/image-segmentation-keras.git /tmp/pip-req-build-s9h1a0s0\n","  Resolved https://github.com/davej23/image-segmentation-keras.git to commit e01b0a8d5859854cd9d259a618829889166439f5\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting rarfile\n","  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n","Collecting segmentation-models\n","  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n","Collecting rioxarray\n","  Downloading rioxarray-0.9.1.tar.gz (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting image-classifiers==1.0.0\n","  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n","Collecting keras-applications<=1.0.8,>=1.0.7\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting efficientnet==1.0.0\n","  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n","Collecting h5py<=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: Keras>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (2.11.0)\n","Collecting imageio==2.5.0\n","  Downloading imageio-2.5.0-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (0.4.0)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.5.4.60)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.64.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (1.21.6)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (9.4.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from rioxarray) (23.0)\n","Requirement already satisfied: pyproj>=2.2 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (3.1.0)\n","Requirement already satisfied: rasterio in /opt/conda/lib/python3.7/site-packages (from rioxarray) (1.2.10)\n","Requirement already satisfied: xarray>=0.17 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (0.20.2)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py<=2.10.0->keras-segmentation==0.3.0) (1.16.0)\n","Requirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.8.0)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (3.5.3)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.7.3)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from pyproj>=2.2->rioxarray) (2022.12.7)\n","Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.4.0)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.11.4)\n","Requirement already satisfied: pandas>=1.1 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (1.3.5)\n","Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (22.2.0)\n","Requirement already satisfied: affine in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (2.4.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (59.8.0)\n","Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (8.1.3)\n","Requirement already satisfied: click-plugins in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.1.1)\n","Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (0.7.2)\n","Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.4.7)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2022.7.1)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.11.2)\n","Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n","Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio->rioxarray) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->xarray>=0.17->rioxarray) (3.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (1.4.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (4.38.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (0.11.0)\n","Building wheels for collected packages: keras-segmentation, rioxarray\n","  Building wheel for keras-segmentation (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for keras-segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34377 sha256=0c71edab1e9a5a755cc682590efac1015947d456af0f12f7141a2bfbaafe2c20\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-t62bfolr/wheels/f4/fb/07/8f81ceb3d9fe936f5e4dcd1a64cbc489e42e6e7f9c2f166785\n","  Building wheel for rioxarray (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rioxarray: filename=rioxarray-0.9.1-py3-none-any.whl size=54590 sha256=64d9b3230ae89d16bc3cd05bd0c7ba1e3b29712774c4bea2c4eff0e24aa6413b\n","  Stored in directory: /root/.cache/pip/wheels/03/b2/26/2e2cc1797ac99cc070d2cae87c340bd3429bbb583c90b1c780\n","Successfully built keras-segmentation rioxarray\n","Installing collected packages: rarfile, imageio, h5py, keras-applications, image-classifiers, efficientnet, segmentation-models, keras-segmentation, rioxarray\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.25.0\n","    Uninstalling imageio-2.25.0:\n","      Successfully uninstalled imageio-2.25.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n","tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed efficientnet-1.0.0 h5py-2.10.0 image-classifiers-1.0.0 imageio-2.5.0 keras-applications-1.0.8 keras-segmentation-0.3.0 rarfile-4.0 rioxarray-0.9.1 segmentation-models-1.0.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#@title import packages\n","import keras\n","import numpy as np\n","import os\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.losses import *\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint\n","import sys\n","\n","!pip install rarfile segmentation-models git+https://github.com/davej23/image-segmentation-keras.git rioxarray\n","from rarfile import RarFile\n","from sklearn.metrics import *\n","import rioxarray as rxr"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T19:16:46.018374Z","iopub.status.busy":"2023-04-03T19:16:46.015931Z","iopub.status.idle":"2023-04-03T19:18:21.912626Z","shell.execute_reply":"2023-04-03T19:18:21.911548Z","shell.execute_reply.started":"2023-04-03T19:16:46.018334Z"},"trusted":true},"outputs":[],"source":["base_dir = r\"/content/gdrive/MyDrive/mudtrain/\"\n","#@title Read training images and normalise\n","training_images_list = os.listdir(r\"{}train/images/\".format(base_dir))\n","training_masks_list = []\n","training_images = []\n","for n in training_images_list:\n","  training_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}train/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  training_images.append(a)\n","\n","## Training masks\n","training_masks = []\n","for n in training_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}train/labels/{}\".format(base_dir,n))))\n","  training_masks.append(a)\n","\n","\n","## Validation images\n","validation_images_list = os.listdir(r\"{}val/images/\".format(base_dir))\n","validation_masks_list = []\n","validation_images = []\n","for n in validation_images_list:\n","  validation_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}val/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  validation_images.append(a)\n","\n","## Validation masks\n","validation_masks = []\n","for n in validation_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}val/labels/{}\".format(base_dir,n))))\n","  validation_masks.append(a)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T19:18:37.483808Z","iopub.status.busy":"2023-04-03T19:18:37.483047Z","iopub.status.idle":"2023-04-03T19:18:38.093890Z","shell.execute_reply":"2023-04-03T19:18:38.092819Z","shell.execute_reply.started":"2023-04-03T19:18:37.483769Z"},"trusted":true},"outputs":[],"source":["#@title Pre-process data, reshaping and transposing\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].astype('float32')\n","  training_images[i] = training_images[i].T\n","\n","for i in range(len(training_masks)):\n","  training_masks[i] = training_masks[i].reshape(1,256,256)\n","  training_masks[i] = training_masks[i].T\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].astype('float32')\n","  validation_images[i] = validation_images[i].T\n","\n","for i in range(len(validation_masks)):\n","  validation_masks[i] = validation_masks[i].reshape(1,256,256)\n","  validation_masks[i] = validation_masks[i].T\n","\n","\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].reshape(256,256,10)\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].reshape(256,256,10)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T19:18:41.843752Z","iopub.status.busy":"2023-04-03T19:18:41.843141Z","iopub.status.idle":"2023-04-03T19:18:46.722858Z","shell.execute_reply":"2023-04-03T19:18:46.721606Z","shell.execute_reply.started":"2023-04-03T19:18:41.843713Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 10)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["images=np.vstack([training_images])\n","val_images=np.vstack([validation_images])\n","images.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T19:18:50.542010Z","iopub.status.busy":"2023-04-03T19:18:50.541097Z","iopub.status.idle":"2023-04-03T19:18:50.698490Z","shell.execute_reply":"2023-04-03T19:18:50.697352Z","shell.execute_reply.started":"2023-04-03T19:18:50.541957Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 1)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["masks=np.vstack([training_masks])\n","val_masks=np.vstack([validation_masks])\n","val_masks.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T19:18:55.783485Z","iopub.status.busy":"2023-04-03T19:18:55.782976Z","iopub.status.idle":"2023-04-03T19:18:56.097635Z","shell.execute_reply":"2023-04-03T19:18:56.096395Z","shell.execute_reply.started":"2023-04-03T19:18:55.783446Z"},"trusted":true},"outputs":[{"data":{"text/plain":["889"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del training_images,validation_images,training_masks,validation_masks,\n","training_images_list,validation_images_list,\n","training_masks_list,validation_masks_list\n","gc.collect()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T00:37:30.009389Z","iopub.status.busy":"2023-04-04T00:37:30.008760Z","iopub.status.idle":"2023-04-04T00:37:30.023812Z","shell.execute_reply":"2023-04-04T00:37:30.022551Z","shell.execute_reply.started":"2023-04-04T00:37:30.009339Z"},"trusted":true},"outputs":[],"source":["del images,masks,val_images,val_masks"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T19:19:00.071069Z","iopub.status.busy":"2023-04-03T19:19:00.070189Z","iopub.status.idle":"2023-04-03T19:19:00.081791Z","shell.execute_reply":"2023-04-03T19:19:00.080734Z","shell.execute_reply.started":"2023-04-03T19:19:00.071033Z"},"trusted":true},"outputs":[],"source":["#@title boundary loss\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import models\n","\n","#Shape of semantic segmentation mask\n","OUTPUT_SHAPE = (256, 256, 1)\n","def boundary_loss(y_true, y_pred):\n","\n","    \"\"\"\n","    Paper Implemented : https://arxiv.org/abs/1905.07852\n","    Using Binary Segmentation mask, generates boundary mask on fly and claculates boundary loss.\n","    :param y_true:\n","    :param y_pred:\n","    :return:\n","    \"\"\"\n","    y_true=tf.cast(y_true,tf.float32)\n","    y_pred=tf.cast(y_pred,tf.float32)\n","    \n","    y_pred_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd = y_pred_bd - (1 - y_pred)\n","    y_true_bd = y_true_bd - (1 - y_true)\n","\n","    y_pred_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd_ext = y_pred_bd_ext - (1 - y_pred)\n","    y_true_bd_ext = y_true_bd_ext - (1 - y_true)\n","\n","    P = K.sum(y_pred_bd * y_true_bd_ext) / K.sum(y_pred_bd) + 1e-7\n","    R = K.sum(y_true_bd * y_pred_bd_ext) / K.sum(y_true_bd) + 1e-7\n","    F1_Score = 2 * P * R / (P + R + 1e-7)\n","    loss = K.mean(1 - F1_Score)\n","    \n","    return loss"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T19:19:03.741343Z","iopub.status.busy":"2023-04-03T19:19:03.740723Z","iopub.status.idle":"2023-04-03T19:19:06.065724Z","shell.execute_reply":"2023-04-03T19:19:06.064654Z","shell.execute_reply.started":"2023-04-03T19:19:03.741279Z"},"trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, Callback\n","class AlphaScheduler(Callback):\n","  def init(self, alpha, update_fn):\n","    self.alpha = alpha\n","    self.update_fn = update_fn\n","  def on_epoch_end(self, epoch, logs=None):\n","    updated_alpha = self.update_fn(K.get_value(self.alpha))\n","\n","alpha = K.variable(1, dtype='float32')\n","\n","def update_alpha(value):\n","  return np.clip(value - 0.005, 0.005, 1)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T19:19:15.874876Z","iopub.status.busy":"2023-04-03T19:19:15.873847Z","iopub.status.idle":"2023-04-03T19:19:15.881571Z","shell.execute_reply":"2023-04-03T19:19:15.880409Z","shell.execute_reply.started":"2023-04-03T19:19:15.874834Z"},"trusted":true},"outputs":[],"source":["def gl_sl_wrapper(alpha):\n","    def gl_sl(y_true, y_pred):\n","        return alpha*keras.losses.binary_crossentropy(y_true, y_pred) +  (1-alpha)* boundary_loss(y_true, y_pred)\n","    return gl_sl"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T19:19:18.702832Z","iopub.status.busy":"2023-04-03T19:19:18.702283Z","iopub.status.idle":"2023-04-03T19:19:19.125348Z","shell.execute_reply":"2023-04-03T19:19:19.124359Z","shell.execute_reply.started":"2023-04-03T19:19:18.702789Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import models, layers, regularizers\n","from tensorflow.keras import backend as K\n","\n","#convolutional block\n","\n","def conv_block(x,filters, kernelsize=3, dropout=0, batchnorm=True): \n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    if dropout > 0:\n","        conv = layers.Dropout(dropout)(conv)\n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(conv)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    return conv\n","\n","def conv_block_one(x,filters, kernelsize=3, batchnorm=True): \n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    return conv\n","\n","def unet3plus(input_shape):\n","\n","    filters = [32,64, 128, 256, 512]\n","    kernelsize = 3\n","    inputs =layers.Input(input_shape) \n","    \n","    \"\"\" Encoder\"\"\"\n","    # block 1\n","    e1 = conv_block(inputs, filters[0])\n","\n","    # block 2\n","    e2 = layers.MaxPool2D(pool_size=(2, 2))(e1) \n","    e2 = conv_block(e2, filters[1]) \n","\n","    # block 3\n","    e3 = layers.MaxPool2D(pool_size=(2, 2))(e2) \n","    e3 = conv_block(e3, filters[2])\n","\n","    # block 4\n","    e4 = layers.MaxPool2D(pool_size=(2, 2))(e3) \n","    e4 = conv_block(e4, filters[3]) \n","\n","    # block 5\n","    # bottleneck layer\n","    e5 = layers.MaxPool2D(pool_size=(2, 2))(e4) \n","    e5 = conv_block(e5, filters[4])  \n","\n","    \"\"\" Decoder \"\"\"\n","    cat_channels = filters[0]\n","    cat_blocks = len(filters)\n","    upsample_channels = cat_blocks * cat_channels\n","\n","    \"\"\" d4 \"\"\"\n","    e1_d4 = layers.MaxPool2D(pool_size=(8, 8))(e1)  \n","    e1_d4 = conv_block_one(e1_d4, cat_channels) \n","\n","    e2_d4 = layers.MaxPool2D(pool_size=(4, 4))(e2)  \n","    e2_d4 = conv_block_one(e2_d4, cat_channels)  \n","\n","    e3_d4 = layers.MaxPool2D(pool_size=(2, 2))(e3)  \n","    e3_d4 = conv_block_one(e3_d4, cat_channels) \n","\n","    e4_d4 = conv_block_one(e4, cat_channels) \n","\n","    e5_d4 = layers.UpSampling2D(size=(2, 2))(e5) \n","    e5_d4 = conv_block_one(e5_d4, cat_channels)  \n","\n","    d4 = layers.concatenate([e1_d4, e2_d4, e3_d4, e4_d4, e5_d4])\n","    d4 = conv_block_one(d4, upsample_channels) \n","\n","    \"\"\" d3 \"\"\"\n","    e1_d3 = layers.MaxPool2D(pool_size=(4, 4))(e1) \n","    e1_d3 = conv_block_one(e1_d3, cat_channels)  \n","\n","    e2_d3 = layers.MaxPool2D(pool_size=(2, 2))(e2) \n","    e2_d3 = conv_block_one(e2_d3, cat_channels) \n","\n","    e3_d3 = conv_block_one(e3, cat_channels) \n","\n","    e4_d3 = layers.UpSampling2D(size=(2, 2))(d4) \n","    e4_d3 = conv_block_one(e4_d3, cat_channels)\n","\n","    e5_d3 = layers.UpSampling2D(size=(4, 4))(e5) \n","    e5_d3 = conv_block_one(e5_d3, cat_channels)\n","\n","    d3 = layers.concatenate([e1_d3, e2_d3, e3_d3, e4_d3, e5_d3])\n","    d3 = conv_block_one(d3, upsample_channels) \n","\n","    \"\"\" d2 \"\"\"\n","    e1_d2 = layers.MaxPool2D(pool_size=(2, 2))(e1)\n","    e1_d2 = conv_block_one(e1_d2, cat_channels) \n","\n","    e2_d2 = conv_block_one(e2, cat_channels) \n","\n","    d3_d2 = layers.UpSampling2D(size=(2, 2))(d3)  \n","    d3_d2 = conv_block_one(d3_d2, cat_channels) \n","\n","    d4_d2 = layers.UpSampling2D(size=(4, 4))(d4) \n","    d4_d2 = conv_block_one(d4_d2, cat_channels) \n","\n","    e5_d2 = layers.UpSampling2D(size=(8, 8))(e5)\n","    e5_d2 = conv_block_one(e5_d2, cat_channels) \n","\n","    d2 = layers.concatenate([e1_d2, e2_d2, d3_d2, d4_d2, e5_d2])\n","    d2 = conv_block_one(d2, upsample_channels) \n","\n","    \"\"\" d1 \"\"\"\n","    e1_d1 = conv_block_one(e1, cat_channels)  \n","\n","    d2_d1 = layers.UpSampling2D(size=(2, 2))(d2)\n","    d2_d1 = conv_block_one(d2_d1, cat_channels)\n","\n","    d3_d1 = layers.UpSampling2D(size=(4, 4))(d3) \n","    d3_d1 = conv_block_one(d3_d1, cat_channels) \n","\n","    d4_d1 = layers.UpSampling2D(size=(8, 8))(d4) \n","    d4_d1 = conv_block_one(d4_d1, cat_channels) \n","\n","    e5_d1 = layers.UpSampling2D(size=(16, 16))(e5) \n","    e5_d1 = conv_block_one(e5_d1, cat_channels) \n","\n","    d1 = layers.concatenate([e1_d1, d2_d1, d3_d1, d4_d1, e5_d1, ])\n","    d1 = conv_block_one(d1, upsample_channels) \n","\n","    conv_final = layers.Conv2D(1, kernel_size=(1,1))( d1)\n","    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n","    outputs = layers.Activation('sigmoid')(conv_final) \n","    \n","    return tf.keras.Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T19:19:35.119765Z","iopub.status.busy":"2023-04-03T19:19:35.119401Z","iopub.status.idle":"2023-04-03T19:19:36.268396Z","shell.execute_reply":"2023-04-03T19:19:36.267544Z","shell.execute_reply.started":"2023-04-03T19:19:35.119734Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 256, 256, 1  0           []                               \n","                                0)]                                                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 256, 256, 32  2912        ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 256, 256, 32  128        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," activation (Activation)        (None, 256, 256, 32  0           ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 256, 256, 32  9248        ['activation[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 256, 256, 32  128        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_1 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 128, 128, 32  0           ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 128, 128, 64  18496       ['max_pooling2d[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 128, 128, 64  256        ['conv2d_2[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_2 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_2[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 128, 128, 64  36928       ['activation_2[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 128, 128, 64  256        ['conv2d_3[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_3 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_3[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 64, 64, 128)  73856       ['max_pooling2d_1[0][0]']        \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_4[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_4 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 64, 64, 128)  147584      ['activation_4[0][0]']           \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_5[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_5 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0          ['activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 32, 32, 256)  295168      ['max_pooling2d_2[0][0]']        \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_6[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_6 (Activation)      (None, 32, 32, 256)  0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 32, 32, 256)  590080      ['activation_6[0][0]']           \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_7[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_7 (Activation)      (None, 32, 32, 256)  0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0          ['activation_7[0][0]']           \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 16, 16, 512)  1180160     ['max_pooling2d_3[0][0]']        \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_8[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_8 (Activation)      (None, 16, 16, 512)  0           ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 16, 16, 512)  2359808     ['activation_8[0][0]']           \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_9[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_9 (Activation)      (None, 16, 16, 512)  0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 32)  0           ['activation_1[0][0]']           \n","                                                                                                  \n"," max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 64)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 128)  0          ['activation_5[0][0]']           \n","                                                                                                  \n"," up_sampling2d (UpSampling2D)   (None, 32, 32, 512)  0           ['activation_9[0][0]']           \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 32, 32, 32)   9248        ['max_pooling2d_4[0][0]']        \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 32, 32, 32)   18464       ['max_pooling2d_5[0][0]']        \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 32, 32, 32)   36896       ['max_pooling2d_6[0][0]']        \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 32, 32, 32)   73760       ['activation_7[0][0]']           \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 32, 32, 32)   147488      ['up_sampling2d[0][0]']          \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 32, 32, 32)  128         ['conv2d_10[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 32, 32, 32)  128         ['conv2d_11[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 32, 32, 32)  128         ['conv2d_12[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 32, 32, 32)  128         ['conv2d_13[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 32, 32, 32)  128         ['conv2d_14[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_10 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_10[0][0]'] \n","                                                                                                  \n"," activation_11 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," activation_12 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," activation_13 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," activation_14 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_14[0][0]'] \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 32, 32, 160)  0           ['activation_10[0][0]',          \n","                                                                  'activation_11[0][0]',          \n","                                                                  'activation_12[0][0]',          \n","                                                                  'activation_13[0][0]',          \n","                                                                  'activation_14[0][0]']          \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 32, 32, 160)  230560      ['concatenate[0][0]']            \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 32, 32, 160)  640        ['conv2d_15[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_15 (Activation)     (None, 32, 32, 160)  0           ['batch_normalization_15[0][0]'] \n","                                                                                                  \n"," max_pooling2d_7 (MaxPooling2D)  (None, 64, 64, 32)  0           ['activation_1[0][0]']           \n","                                                                                                  \n"," max_pooling2d_8 (MaxPooling2D)  (None, 64, 64, 64)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 160)  0          ['activation_15[0][0]']          \n","                                                                                                  \n"," up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 512)  0          ['activation_9[0][0]']           \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 64, 64, 32)   9248        ['max_pooling2d_7[0][0]']        \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 64, 64, 32)   18464       ['max_pooling2d_8[0][0]']        \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 64, 64, 32)   36896       ['activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 64, 64, 32)   46112       ['up_sampling2d_1[0][0]']        \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 64, 64, 32)   147488      ['up_sampling2d_2[0][0]']        \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 64, 64, 32)  128         ['conv2d_16[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 64, 64, 32)  128         ['conv2d_17[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 64, 64, 32)  128         ['conv2d_18[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_19 (BatchN  (None, 64, 64, 32)  128         ['conv2d_19[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_20 (BatchN  (None, 64, 64, 32)  128         ['conv2d_20[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_16 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_16[0][0]'] \n","                                                                                                  \n"," activation_17 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_17[0][0]'] \n","                                                                                                  \n"," activation_18 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_18[0][0]'] \n","                                                                                                  \n"," activation_19 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_19[0][0]'] \n","                                                                                                  \n"," activation_20 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_20[0][0]'] \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 64, 64, 160)  0           ['activation_16[0][0]',          \n","                                                                  'activation_17[0][0]',          \n","                                                                  'activation_18[0][0]',          \n","                                                                  'activation_19[0][0]',          \n","                                                                  'activation_20[0][0]']          \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 64, 64, 160)  230560      ['concatenate_1[0][0]']          \n","                                                                                                  \n"," batch_normalization_21 (BatchN  (None, 64, 64, 160)  640        ['conv2d_21[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_21 (Activation)     (None, 64, 64, 160)  0           ['batch_normalization_21[0][0]'] \n","                                                                                                  \n"," max_pooling2d_9 (MaxPooling2D)  (None, 128, 128, 32  0          ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 16  0          ['activation_21[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," up_sampling2d_4 (UpSampling2D)  (None, 128, 128, 16  0          ['activation_15[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," up_sampling2d_5 (UpSampling2D)  (None, 128, 128, 51  0          ['activation_9[0][0]']           \n","                                2)                                                                \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 128, 128, 32  9248        ['max_pooling2d_9[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 128, 128, 32  18464       ['activation_3[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_24 (Conv2D)             (None, 128, 128, 32  46112       ['up_sampling2d_3[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_25 (Conv2D)             (None, 128, 128, 32  46112       ['up_sampling2d_4[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_26 (Conv2D)             (None, 128, 128, 32  147488      ['up_sampling2d_5[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_22 (BatchN  (None, 128, 128, 32  128        ['conv2d_22[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_23 (BatchN  (None, 128, 128, 32  128        ['conv2d_23[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_24 (BatchN  (None, 128, 128, 32  128        ['conv2d_24[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_25 (BatchN  (None, 128, 128, 32  128        ['conv2d_25[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_26 (BatchN  (None, 128, 128, 32  128        ['conv2d_26[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_22 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_22[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_23 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_23[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_24 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_24[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_25 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_25[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_26 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_26[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 128, 128, 16  0           ['activation_22[0][0]',          \n","                                0)                                'activation_23[0][0]',          \n","                                                                  'activation_24[0][0]',          \n","                                                                  'activation_25[0][0]',          \n","                                                                  'activation_26[0][0]']          \n","                                                                                                  \n"," conv2d_27 (Conv2D)             (None, 128, 128, 16  230560      ['concatenate_2[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," batch_normalization_27 (BatchN  (None, 128, 128, 16  640        ['conv2d_27[0][0]']              \n"," ormalization)                  0)                                                                \n","                                                                                                  \n"," activation_27 (Activation)     (None, 128, 128, 16  0           ['batch_normalization_27[0][0]'] \n","                                0)                                                                \n","                                                                                                  \n"," up_sampling2d_6 (UpSampling2D)  (None, 256, 256, 16  0          ['activation_27[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," up_sampling2d_7 (UpSampling2D)  (None, 256, 256, 16  0          ['activation_21[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," up_sampling2d_8 (UpSampling2D)  (None, 256, 256, 16  0          ['activation_15[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," up_sampling2d_9 (UpSampling2D)  (None, 256, 256, 51  0          ['activation_9[0][0]']           \n","                                2)                                                                \n","                                                                                                  \n"," conv2d_28 (Conv2D)             (None, 256, 256, 32  9248        ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_29 (Conv2D)             (None, 256, 256, 32  46112       ['up_sampling2d_6[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_30 (Conv2D)             (None, 256, 256, 32  46112       ['up_sampling2d_7[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_31 (Conv2D)             (None, 256, 256, 32  46112       ['up_sampling2d_8[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_32 (Conv2D)             (None, 256, 256, 32  147488      ['up_sampling2d_9[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_28 (BatchN  (None, 256, 256, 32  128        ['conv2d_28[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_29 (BatchN  (None, 256, 256, 32  128        ['conv2d_29[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_30 (BatchN  (None, 256, 256, 32  128        ['conv2d_30[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_31 (BatchN  (None, 256, 256, 32  128        ['conv2d_31[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_32 (BatchN  (None, 256, 256, 32  128        ['conv2d_32[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_28 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_28[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_29 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_29[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_30 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_30[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_31 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_31[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_32 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_32[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 256, 256, 16  0           ['activation_28[0][0]',          \n","                                0)                                'activation_29[0][0]',          \n","                                                                  'activation_30[0][0]',          \n","                                                                  'activation_31[0][0]',          \n","                                                                  'activation_32[0][0]']          \n","                                                                                                  \n"," conv2d_33 (Conv2D)             (None, 256, 256, 16  230560      ['concatenate_3[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," batch_normalization_33 (BatchN  (None, 256, 256, 16  640        ['conv2d_33[0][0]']              \n"," ormalization)                  0)                                                                \n","                                                                                                  \n"," activation_33 (Activation)     (None, 256, 256, 16  0           ['batch_normalization_33[0][0]'] \n","                                0)                                                                \n","                                                                                                  \n"," conv2d_34 (Conv2D)             (None, 256, 256, 1)  161         ['activation_33[0][0]']          \n","                                                                                                  \n"," batch_normalization_34 (BatchN  (None, 256, 256, 1)  4          ['conv2d_34[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_34 (Activation)     (None, 256, 256, 1)  0           ['batch_normalization_34[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 6,756,261\n","Trainable params: 6,749,731\n","Non-trainable params: 6,530\n","__________________________________________________________________________________________________\n"]}],"source":["unet = unet3plus(input_shape=(256,256,10))\n","# gl_sl_wrapper(alpha)\n","unet.compile(optimizer = adam_v2.Adam(learning_rate = 1e-4), loss =gl_sl_wrapper(alpha), metrics = ['accuracy'])\n","unet.summary()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T19:19:48.444176Z","iopub.status.busy":"2023-04-03T19:19:48.443584Z","iopub.status.idle":"2023-04-04T00:36:50.392323Z","shell.execute_reply":"2023-04-04T00:36:50.391337Z","shell.execute_reply.started":"2023-04-03T19:19:48.444137Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.4273 - accuracy: 0.9063\n","Epoch 1: val_loss improved from inf to 0.53977, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 62s 442ms/step - loss: 0.4273 - accuracy: 0.9063 - val_loss: 0.5398 - val_accuracy: 0.8054 - lr: 1.0000e-04\n","Epoch 2/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3962 - accuracy: 0.9446\n","Epoch 2: val_loss improved from 0.53977 to 0.44062, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.3962 - accuracy: 0.9446 - val_loss: 0.4406 - val_accuracy: 0.9016 - lr: 1.0000e-04\n","Epoch 3/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3793 - accuracy: 0.9612\n","Epoch 3: val_loss improved from 0.44062 to 0.38192, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.3793 - accuracy: 0.9612 - val_loss: 0.3819 - val_accuracy: 0.9616 - lr: 1.0000e-04\n","Epoch 4/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3747 - accuracy: 0.9640\n","Epoch 4: val_loss improved from 0.38192 to 0.37381, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.3747 - accuracy: 0.9640 - val_loss: 0.3738 - val_accuracy: 0.9699 - lr: 1.0000e-04\n","Epoch 5/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3670 - accuracy: 0.9667\n","Epoch 5: val_loss improved from 0.37381 to 0.36888, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 47s 417ms/step - loss: 0.3670 - accuracy: 0.9667 - val_loss: 0.3689 - val_accuracy: 0.9790 - lr: 1.0000e-04\n","Epoch 6/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3566 - accuracy: 0.9731\n","Epoch 6: val_loss improved from 0.36888 to 0.35958, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.3566 - accuracy: 0.9731 - val_loss: 0.3596 - val_accuracy: 0.9798 - lr: 1.0000e-04\n","Epoch 7/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3659 - accuracy: 0.9620\n","Epoch 7: val_loss improved from 0.35958 to 0.35464, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 47s 416ms/step - loss: 0.3659 - accuracy: 0.9620 - val_loss: 0.3546 - val_accuracy: 0.9724 - lr: 1.0000e-04\n","Epoch 8/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.9711\n","Epoch 8: val_loss improved from 0.35464 to 0.34864, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.3563 - accuracy: 0.9711 - val_loss: 0.3486 - val_accuracy: 0.9820 - lr: 1.0000e-04\n","Epoch 9/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3442 - accuracy: 0.9785\n","Epoch 9: val_loss improved from 0.34864 to 0.34161, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.3442 - accuracy: 0.9785 - val_loss: 0.3416 - val_accuracy: 0.9837 - lr: 1.0000e-04\n","Epoch 10/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.9794\n","Epoch 10: val_loss improved from 0.34161 to 0.33135, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.3413 - accuracy: 0.9794 - val_loss: 0.3313 - val_accuracy: 0.9831 - lr: 1.0000e-04\n","Epoch 11/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3411 - accuracy: 0.9731\n","Epoch 11: val_loss improved from 0.33135 to 0.32230, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.3411 - accuracy: 0.9731 - val_loss: 0.3223 - val_accuracy: 0.9772 - lr: 1.0000e-04\n","Epoch 12/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3358 - accuracy: 0.9784\n","Epoch 12: val_loss did not improve from 0.32230\n","112/112 [==============================] - 46s 410ms/step - loss: 0.3358 - accuracy: 0.9784 - val_loss: 0.3357 - val_accuracy: 0.9834 - lr: 1.0000e-04\n","Epoch 13/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.9818\n","Epoch 13: val_loss did not improve from 0.32230\n","112/112 [==============================] - 46s 410ms/step - loss: 0.3271 - accuracy: 0.9818 - val_loss: 0.3318 - val_accuracy: 0.9841 - lr: 1.0000e-04\n","Epoch 14/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3225 - accuracy: 0.9829\n","Epoch 14: val_loss improved from 0.32230 to 0.31961, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.3225 - accuracy: 0.9829 - val_loss: 0.3196 - val_accuracy: 0.9848 - lr: 1.0000e-04\n","Epoch 15/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3202 - accuracy: 0.9827\n","Epoch 15: val_loss improved from 0.31961 to 0.31184, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.3202 - accuracy: 0.9827 - val_loss: 0.3118 - val_accuracy: 0.9854 - lr: 1.0000e-04\n","Epoch 16/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.9822\n","Epoch 16: val_loss improved from 0.31184 to 0.31146, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.3183 - accuracy: 0.9822 - val_loss: 0.3115 - val_accuracy: 0.9800 - lr: 1.0000e-04\n","Epoch 17/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3167 - accuracy: 0.9795\n","Epoch 17: val_loss did not improve from 0.31146\n","112/112 [==============================] - 46s 410ms/step - loss: 0.3167 - accuracy: 0.9795 - val_loss: 0.3215 - val_accuracy: 0.9551 - lr: 1.0000e-04\n","Epoch 18/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.9796\n","Epoch 18: val_loss improved from 0.31146 to 0.30676, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.3138 - accuracy: 0.9796 - val_loss: 0.3068 - val_accuracy: 0.9860 - lr: 1.0000e-04\n","Epoch 19/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3059 - accuracy: 0.9849\n","Epoch 19: val_loss did not improve from 0.30676\n","112/112 [==============================] - 46s 410ms/step - loss: 0.3059 - accuracy: 0.9849 - val_loss: 0.3093 - val_accuracy: 0.9859 - lr: 1.0000e-04\n","Epoch 20/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3013 - accuracy: 0.9838\n","Epoch 20: val_loss improved from 0.30676 to 0.29692, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.3013 - accuracy: 0.9838 - val_loss: 0.2969 - val_accuracy: 0.9864 - lr: 1.0000e-04\n","Epoch 21/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.9828\n","Epoch 21: val_loss improved from 0.29692 to 0.28639, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.2996 - accuracy: 0.9828 - val_loss: 0.2864 - val_accuracy: 0.9857 - lr: 1.0000e-04\n","Epoch 22/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2978 - accuracy: 0.9840\n","Epoch 22: val_loss did not improve from 0.28639\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2978 - accuracy: 0.9840 - val_loss: 0.2909 - val_accuracy: 0.9855 - lr: 1.0000e-04\n","Epoch 23/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9848\n","Epoch 23: val_loss did not improve from 0.28639\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2941 - accuracy: 0.9848 - val_loss: 0.2889 - val_accuracy: 0.9862 - lr: 1.0000e-04\n","Epoch 24/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2882 - accuracy: 0.9857\n","Epoch 24: val_loss did not improve from 0.28639\n","112/112 [==============================] - 44s 392ms/step - loss: 0.2882 - accuracy: 0.9857 - val_loss: 0.2927 - val_accuracy: 0.9810 - lr: 1.0000e-04\n","Epoch 25/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9852\n","Epoch 25: val_loss improved from 0.28639 to 0.27709, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.2878 - accuracy: 0.9852 - val_loss: 0.2771 - val_accuracy: 0.9852 - lr: 1.0000e-04\n","Epoch 26/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.9875\n","Epoch 26: val_loss improved from 0.27709 to 0.27575, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.2800 - accuracy: 0.9875 - val_loss: 0.2758 - val_accuracy: 0.9872 - lr: 1.0000e-04\n","Epoch 27/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2802 - accuracy: 0.9849\n","Epoch 27: val_loss improved from 0.27575 to 0.27306, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 398ms/step - loss: 0.2802 - accuracy: 0.9849 - val_loss: 0.2731 - val_accuracy: 0.9882 - lr: 1.0000e-04\n","Epoch 28/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.9861\n","Epoch 28: val_loss did not improve from 0.27306\n","112/112 [==============================] - 44s 392ms/step - loss: 0.2775 - accuracy: 0.9861 - val_loss: 0.2755 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 29/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.9807\n","Epoch 29: val_loss improved from 0.27306 to 0.26555, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 398ms/step - loss: 0.2800 - accuracy: 0.9807 - val_loss: 0.2655 - val_accuracy: 0.9861 - lr: 1.0000e-04\n","Epoch 30/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.9872\n","Epoch 30: val_loss improved from 0.26555 to 0.26375, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.2710 - accuracy: 0.9872 - val_loss: 0.2638 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 31/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.9877\n","Epoch 31: val_loss did not improve from 0.26375\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2675 - accuracy: 0.9877 - val_loss: 0.2651 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 32/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.9877\n","Epoch 32: val_loss did not improve from 0.26375\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2642 - accuracy: 0.9877 - val_loss: 0.2639 - val_accuracy: 0.9854 - lr: 1.0000e-04\n","Epoch 33/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2656 - accuracy: 0.9854\n","Epoch 33: val_loss improved from 0.26375 to 0.25479, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.2656 - accuracy: 0.9854 - val_loss: 0.2548 - val_accuracy: 0.9857 - lr: 1.0000e-04\n","Epoch 34/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.9868\n","Epoch 34: val_loss did not improve from 0.25479\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2585 - accuracy: 0.9868 - val_loss: 0.2555 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 35/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.9890\n","Epoch 35: val_loss did not improve from 0.25479\n","112/112 [==============================] - 46s 409ms/step - loss: 0.2547 - accuracy: 0.9890 - val_loss: 0.2556 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 36/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.9867\n","Epoch 36: val_loss improved from 0.25479 to 0.24909, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 416ms/step - loss: 0.2590 - accuracy: 0.9867 - val_loss: 0.2491 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 37/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.9860\n","Epoch 37: val_loss improved from 0.24909 to 0.23870, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.2554 - accuracy: 0.9860 - val_loss: 0.2387 - val_accuracy: 0.9881 - lr: 1.0000e-04\n","Epoch 38/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.9886\n","Epoch 38: val_loss did not improve from 0.23870\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2510 - accuracy: 0.9886 - val_loss: 0.2400 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 39/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.9899\n","Epoch 39: val_loss did not improve from 0.23870\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2454 - accuracy: 0.9899 - val_loss: 0.2389 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 40/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2439 - accuracy: 0.9889\n","Epoch 40: val_loss improved from 0.23870 to 0.23507, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.2439 - accuracy: 0.9889 - val_loss: 0.2351 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 41/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.9899\n","Epoch 41: val_loss did not improve from 0.23507\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2378 - accuracy: 0.9899 - val_loss: 0.2422 - val_accuracy: 0.9868 - lr: 1.0000e-04\n","Epoch 42/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9882\n","Epoch 42: val_loss improved from 0.23507 to 0.23441, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.2411 - accuracy: 0.9882 - val_loss: 0.2344 - val_accuracy: 0.9893 - lr: 1.0000e-04\n","Epoch 43/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.9890\n","Epoch 43: val_loss improved from 0.23441 to 0.22538, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.2351 - accuracy: 0.9890 - val_loss: 0.2254 - val_accuracy: 0.9882 - lr: 1.0000e-04\n","Epoch 44/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2360 - accuracy: 0.9892\n","Epoch 44: val_loss did not improve from 0.22538\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2360 - accuracy: 0.9892 - val_loss: 0.2292 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 45/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.9878\n","Epoch 45: val_loss improved from 0.22538 to 0.22031, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.2326 - accuracy: 0.9878 - val_loss: 0.2203 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 46/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2331 - accuracy: 0.9862\n","Epoch 46: val_loss did not improve from 0.22031\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2331 - accuracy: 0.9862 - val_loss: 0.2225 - val_accuracy: 0.9882 - lr: 1.0000e-04\n","Epoch 47/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2293 - accuracy: 0.9874\n","Epoch 47: val_loss did not improve from 0.22031\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2293 - accuracy: 0.9874 - val_loss: 0.2252 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 48/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2242 - accuracy: 0.9893\n","Epoch 48: val_loss improved from 0.22031 to 0.21983, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.2242 - accuracy: 0.9893 - val_loss: 0.2198 - val_accuracy: 0.9886 - lr: 1.0000e-04\n","Epoch 49/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2231 - accuracy: 0.9903\n","Epoch 49: val_loss improved from 0.21983 to 0.21172, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.2231 - accuracy: 0.9903 - val_loss: 0.2117 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 50/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2201 - accuracy: 0.9900\n","Epoch 50: val_loss did not improve from 0.21172\n","112/112 [==============================] - 46s 409ms/step - loss: 0.2201 - accuracy: 0.9900 - val_loss: 0.2127 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 51/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2143 - accuracy: 0.9903\n","Epoch 51: val_loss improved from 0.21172 to 0.20372, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.2143 - accuracy: 0.9903 - val_loss: 0.2037 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 52/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2146 - accuracy: 0.9905\n","Epoch 52: val_loss did not improve from 0.20372\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2146 - accuracy: 0.9905 - val_loss: 0.2076 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 53/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2130 - accuracy: 0.9875\n","Epoch 53: val_loss did not improve from 0.20372\n","112/112 [==============================] - 44s 392ms/step - loss: 0.2130 - accuracy: 0.9875 - val_loss: 0.2072 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 54/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2142 - accuracy: 0.9864\n","Epoch 54: val_loss improved from 0.20372 to 0.19927, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.2142 - accuracy: 0.9864 - val_loss: 0.1993 - val_accuracy: 0.9885 - lr: 1.0000e-04\n","Epoch 55/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9880\n","Epoch 55: val_loss did not improve from 0.19927\n","112/112 [==============================] - 44s 392ms/step - loss: 0.2104 - accuracy: 0.9880 - val_loss: 0.2031 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 56/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2074 - accuracy: 0.9903\n","Epoch 56: val_loss did not improve from 0.19927\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2074 - accuracy: 0.9903 - val_loss: 0.2015 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 57/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9897\n","Epoch 57: val_loss did not improve from 0.19927\n","112/112 [==============================] - 46s 410ms/step - loss: 0.2092 - accuracy: 0.9897 - val_loss: 0.1998 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 58/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2006 - accuracy: 0.9918\n","Epoch 58: val_loss improved from 0.19927 to 0.19709, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.2006 - accuracy: 0.9918 - val_loss: 0.1971 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 59/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.9920\n","Epoch 59: val_loss improved from 0.19709 to 0.19573, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.2015 - accuracy: 0.9920 - val_loss: 0.1957 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 60/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.9917\n","Epoch 60: val_loss improved from 0.19573 to 0.19043, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.1972 - accuracy: 0.9917 - val_loss: 0.1904 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 61/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9920\n","Epoch 61: val_loss improved from 0.19043 to 0.18508, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 398ms/step - loss: 0.1932 - accuracy: 0.9920 - val_loss: 0.1851 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 62/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.9920\n","Epoch 62: val_loss improved from 0.18508 to 0.18467, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1920 - accuracy: 0.9920 - val_loss: 0.1847 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 63/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.9921\n","Epoch 63: val_loss did not improve from 0.18467\n","112/112 [==============================] - 44s 391ms/step - loss: 0.1915 - accuracy: 0.9921 - val_loss: 0.1853 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 64/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1900 - accuracy: 0.9925\n","Epoch 64: val_loss improved from 0.18467 to 0.18371, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1900 - accuracy: 0.9925 - val_loss: 0.1837 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 65/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1872 - accuracy: 0.9917\n","Epoch 65: val_loss improved from 0.18371 to 0.17932, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1872 - accuracy: 0.9917 - val_loss: 0.1793 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 66/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.9909\n","Epoch 66: val_loss improved from 0.17932 to 0.17684, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.1851 - accuracy: 0.9909 - val_loss: 0.1768 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 67/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1803 - accuracy: 0.9916\n","Epoch 67: val_loss improved from 0.17684 to 0.17622, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.1803 - accuracy: 0.9916 - val_loss: 0.1762 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 68/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1801 - accuracy: 0.9917\n","Epoch 68: val_loss improved from 0.17622 to 0.17130, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1801 - accuracy: 0.9917 - val_loss: 0.1713 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 69/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1783 - accuracy: 0.9921\n","Epoch 69: val_loss did not improve from 0.17130\n","112/112 [==============================] - 46s 410ms/step - loss: 0.1783 - accuracy: 0.9921 - val_loss: 0.1757 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 70/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1849 - accuracy: 0.9875\n","Epoch 70: val_loss did not improve from 0.17130\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1849 - accuracy: 0.9875 - val_loss: 0.1765 - val_accuracy: 0.9860 - lr: 1.0000e-04\n","Epoch 71/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1761 - accuracy: 0.9912\n","Epoch 71: val_loss did not improve from 0.17130\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1761 - accuracy: 0.9912 - val_loss: 0.1732 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 72/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.9907\n","Epoch 72: val_loss improved from 0.17130 to 0.16431, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1767 - accuracy: 0.9907 - val_loss: 0.1643 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 73/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.9920\n","Epoch 73: val_loss did not improve from 0.16431\n","112/112 [==============================] - 46s 410ms/step - loss: 0.1757 - accuracy: 0.9920 - val_loss: 0.1646 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 74/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.9920\n","Epoch 74: val_loss improved from 0.16431 to 0.16341, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.1697 - accuracy: 0.9920 - val_loss: 0.1634 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 75/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1694 - accuracy: 0.9927\n","Epoch 75: val_loss improved from 0.16341 to 0.16177, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1694 - accuracy: 0.9927 - val_loss: 0.1618 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 76/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1653 - accuracy: 0.9924\n","Epoch 76: val_loss improved from 0.16177 to 0.15794, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1653 - accuracy: 0.9924 - val_loss: 0.1579 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 77/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9919\n","Epoch 77: val_loss improved from 0.15794 to 0.15493, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1642 - accuracy: 0.9919 - val_loss: 0.1549 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 78/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9909\n","Epoch 78: val_loss did not improve from 0.15493\n","112/112 [==============================] - 44s 392ms/step - loss: 0.1667 - accuracy: 0.9909 - val_loss: 0.1608 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 79/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1664 - accuracy: 0.9919\n","Epoch 79: val_loss did not improve from 0.15493\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1664 - accuracy: 0.9919 - val_loss: 0.1554 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 80/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9919\n","Epoch 80: val_loss improved from 0.15493 to 0.15188, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1590 - accuracy: 0.9919 - val_loss: 0.1519 - val_accuracy: 0.9883 - lr: 1.0000e-04\n","Epoch 81/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1698 - accuracy: 0.9840\n","Epoch 81: val_loss did not improve from 0.15188\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1698 - accuracy: 0.9840 - val_loss: 0.2225 - val_accuracy: 0.9504 - lr: 1.0000e-04\n","Epoch 82/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.9896\n","Epoch 82: val_loss improved from 0.15188 to 0.14961, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1622 - accuracy: 0.9896 - val_loss: 0.1496 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 83/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9915\n","Epoch 83: val_loss improved from 0.14961 to 0.14898, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 47s 416ms/step - loss: 0.1599 - accuracy: 0.9915 - val_loss: 0.1490 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 84/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9885\n","Epoch 84: val_loss improved from 0.14898 to 0.14184, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1633 - accuracy: 0.9885 - val_loss: 0.1418 - val_accuracy: 0.9880 - lr: 1.0000e-04\n","Epoch 85/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1584 - accuracy: 0.9902\n","Epoch 85: val_loss did not improve from 0.14184\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1584 - accuracy: 0.9902 - val_loss: 0.1502 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 86/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1518 - accuracy: 0.9926\n","Epoch 86: val_loss did not improve from 0.14184\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1518 - accuracy: 0.9926 - val_loss: 0.1453 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 87/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1547 - accuracy: 0.9922\n","Epoch 87: val_loss did not improve from 0.14184\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1547 - accuracy: 0.9922 - val_loss: 0.1440 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 88/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9924\n","Epoch 88: val_loss did not improve from 0.14184\n","112/112 [==============================] - 46s 410ms/step - loss: 0.1481 - accuracy: 0.9924 - val_loss: 0.1421 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 89/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9925\n","Epoch 89: val_loss improved from 0.14184 to 0.14035, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1484 - accuracy: 0.9925 - val_loss: 0.1404 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 90/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9926\n","Epoch 90: val_loss improved from 0.14035 to 0.13652, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.1491 - accuracy: 0.9926 - val_loss: 0.1365 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 91/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.9916\n","Epoch 91: val_loss did not improve from 0.13652\n","112/112 [==============================] - 46s 410ms/step - loss: 0.1468 - accuracy: 0.9916 - val_loss: 0.1388 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 92/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9928\n","Epoch 92: val_loss improved from 0.13652 to 0.13518, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1451 - accuracy: 0.9928 - val_loss: 0.1352 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 93/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1450 - accuracy: 0.9909\n","Epoch 93: val_loss improved from 0.13518 to 0.13379, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1450 - accuracy: 0.9909 - val_loss: 0.1338 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 94/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1430 - accuracy: 0.9918\n","Epoch 94: val_loss improved from 0.13379 to 0.13255, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.1430 - accuracy: 0.9918 - val_loss: 0.1326 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 95/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9933\n","Epoch 95: val_loss improved from 0.13255 to 0.13250, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1402 - accuracy: 0.9933 - val_loss: 0.1325 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 96/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1377 - accuracy: 0.9930\n","Epoch 96: val_loss improved from 0.13250 to 0.12994, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.1377 - accuracy: 0.9930 - val_loss: 0.1299 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 97/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9930\n","Epoch 97: val_loss improved from 0.12994 to 0.12580, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1381 - accuracy: 0.9930 - val_loss: 0.1258 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 98/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9893\n","Epoch 98: val_loss did not improve from 0.12580\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1417 - accuracy: 0.9893 - val_loss: 0.1279 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 99/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9929\n","Epoch 99: val_loss did not improve from 0.12580\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1361 - accuracy: 0.9929 - val_loss: 0.1263 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 100/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9934\n","Epoch 100: val_loss did not improve from 0.12580\n","112/112 [==============================] - 44s 392ms/step - loss: 0.1317 - accuracy: 0.9934 - val_loss: 0.1264 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 101/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1312 - accuracy: 0.9928\n","Epoch 101: val_loss improved from 0.12580 to 0.12262, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.1312 - accuracy: 0.9928 - val_loss: 0.1226 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 102/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1328 - accuracy: 0.9919\n","Epoch 102: val_loss improved from 0.12262 to 0.12192, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1328 - accuracy: 0.9919 - val_loss: 0.1219 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 103/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9934\n","Epoch 103: val_loss did not improve from 0.12192\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1289 - accuracy: 0.9934 - val_loss: 0.1219 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 104/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9935\n","Epoch 104: val_loss did not improve from 0.12192\n","112/112 [==============================] - 46s 410ms/step - loss: 0.1301 - accuracy: 0.9935 - val_loss: 0.1221 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 105/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.9937\n","Epoch 105: val_loss improved from 0.12192 to 0.12027, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1298 - accuracy: 0.9937 - val_loss: 0.1203 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 106/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9937\n","Epoch 106: val_loss improved from 0.12027 to 0.11700, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1249 - accuracy: 0.9937 - val_loss: 0.1170 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 107/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9939\n","Epoch 107: val_loss did not improve from 0.11700\n","112/112 [==============================] - 44s 391ms/step - loss: 0.1249 - accuracy: 0.9939 - val_loss: 0.1173 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 108/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9941\n","Epoch 108: val_loss improved from 0.11700 to 0.11464, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1223 - accuracy: 0.9941 - val_loss: 0.1146 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 109/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9933\n","Epoch 109: val_loss did not improve from 0.11464\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1200 - accuracy: 0.9933 - val_loss: 0.1148 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 110/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9910\n","Epoch 110: val_loss did not improve from 0.11464\n","112/112 [==============================] - 46s 410ms/step - loss: 0.1276 - accuracy: 0.9910 - val_loss: 0.1210 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 111/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9870\n","Epoch 111: val_loss did not improve from 0.11464\n","112/112 [==============================] - 46s 410ms/step - loss: 0.1331 - accuracy: 0.9870 - val_loss: 0.1160 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 112/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1212 - accuracy: 0.9923\n","Epoch 112: val_loss did not improve from 0.11464\n","112/112 [==============================] - 46s 410ms/step - loss: 0.1212 - accuracy: 0.9923 - val_loss: 0.1150 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 113/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1199 - accuracy: 0.9929\n","Epoch 113: val_loss did not improve from 0.11464\n","112/112 [==============================] - 44s 392ms/step - loss: 0.1199 - accuracy: 0.9929 - val_loss: 0.1165 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 114/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9923\n","Epoch 114: val_loss improved from 0.11464 to 0.11112, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1204 - accuracy: 0.9923 - val_loss: 0.1111 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 115/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1168 - accuracy: 0.9936\n","Epoch 115: val_loss improved from 0.11112 to 0.11069, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.1168 - accuracy: 0.9936 - val_loss: 0.1107 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 116/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1157 - accuracy: 0.9940\n","Epoch 116: val_loss improved from 0.11069 to 0.10749, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1157 - accuracy: 0.9940 - val_loss: 0.1075 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 117/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1165 - accuracy: 0.9938\n","Epoch 117: val_loss improved from 0.10749 to 0.10523, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.1165 - accuracy: 0.9938 - val_loss: 0.1052 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 118/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9937\n","Epoch 118: val_loss improved from 0.10523 to 0.10503, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1115 - accuracy: 0.9937 - val_loss: 0.1050 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 119/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1138 - accuracy: 0.9937\n","Epoch 119: val_loss did not improve from 0.10503\n","112/112 [==============================] - 44s 391ms/step - loss: 0.1138 - accuracy: 0.9937 - val_loss: 0.1052 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 120/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9941\n","Epoch 120: val_loss improved from 0.10503 to 0.10366, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1101 - accuracy: 0.9941 - val_loss: 0.1037 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 121/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9941\n","Epoch 121: val_loss improved from 0.10366 to 0.10254, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1100 - accuracy: 0.9941 - val_loss: 0.1025 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 122/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.9944\n","Epoch 122: val_loss did not improve from 0.10254\n","112/112 [==============================] - 44s 393ms/step - loss: 0.1094 - accuracy: 0.9944 - val_loss: 0.1035 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 123/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9945\n","Epoch 123: val_loss improved from 0.10254 to 0.10061, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1045 - accuracy: 0.9945 - val_loss: 0.1006 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 124/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9941\n","Epoch 124: val_loss improved from 0.10061 to 0.09893, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1081 - accuracy: 0.9941 - val_loss: 0.0989 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 125/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9944\n","Epoch 125: val_loss did not improve from 0.09893\n","112/112 [==============================] - 44s 393ms/step - loss: 0.1070 - accuracy: 0.9944 - val_loss: 0.1018 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 126/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 0.9917\n","Epoch 126: val_loss improved from 0.09893 to 0.09452, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 398ms/step - loss: 0.1088 - accuracy: 0.9917 - val_loss: 0.0945 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 127/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9932\n","Epoch 127: val_loss did not improve from 0.09452\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1042 - accuracy: 0.9932 - val_loss: 0.0977 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 128/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1000 - accuracy: 0.9940\n","Epoch 128: val_loss did not improve from 0.09452\n","112/112 [==============================] - 44s 393ms/step - loss: 0.1000 - accuracy: 0.9940 - val_loss: 0.0973 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 129/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9936\n","Epoch 129: val_loss did not improve from 0.09452\n","112/112 [==============================] - 46s 410ms/step - loss: 0.1077 - accuracy: 0.9936 - val_loss: 0.0961 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 130/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9940\n","Epoch 130: val_loss improved from 0.09452 to 0.09346, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1014 - accuracy: 0.9940 - val_loss: 0.0935 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 131/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9937\n","Epoch 131: val_loss improved from 0.09346 to 0.09174, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1023 - accuracy: 0.9937 - val_loss: 0.0917 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 132/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9936\n","Epoch 132: val_loss did not improve from 0.09174\n","112/112 [==============================] - 46s 410ms/step - loss: 0.1030 - accuracy: 0.9936 - val_loss: 0.0947 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 133/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9937\n","Epoch 133: val_loss did not improve from 0.09174\n","112/112 [==============================] - 44s 393ms/step - loss: 0.1005 - accuracy: 0.9937 - val_loss: 0.0946 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 134/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9924\n","Epoch 134: val_loss improved from 0.09174 to 0.08894, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1043 - accuracy: 0.9924 - val_loss: 0.0889 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 135/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9935\n","Epoch 135: val_loss did not improve from 0.08894\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0984 - accuracy: 0.9935 - val_loss: 0.0911 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 136/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9943\n","Epoch 136: val_loss did not improve from 0.08894\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0970 - accuracy: 0.9943 - val_loss: 0.0919 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 137/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9945\n","Epoch 137: val_loss did not improve from 0.08894\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0944 - accuracy: 0.9945 - val_loss: 0.0905 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 138/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9948\n","Epoch 138: val_loss improved from 0.08894 to 0.08802, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0950 - accuracy: 0.9948 - val_loss: 0.0880 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 139/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9945\n","Epoch 139: val_loss improved from 0.08802 to 0.08602, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0972 - accuracy: 0.9945 - val_loss: 0.0860 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 140/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9946\n","Epoch 140: val_loss did not improve from 0.08602\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0955 - accuracy: 0.9946 - val_loss: 0.0893 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 141/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9930\n","Epoch 141: val_loss improved from 0.08602 to 0.08100, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0971 - accuracy: 0.9930 - val_loss: 0.0810 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 142/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.9918\n","Epoch 142: val_loss did not improve from 0.08100\n","112/112 [==============================] - 46s 409ms/step - loss: 0.1009 - accuracy: 0.9918 - val_loss: 0.0927 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 143/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9935\n","Epoch 143: val_loss did not improve from 0.08100\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0933 - accuracy: 0.9935 - val_loss: 0.0844 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 144/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9943\n","Epoch 144: val_loss did not improve from 0.08100\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0921 - accuracy: 0.9943 - val_loss: 0.0819 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 145/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9935\n","Epoch 145: val_loss did not improve from 0.08100\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0931 - accuracy: 0.9935 - val_loss: 0.0813 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 146/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9930\n","Epoch 146: val_loss improved from 0.08100 to 0.08082, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0938 - accuracy: 0.9930 - val_loss: 0.0808 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 147/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9946\n","Epoch 147: val_loss did not improve from 0.08082\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0889 - accuracy: 0.9946 - val_loss: 0.0826 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 148/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9944\n","Epoch 148: val_loss improved from 0.08082 to 0.07922, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0885 - accuracy: 0.9944 - val_loss: 0.0792 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 149/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9909\n","Epoch 149: val_loss improved from 0.07922 to 0.07834, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.0948 - accuracy: 0.9909 - val_loss: 0.0783 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 150/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0894 - accuracy: 0.9933\n","Epoch 150: val_loss did not improve from 0.07834\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0894 - accuracy: 0.9933 - val_loss: 0.0811 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 151/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9942\n","Epoch 151: val_loss did not improve from 0.07834\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0889 - accuracy: 0.9942 - val_loss: 0.0828 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 152/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9943\n","Epoch 152: val_loss improved from 0.07834 to 0.07717, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0869 - accuracy: 0.9943 - val_loss: 0.0772 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 153/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9937\n","Epoch 153: val_loss did not improve from 0.07717\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0861 - accuracy: 0.9937 - val_loss: 0.0791 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 154/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9946\n","Epoch 154: val_loss did not improve from 0.07717\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0849 - accuracy: 0.9946 - val_loss: 0.0791 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 155/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9931\n","Epoch 155: val_loss did not improve from 0.07717\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0869 - accuracy: 0.9931 - val_loss: 0.0789 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 156/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9944\n","Epoch 156: val_loss improved from 0.07717 to 0.07690, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0826 - accuracy: 0.9944 - val_loss: 0.0769 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 157/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9947\n","Epoch 157: val_loss improved from 0.07690 to 0.07525, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0807 - accuracy: 0.9947 - val_loss: 0.0753 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 158/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9945\n","Epoch 158: val_loss improved from 0.07525 to 0.07359, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0816 - accuracy: 0.9945 - val_loss: 0.0736 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 159/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9946\n","Epoch 159: val_loss did not improve from 0.07359\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0807 - accuracy: 0.9946 - val_loss: 0.0738 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 160/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9949\n","Epoch 160: val_loss improved from 0.07359 to 0.07220, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0780 - accuracy: 0.9949 - val_loss: 0.0722 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 161/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9948\n","Epoch 161: val_loss did not improve from 0.07220\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0799 - accuracy: 0.9948 - val_loss: 0.0724 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 162/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9914\n","Epoch 162: val_loss did not improve from 0.07220\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0828 - accuracy: 0.9914 - val_loss: 0.0778 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 163/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9938\n","Epoch 163: val_loss did not improve from 0.07220\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0831 - accuracy: 0.9938 - val_loss: 0.0757 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 164/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9945\n","Epoch 164: val_loss improved from 0.07220 to 0.07079, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0800 - accuracy: 0.9945 - val_loss: 0.0708 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 165/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9948\n","Epoch 165: val_loss improved from 0.07079 to 0.07013, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0759 - accuracy: 0.9948 - val_loss: 0.0701 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 166/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9945\n","Epoch 166: val_loss improved from 0.07013 to 0.06905, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0774 - accuracy: 0.9945 - val_loss: 0.0690 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 167/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9927\n","Epoch 167: val_loss improved from 0.06905 to 0.06640, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0786 - accuracy: 0.9927 - val_loss: 0.0664 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 168/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9947\n","Epoch 168: val_loss did not improve from 0.06640\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0747 - accuracy: 0.9947 - val_loss: 0.0667 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 169/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9950\n","Epoch 169: val_loss did not improve from 0.06640\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0720 - accuracy: 0.9950 - val_loss: 0.0670 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 170/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9951\n","Epoch 170: val_loss did not improve from 0.06640\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0739 - accuracy: 0.9951 - val_loss: 0.0669 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 171/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9952\n","Epoch 171: val_loss did not improve from 0.06640\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0755 - accuracy: 0.9952 - val_loss: 0.0675 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 172/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9953\n","Epoch 172: val_loss improved from 0.06640 to 0.06629, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 398ms/step - loss: 0.0702 - accuracy: 0.9953 - val_loss: 0.0663 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 173/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9954\n","Epoch 173: val_loss improved from 0.06629 to 0.06505, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0703 - accuracy: 0.9954 - val_loss: 0.0651 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 174/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9953\n","Epoch 174: val_loss improved from 0.06505 to 0.06477, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0716 - accuracy: 0.9953 - val_loss: 0.0648 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 175/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9953\n","Epoch 175: val_loss improved from 0.06477 to 0.06404, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0688 - accuracy: 0.9953 - val_loss: 0.0640 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 176/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9940\n","Epoch 176: val_loss did not improve from 0.06404\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0729 - accuracy: 0.9940 - val_loss: 0.0645 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 177/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9950\n","Epoch 177: val_loss did not improve from 0.06404\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0723 - accuracy: 0.9950 - val_loss: 0.0646 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 178/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9951\n","Epoch 178: val_loss improved from 0.06404 to 0.06260, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0670 - accuracy: 0.9951 - val_loss: 0.0626 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 179/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9945\n","Epoch 179: val_loss improved from 0.06260 to 0.06108, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.0704 - accuracy: 0.9945 - val_loss: 0.0611 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 180/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9953\n","Epoch 180: val_loss did not improve from 0.06108\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0680 - accuracy: 0.9953 - val_loss: 0.0628 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 181/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9954\n","Epoch 181: val_loss did not improve from 0.06108\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0683 - accuracy: 0.9954 - val_loss: 0.0617 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 182/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9956\n","Epoch 182: val_loss improved from 0.06108 to 0.06102, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0632 - accuracy: 0.9956 - val_loss: 0.0610 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 183/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9956\n","Epoch 183: val_loss improved from 0.06102 to 0.05919, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0655 - accuracy: 0.9956 - val_loss: 0.0592 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 184/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9955\n","Epoch 184: val_loss did not improve from 0.05919\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0669 - accuracy: 0.9955 - val_loss: 0.0607 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 185/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9953\n","Epoch 185: val_loss improved from 0.05919 to 0.05906, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0656 - accuracy: 0.9953 - val_loss: 0.0591 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 186/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9948\n","Epoch 186: val_loss did not improve from 0.05906\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0639 - accuracy: 0.9948 - val_loss: 0.0605 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 187/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9951\n","Epoch 187: val_loss improved from 0.05906 to 0.05736, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 416ms/step - loss: 0.0640 - accuracy: 0.9951 - val_loss: 0.0574 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 188/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9938\n","Epoch 188: val_loss did not improve from 0.05736\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0680 - accuracy: 0.9938 - val_loss: 0.0605 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 189/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9952\n","Epoch 189: val_loss did not improve from 0.05736\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0650 - accuracy: 0.9952 - val_loss: 0.0584 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 190/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9955\n","Epoch 190: val_loss did not improve from 0.05736\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0610 - accuracy: 0.9955 - val_loss: 0.0577 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 191/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9955\n","Epoch 191: val_loss did not improve from 0.05736\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0636 - accuracy: 0.9955 - val_loss: 0.0574 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 192/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9954\n","Epoch 192: val_loss improved from 0.05736 to 0.05668, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0622 - accuracy: 0.9954 - val_loss: 0.0567 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 193/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9953\n","Epoch 193: val_loss did not improve from 0.05668\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0624 - accuracy: 0.9953 - val_loss: 0.0579 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 194/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9957\n","Epoch 194: val_loss improved from 0.05668 to 0.05536, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0594 - accuracy: 0.9957 - val_loss: 0.0554 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 195/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9956\n","Epoch 195: val_loss improved from 0.05536 to 0.05396, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0573 - accuracy: 0.9956 - val_loss: 0.0540 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 196/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9957\n","Epoch 196: val_loss improved from 0.05396 to 0.05294, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0588 - accuracy: 0.9957 - val_loss: 0.0529 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 197/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9959\n","Epoch 197: val_loss did not improve from 0.05294\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0583 - accuracy: 0.9959 - val_loss: 0.0531 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 198/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9959\n","Epoch 198: val_loss did not improve from 0.05294\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0552 - accuracy: 0.9959 - val_loss: 0.0561 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 199/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9936\n","Epoch 199: val_loss improved from 0.05294 to 0.05225, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 47s 416ms/step - loss: 0.0664 - accuracy: 0.9936 - val_loss: 0.0522 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 200/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0585 - accuracy: 0.9956\n","Epoch 200: val_loss did not improve from 0.05225\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0585 - accuracy: 0.9956 - val_loss: 0.0533 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 201/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9959\n","Epoch 201: val_loss did not improve from 0.05225\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0558 - accuracy: 0.9959 - val_loss: 0.0533 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 202/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9952\n","Epoch 202: val_loss did not improve from 0.05225\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0567 - accuracy: 0.9952 - val_loss: 0.0524 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 203/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9958\n","Epoch 203: val_loss improved from 0.05225 to 0.05185, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0558 - accuracy: 0.9958 - val_loss: 0.0519 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 204/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9959\n","Epoch 204: val_loss improved from 0.05185 to 0.05053, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0552 - accuracy: 0.9959 - val_loss: 0.0505 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 205/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9941\n","Epoch 205: val_loss did not improve from 0.05053\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0568 - accuracy: 0.9941 - val_loss: 0.0529 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 206/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9952\n","Epoch 206: val_loss did not improve from 0.05053\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0580 - accuracy: 0.9952 - val_loss: 0.0512 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 207/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9958\n","Epoch 207: val_loss did not improve from 0.05053\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0531 - accuracy: 0.9958 - val_loss: 0.0509 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 208/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9958\n","Epoch 208: val_loss did not improve from 0.05053\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0559 - accuracy: 0.9958 - val_loss: 0.0510 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 209/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9958\n","Epoch 209: val_loss improved from 0.05053 to 0.04926, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0528 - accuracy: 0.9958 - val_loss: 0.0493 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 210/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9953\n","Epoch 210: val_loss did not improve from 0.04926\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0545 - accuracy: 0.9953 - val_loss: 0.0535 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 211/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9959\n","Epoch 211: val_loss improved from 0.04926 to 0.04731, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0511 - accuracy: 0.9959 - val_loss: 0.0473 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 212/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9958\n","Epoch 212: val_loss did not improve from 0.04731\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0568 - accuracy: 0.9958 - val_loss: 0.0479 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 213/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9956\n","Epoch 213: val_loss did not improve from 0.04731\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0516 - accuracy: 0.9956 - val_loss: 0.0519 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 214/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9958\n","Epoch 214: val_loss improved from 0.04731 to 0.04592, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0529 - accuracy: 0.9958 - val_loss: 0.0459 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 215/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9957\n","Epoch 215: val_loss did not improve from 0.04592\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0518 - accuracy: 0.9957 - val_loss: 0.0472 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 216/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9930\n","Epoch 216: val_loss did not improve from 0.04592\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0582 - accuracy: 0.9930 - val_loss: 0.0536 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 217/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9937\n","Epoch 217: val_loss did not improve from 0.04592\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0580 - accuracy: 0.9937 - val_loss: 0.0489 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 218/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9953\n","Epoch 218: val_loss did not improve from 0.04592\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0532 - accuracy: 0.9953 - val_loss: 0.0477 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 219/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9958\n","Epoch 219: val_loss did not improve from 0.04592\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0516 - accuracy: 0.9958 - val_loss: 0.0489 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 220/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9948\n","Epoch 220: val_loss did not improve from 0.04592\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0533 - accuracy: 0.9948 - val_loss: 0.0475 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 221/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9953\n","Epoch 221: val_loss improved from 0.04592 to 0.04584, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0514 - accuracy: 0.9953 - val_loss: 0.0458 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 222/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9959\n","Epoch 222: val_loss improved from 0.04584 to 0.04413, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0491 - accuracy: 0.9959 - val_loss: 0.0441 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 223/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9960\n","Epoch 223: val_loss did not improve from 0.04413\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0490 - accuracy: 0.9960 - val_loss: 0.0454 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 224/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9961\n","Epoch 224: val_loss improved from 0.04413 to 0.04376, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0490 - accuracy: 0.9961 - val_loss: 0.0438 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 225/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9962\n","Epoch 225: val_loss did not improve from 0.04376\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0503 - accuracy: 0.9962 - val_loss: 0.0441 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 226/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9957\n","Epoch 226: val_loss did not improve from 0.04376\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0505 - accuracy: 0.9957 - val_loss: 0.0442 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 227/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9962\n","Epoch 227: val_loss improved from 0.04376 to 0.04334, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0452 - accuracy: 0.9962 - val_loss: 0.0433 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 228/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9962\n","Epoch 228: val_loss improved from 0.04334 to 0.04244, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0472 - accuracy: 0.9962 - val_loss: 0.0424 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 229/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9962\n","Epoch 229: val_loss improved from 0.04244 to 0.04193, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0455 - accuracy: 0.9962 - val_loss: 0.0419 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 230/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9961\n","Epoch 230: val_loss did not improve from 0.04193\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0488 - accuracy: 0.9961 - val_loss: 0.0421 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 231/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9960\n","Epoch 231: val_loss did not improve from 0.04193\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0510 - accuracy: 0.9960 - val_loss: 0.0450 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 232/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9961\n","Epoch 232: val_loss did not improve from 0.04193\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0470 - accuracy: 0.9961 - val_loss: 0.0426 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 233/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9963\n","Epoch 233: val_loss improved from 0.04193 to 0.04044, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 47s 417ms/step - loss: 0.0411 - accuracy: 0.9963 - val_loss: 0.0404 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 234/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9964\n","Epoch 234: val_loss did not improve from 0.04044\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0437 - accuracy: 0.9964 - val_loss: 0.0409 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 235/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9964\n","Epoch 235: val_loss improved from 0.04044 to 0.04033, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0445 - accuracy: 0.9964 - val_loss: 0.0403 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 236/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9964\n","Epoch 236: val_loss did not improve from 0.04033\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0455 - accuracy: 0.9964 - val_loss: 0.0421 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 237/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9958\n","Epoch 237: val_loss did not improve from 0.04033\n","112/112 [==============================] - 46s 411ms/step - loss: 0.0431 - accuracy: 0.9958 - val_loss: 0.0413 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 238/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9963\n","Epoch 238: val_loss did not improve from 0.04033\n","112/112 [==============================] - 46s 411ms/step - loss: 0.0455 - accuracy: 0.9963 - val_loss: 0.0423 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 239/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9965\n","Epoch 239: val_loss improved from 0.04033 to 0.03975, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 47s 416ms/step - loss: 0.0404 - accuracy: 0.9965 - val_loss: 0.0398 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 240/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9960\n","Epoch 240: val_loss did not improve from 0.03975\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0443 - accuracy: 0.9960 - val_loss: 0.0411 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 241/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9963\n","Epoch 241: val_loss improved from 0.03975 to 0.03869, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 416ms/step - loss: 0.0417 - accuracy: 0.9963 - val_loss: 0.0387 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 242/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9964\n","Epoch 242: val_loss did not improve from 0.03869\n","112/112 [==============================] - 46s 411ms/step - loss: 0.0449 - accuracy: 0.9964 - val_loss: 0.0404 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 243/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9898\n","Epoch 243: val_loss improved from 0.03869 to 0.03844, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0572 - accuracy: 0.9898 - val_loss: 0.0384 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 244/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9948\n","Epoch 244: val_loss did not improve from 0.03844\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0468 - accuracy: 0.9948 - val_loss: 0.0393 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 245/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9958\n","Epoch 245: val_loss did not improve from 0.03844\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0431 - accuracy: 0.9958 - val_loss: 0.0402 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 246/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9953\n","Epoch 246: val_loss did not improve from 0.03844\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0442 - accuracy: 0.9953 - val_loss: 0.0397 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 247/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9957\n","Epoch 247: val_loss did not improve from 0.03844\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0445 - accuracy: 0.9957 - val_loss: 0.0385 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 248/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9962\n","Epoch 248: val_loss improved from 0.03844 to 0.03814, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 400ms/step - loss: 0.0415 - accuracy: 0.9962 - val_loss: 0.0381 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 249/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9961\n","Epoch 249: val_loss did not improve from 0.03814\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0418 - accuracy: 0.9961 - val_loss: 0.0389 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 250/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9963\n","Epoch 250: val_loss improved from 0.03814 to 0.03693, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 47s 416ms/step - loss: 0.0385 - accuracy: 0.9963 - val_loss: 0.0369 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 251/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9964\n","Epoch 251: val_loss did not improve from 0.03693\n","112/112 [==============================] - 46s 411ms/step - loss: 0.0397 - accuracy: 0.9964 - val_loss: 0.0370 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 252/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9963\n","Epoch 252: val_loss did not improve from 0.03693\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0426 - accuracy: 0.9963 - val_loss: 0.0377 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 253/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9965\n","Epoch 253: val_loss improved from 0.03693 to 0.03614, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 399ms/step - loss: 0.0390 - accuracy: 0.9965 - val_loss: 0.0361 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 254/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9965\n","Epoch 254: val_loss did not improve from 0.03614\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0392 - accuracy: 0.9965 - val_loss: 0.0363 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 255/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9964\n","Epoch 255: val_loss improved from 0.03614 to 0.03566, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0385 - accuracy: 0.9964 - val_loss: 0.0357 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 256/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9966\n","Epoch 256: val_loss improved from 0.03566 to 0.03481, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 398ms/step - loss: 0.0380 - accuracy: 0.9966 - val_loss: 0.0348 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 257/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9966\n","Epoch 257: val_loss did not improve from 0.03481\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0368 - accuracy: 0.9966 - val_loss: 0.0359 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 258/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9966\n","Epoch 258: val_loss improved from 0.03481 to 0.03362, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 398ms/step - loss: 0.0379 - accuracy: 0.9966 - val_loss: 0.0336 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 259/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9963\n","Epoch 259: val_loss did not improve from 0.03362\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0392 - accuracy: 0.9963 - val_loss: 0.0356 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 260/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9960\n","Epoch 260: val_loss did not improve from 0.03362\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0381 - accuracy: 0.9960 - val_loss: 0.0352 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 261/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9965\n","Epoch 261: val_loss did not improve from 0.03362\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0386 - accuracy: 0.9965 - val_loss: 0.0388 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 262/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9961\n","Epoch 262: val_loss did not improve from 0.03362\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0397 - accuracy: 0.9961 - val_loss: 0.0429 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 263/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9963\n","Epoch 263: val_loss did not improve from 0.03362\n","112/112 [==============================] - 46s 411ms/step - loss: 0.0395 - accuracy: 0.9963 - val_loss: 0.0340 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 264/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9964\n","Epoch 264: val_loss did not improve from 0.03362\n","112/112 [==============================] - 44s 394ms/step - loss: 0.0367 - accuracy: 0.9964 - val_loss: 0.0343 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 265/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9964\n","Epoch 265: val_loss improved from 0.03362 to 0.03315, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 398ms/step - loss: 0.0377 - accuracy: 0.9964 - val_loss: 0.0332 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 266/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9967\n","Epoch 266: val_loss did not improve from 0.03315\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0358 - accuracy: 0.9967 - val_loss: 0.0332 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 267/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9967\n","Epoch 267: val_loss improved from 0.03315 to 0.03190, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0341 - accuracy: 0.9967 - val_loss: 0.0319 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 268/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9956\n","Epoch 268: val_loss did not improve from 0.03190\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0359 - accuracy: 0.9956 - val_loss: 0.0333 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 269/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9963\n","Epoch 269: val_loss did not improve from 0.03190\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0340 - accuracy: 0.9963 - val_loss: 0.0357 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 270/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9941\n","Epoch 270: val_loss did not improve from 0.03190\n","112/112 [==============================] - 46s 411ms/step - loss: 0.0419 - accuracy: 0.9941 - val_loss: 0.0326 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 271/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9960\n","Epoch 271: val_loss did not improve from 0.03190\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0361 - accuracy: 0.9960 - val_loss: 0.0323 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 272/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9964\n","Epoch 272: val_loss did not improve from 0.03190\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0346 - accuracy: 0.9964 - val_loss: 0.0325 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 273/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9965\n","Epoch 273: val_loss did not improve from 0.03190\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0354 - accuracy: 0.9965 - val_loss: 0.0325 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 274/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9965\n","Epoch 274: val_loss did not improve from 0.03190\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0360 - accuracy: 0.9965 - val_loss: 0.0328 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 275/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9966\n","Epoch 275: val_loss improved from 0.03190 to 0.03154, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0338 - accuracy: 0.9966 - val_loss: 0.0315 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 276/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9968\n","Epoch 276: val_loss did not improve from 0.03154\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0340 - accuracy: 0.9968 - val_loss: 0.0317 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 277/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9968\n","Epoch 277: val_loss did not improve from 0.03154\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0329 - accuracy: 0.9968 - val_loss: 0.0319 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 278/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9965\n","Epoch 278: val_loss did not improve from 0.03154\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0340 - accuracy: 0.9965 - val_loss: 0.0316 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 279/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9967\n","Epoch 279: val_loss improved from 0.03154 to 0.03106, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.0343 - accuracy: 0.9967 - val_loss: 0.0311 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 280/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9968\n","Epoch 280: val_loss improved from 0.03106 to 0.03078, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.0339 - accuracy: 0.9968 - val_loss: 0.0308 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 281/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9969\n","Epoch 281: val_loss improved from 0.03078 to 0.03006, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.0324 - accuracy: 0.9969 - val_loss: 0.0301 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 282/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9968\n","Epoch 282: val_loss improved from 0.03006 to 0.02992, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0314 - accuracy: 0.9968 - val_loss: 0.0299 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 283/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9966\n","Epoch 283: val_loss improved from 0.02992 to 0.02945, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0320 - accuracy: 0.9966 - val_loss: 0.0294 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 284/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9955\n","Epoch 284: val_loss did not improve from 0.02945\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0360 - accuracy: 0.9955 - val_loss: 0.0503 - val_accuracy: 0.9868 - lr: 1.0000e-04\n","Epoch 285/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9961\n","Epoch 285: val_loss did not improve from 0.02945\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0345 - accuracy: 0.9961 - val_loss: 0.0318 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 286/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9968\n","Epoch 286: val_loss did not improve from 0.02945\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0309 - accuracy: 0.9968 - val_loss: 0.0300 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 287/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9970\n","Epoch 287: val_loss did not improve from 0.02945\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0301 - accuracy: 0.9970 - val_loss: 0.0295 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 288/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9968\n","Epoch 288: val_loss did not improve from 0.02945\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0317 - accuracy: 0.9968 - val_loss: 0.0304 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 289/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9955\n","Epoch 289: val_loss did not improve from 0.02945\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0350 - accuracy: 0.9955 - val_loss: 0.0300 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 290/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9967\n","Epoch 290: val_loss improved from 0.02945 to 0.02886, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0314 - accuracy: 0.9967 - val_loss: 0.0289 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 291/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9968\n","Epoch 291: val_loss improved from 0.02886 to 0.02883, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0307 - accuracy: 0.9968 - val_loss: 0.0288 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 292/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9968\n","Epoch 292: val_loss did not improve from 0.02883\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0321 - accuracy: 0.9968 - val_loss: 0.0290 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 293/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9969\n","Epoch 293: val_loss improved from 0.02883 to 0.02811, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0310 - accuracy: 0.9969 - val_loss: 0.0281 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 294/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9970\n","Epoch 294: val_loss improved from 0.02811 to 0.02747, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0314 - accuracy: 0.9970 - val_loss: 0.0275 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 295/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9970\n","Epoch 295: val_loss did not improve from 0.02747\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0290 - accuracy: 0.9970 - val_loss: 0.0277 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 296/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9971\n","Epoch 296: val_loss did not improve from 0.02747\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0282 - accuracy: 0.9971 - val_loss: 0.0281 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 297/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9968\n","Epoch 297: val_loss did not improve from 0.02747\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0302 - accuracy: 0.9968 - val_loss: 0.0282 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 298/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9960\n","Epoch 298: val_loss did not improve from 0.02747\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0329 - accuracy: 0.9960 - val_loss: 0.0291 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 299/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9968\n","Epoch 299: val_loss improved from 0.02747 to 0.02743, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0288 - accuracy: 0.9968 - val_loss: 0.0274 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 300/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9969\n","Epoch 300: val_loss did not improve from 0.02743\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0292 - accuracy: 0.9969 - val_loss: 0.0284 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 301/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9969\n","Epoch 301: val_loss improved from 0.02743 to 0.02579, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0281 - accuracy: 0.9969 - val_loss: 0.0258 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 302/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9971\n","Epoch 302: val_loss did not improve from 0.02579\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0288 - accuracy: 0.9971 - val_loss: 0.0278 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 303/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9968\n","Epoch 303: val_loss did not improve from 0.02579\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0291 - accuracy: 0.9968 - val_loss: 0.0280 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 304/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9971\n","Epoch 304: val_loss did not improve from 0.02579\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0287 - accuracy: 0.9971 - val_loss: 0.0266 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 305/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9970\n","Epoch 305: val_loss did not improve from 0.02579\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0267 - accuracy: 0.9970 - val_loss: 0.0279 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 306/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9968\n","Epoch 306: val_loss did not improve from 0.02579\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0287 - accuracy: 0.9968 - val_loss: 0.0286 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 307/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9969\n","Epoch 307: val_loss did not improve from 0.02579\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0283 - accuracy: 0.9969 - val_loss: 0.0264 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 308/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9969\n","Epoch 308: val_loss improved from 0.02579 to 0.02576, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0306 - accuracy: 0.9969 - val_loss: 0.0258 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 309/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9972\n","Epoch 309: val_loss improved from 0.02576 to 0.02465, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0266 - accuracy: 0.9972 - val_loss: 0.0247 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 310/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9971\n","Epoch 310: val_loss improved from 0.02465 to 0.02449, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0257 - accuracy: 0.9971 - val_loss: 0.0245 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 311/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9973\n","Epoch 311: val_loss did not improve from 0.02449\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0274 - accuracy: 0.9973 - val_loss: 0.0256 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 312/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9971\n","Epoch 312: val_loss did not improve from 0.02449\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0260 - accuracy: 0.9971 - val_loss: 0.0276 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 313/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9950\n","Epoch 313: val_loss did not improve from 0.02449\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0311 - accuracy: 0.9950 - val_loss: 0.0719 - val_accuracy: 0.9779 - lr: 1.0000e-04\n","Epoch 314/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9959\n","Epoch 314: val_loss did not improve from 0.02449\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0298 - accuracy: 0.9959 - val_loss: 0.0320 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 315/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9946\n","Epoch 315: val_loss did not improve from 0.02449\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0329 - accuracy: 0.9946 - val_loss: 0.0299 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 316/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9967\n","Epoch 316: val_loss did not improve from 0.02449\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0260 - accuracy: 0.9967 - val_loss: 0.0267 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 317/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9968\n","Epoch 317: val_loss did not improve from 0.02449\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0271 - accuracy: 0.9968 - val_loss: 0.0249 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 318/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9967\n","Epoch 318: val_loss did not improve from 0.02449\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0264 - accuracy: 0.9967 - val_loss: 0.0246 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 319/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9971\n","Epoch 319: val_loss did not improve from 0.02449\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0251 - accuracy: 0.9971 - val_loss: 0.0248 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 320/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9971\n","Epoch 320: val_loss improved from 0.02449 to 0.02431, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0263 - accuracy: 0.9971 - val_loss: 0.0243 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 321/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9971\n","Epoch 321: val_loss did not improve from 0.02431\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0274 - accuracy: 0.9971 - val_loss: 0.0259 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 322/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9973\n","Epoch 322: val_loss did not improve from 0.02431\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0242 - accuracy: 0.9973 - val_loss: 0.0244 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 323/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9969\n","Epoch 323: val_loss did not improve from 0.02431\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0258 - accuracy: 0.9969 - val_loss: 0.0273 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 324/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9941\n","Epoch 324: val_loss did not improve from 0.02431\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0346 - accuracy: 0.9941 - val_loss: 0.0269 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 325/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9959\n","Epoch 325: val_loss did not improve from 0.02431\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0267 - accuracy: 0.9959 - val_loss: 0.0252 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 326/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9966\n","Epoch 326: val_loss did not improve from 0.02431\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0252 - accuracy: 0.9966 - val_loss: 0.0278 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 327/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9938\n","Epoch 327: val_loss did not improve from 0.02431\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0349 - accuracy: 0.9938 - val_loss: 0.0281 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 328/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9962\n","Epoch 328: val_loss did not improve from 0.02431\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0253 - accuracy: 0.9962 - val_loss: 0.0246 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 329/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9964\n","Epoch 329: val_loss improved from 0.02431 to 0.02386, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 47s 417ms/step - loss: 0.0275 - accuracy: 0.9964 - val_loss: 0.0239 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 330/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9968\n","Epoch 330: val_loss improved from 0.02386 to 0.02316, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 398ms/step - loss: 0.0239 - accuracy: 0.9968 - val_loss: 0.0232 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 331/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9969\n","Epoch 331: val_loss did not improve from 0.02316\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0253 - accuracy: 0.9969 - val_loss: 0.0246 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 332/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9970\n","Epoch 332: val_loss did not improve from 0.02316\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0240 - accuracy: 0.9970 - val_loss: 0.0239 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 333/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9970\n","Epoch 333: val_loss did not improve from 0.02316\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0244 - accuracy: 0.9970 - val_loss: 0.0248 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 334/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9970\n","Epoch 334: val_loss did not improve from 0.02316\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0263 - accuracy: 0.9970 - val_loss: 0.0237 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 335/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9964\n","Epoch 335: val_loss improved from 0.02316 to 0.02316, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.0257 - accuracy: 0.9964 - val_loss: 0.0232 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 336/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9970\n","Epoch 336: val_loss improved from 0.02316 to 0.02242, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 47s 416ms/step - loss: 0.0255 - accuracy: 0.9970 - val_loss: 0.0224 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 337/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9972\n","Epoch 337: val_loss did not improve from 0.02242\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0225 - accuracy: 0.9972 - val_loss: 0.0226 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 338/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9966\n","Epoch 338: val_loss did not improve from 0.02242\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0247 - accuracy: 0.9966 - val_loss: 0.0430 - val_accuracy: 0.9873 - lr: 1.0000e-04\n","Epoch 339/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9968\n","Epoch 339: val_loss did not improve from 0.02242\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0227 - accuracy: 0.9968 - val_loss: 0.0243 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 340/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9971\n","Epoch 340: val_loss did not improve from 0.02242\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0237 - accuracy: 0.9971 - val_loss: 0.0248 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 341/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9971\n","Epoch 341: val_loss did not improve from 0.02242\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0229 - accuracy: 0.9971 - val_loss: 0.0259 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 342/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9969\n","Epoch 342: val_loss did not improve from 0.02242\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0254 - accuracy: 0.9969 - val_loss: 0.0243 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 343/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9972\n","Epoch 343: val_loss did not improve from 0.02242\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0231 - accuracy: 0.9972 - val_loss: 0.0227 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 344/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9971\n","Epoch 344: val_loss did not improve from 0.02242\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0237 - accuracy: 0.9971 - val_loss: 0.0227 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 345/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9973\n","Epoch 345: val_loss did not improve from 0.02242\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0226 - accuracy: 0.9973 - val_loss: 0.0227 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 346/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9974\n","Epoch 346: val_loss improved from 0.02242 to 0.02212, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0229 - accuracy: 0.9974 - val_loss: 0.0221 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 347/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9975\n","Epoch 347: val_loss improved from 0.02212 to 0.02090, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0219 - accuracy: 0.9975 - val_loss: 0.0209 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 348/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9974\n","Epoch 348: val_loss improved from 0.02090 to 0.02050, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0208 - accuracy: 0.9974 - val_loss: 0.0205 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 349/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9975\n","Epoch 349: val_loss did not improve from 0.02050\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0202 - accuracy: 0.9975 - val_loss: 0.0209 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 350/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9975\n","Epoch 350: val_loss did not improve from 0.02050\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0205 - accuracy: 0.9975 - val_loss: 0.0216 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 351/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9975\n","Epoch 351: val_loss did not improve from 0.02050\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0201 - accuracy: 0.9975 - val_loss: 0.0209 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 352/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9975\n","Epoch 352: val_loss did not improve from 0.02050\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0214 - accuracy: 0.9975 - val_loss: 0.0206 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 353/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9976\n","Epoch 353: val_loss did not improve from 0.02050\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0204 - accuracy: 0.9976 - val_loss: 0.0212 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 354/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9975\n","Epoch 354: val_loss did not improve from 0.02050\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0220 - accuracy: 0.9975 - val_loss: 0.0211 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 355/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9975\n","Epoch 355: val_loss did not improve from 0.02050\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0220 - accuracy: 0.9975 - val_loss: 0.0214 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 356/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9971\n","Epoch 356: val_loss improved from 0.02050 to 0.02025, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 398ms/step - loss: 0.0215 - accuracy: 0.9971 - val_loss: 0.0202 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 357/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9974\n","Epoch 357: val_loss did not improve from 0.02025\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0225 - accuracy: 0.9974 - val_loss: 0.0248 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 358/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9923\n","Epoch 358: val_loss did not improve from 0.02025\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0342 - accuracy: 0.9923 - val_loss: 0.0226 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 359/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9966\n","Epoch 359: val_loss did not improve from 0.02025\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0210 - accuracy: 0.9966 - val_loss: 0.0215 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 360/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9970\n","Epoch 360: val_loss did not improve from 0.02025\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0216 - accuracy: 0.9970 - val_loss: 0.0209 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 361/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9972\n","Epoch 361: val_loss did not improve from 0.02025\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0217 - accuracy: 0.9972 - val_loss: 0.0205 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 362/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9973\n","Epoch 362: val_loss did not improve from 0.02025\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0202 - accuracy: 0.9973 - val_loss: 0.0206 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 363/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9969\n","Epoch 363: val_loss did not improve from 0.02025\n","112/112 [==============================] - 46s 411ms/step - loss: 0.0213 - accuracy: 0.9969 - val_loss: 0.0205 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 364/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9972\n","Epoch 364: val_loss improved from 0.02025 to 0.01905, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0204 - accuracy: 0.9972 - val_loss: 0.0191 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 365/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9968\n","Epoch 365: val_loss did not improve from 0.01905\n","112/112 [==============================] - 46s 411ms/step - loss: 0.0223 - accuracy: 0.9968 - val_loss: 0.0198 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 366/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9971\n","Epoch 366: val_loss did not improve from 0.01905\n","112/112 [==============================] - 46s 411ms/step - loss: 0.0210 - accuracy: 0.9971 - val_loss: 0.0198 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 367/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9974\n","Epoch 367: val_loss did not improve from 0.01905\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0198 - accuracy: 0.9974 - val_loss: 0.0197 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 368/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9975\n","Epoch 368: val_loss did not improve from 0.01905\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0202 - accuracy: 0.9975 - val_loss: 0.0196 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 369/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9975\n","Epoch 369: val_loss did not improve from 0.01905\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0199 - accuracy: 0.9975 - val_loss: 0.0196 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 370/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9976\n","Epoch 370: val_loss did not improve from 0.01905\n","112/112 [==============================] - 46s 411ms/step - loss: 0.0189 - accuracy: 0.9976 - val_loss: 0.0201 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 371/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9968\n","Epoch 371: val_loss did not improve from 0.01905\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0204 - accuracy: 0.9968 - val_loss: 0.0195 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 372/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9974\n","Epoch 372: val_loss improved from 0.01905 to 0.01904, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 398ms/step - loss: 0.0200 - accuracy: 0.9974 - val_loss: 0.0190 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 373/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9966\n","Epoch 373: val_loss did not improve from 0.01904\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0214 - accuracy: 0.9966 - val_loss: 0.0201 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 374/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9964\n","Epoch 374: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n","\n","Epoch 374: val_loss did not improve from 0.01904\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0208 - accuracy: 0.9964 - val_loss: 0.0194 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 375/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9975\n","Epoch 375: val_loss improved from 0.01904 to 0.01898, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 398ms/step - loss: 0.0213 - accuracy: 0.9975 - val_loss: 0.0190 - val_accuracy: 0.9963 - lr: 1.0000e-05\n","Epoch 376/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9969\n","Epoch 376: val_loss improved from 0.01898 to 0.01896, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 398ms/step - loss: 0.0195 - accuracy: 0.9969 - val_loss: 0.0190 - val_accuracy: 0.9963 - lr: 1.0000e-05\n","Epoch 377/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9976\n","Epoch 377: val_loss improved from 0.01896 to 0.01842, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0179 - accuracy: 0.9976 - val_loss: 0.0184 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 378/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9976\n","Epoch 378: val_loss did not improve from 0.01842\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0187 - accuracy: 0.9976 - val_loss: 0.0185 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 379/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9974\n","Epoch 379: val_loss did not improve from 0.01842\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0186 - accuracy: 0.9974 - val_loss: 0.0187 - val_accuracy: 0.9963 - lr: 1.0000e-05\n","Epoch 380/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9976\n","Epoch 380: val_loss improved from 0.01842 to 0.01838, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0184 - accuracy: 0.9976 - val_loss: 0.0184 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 381/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9976\n","Epoch 381: val_loss improved from 0.01838 to 0.01828, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0184 - accuracy: 0.9976 - val_loss: 0.0183 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 382/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9976\n","Epoch 382: val_loss did not improve from 0.01828\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0203 - accuracy: 0.9976 - val_loss: 0.0184 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 383/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9976\n","Epoch 383: val_loss did not improve from 0.01828\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0192 - accuracy: 0.9976 - val_loss: 0.0186 - val_accuracy: 0.9963 - lr: 1.0000e-05\n","Epoch 384/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9977\n","Epoch 384: val_loss did not improve from 0.01828\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0185 - accuracy: 0.9977 - val_loss: 0.0185 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 385/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9977\n","Epoch 385: val_loss improved from 0.01828 to 0.01826, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 45s 399ms/step - loss: 0.0181 - accuracy: 0.9977 - val_loss: 0.0183 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 386/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9977\n","Epoch 386: val_loss did not improve from 0.01826\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0187 - accuracy: 0.9977 - val_loss: 0.0183 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 387/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9976\n","Epoch 387: val_loss improved from 0.01826 to 0.01804, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.0176 - accuracy: 0.9976 - val_loss: 0.0180 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 388/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9977\n","Epoch 388: val_loss did not improve from 0.01804\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0198 - accuracy: 0.9977 - val_loss: 0.0183 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 389/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9977\n","Epoch 389: val_loss did not improve from 0.01804\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0185 - accuracy: 0.9977 - val_loss: 0.0183 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 390/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9977\n","Epoch 390: val_loss did not improve from 0.01804\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0187 - accuracy: 0.9977 - val_loss: 0.0183 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 391/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9977\n","Epoch 391: val_loss did not improve from 0.01804\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0192 - accuracy: 0.9977 - val_loss: 0.0183 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 392/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9977\n","Epoch 392: val_loss did not improve from 0.01804\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0193 - accuracy: 0.9977 - val_loss: 0.0184 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 393/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9977\n","Epoch 393: val_loss did not improve from 0.01804\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0189 - accuracy: 0.9977 - val_loss: 0.0183 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 394/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9978\n","Epoch 394: val_loss did not improve from 0.01804\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0174 - accuracy: 0.9978 - val_loss: 0.0181 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 395/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9978\n","Epoch 395: val_loss improved from 0.01804 to 0.01802, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.0188 - accuracy: 0.9978 - val_loss: 0.0180 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 396/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9978\n","Epoch 396: val_loss did not improve from 0.01802\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0179 - accuracy: 0.9978 - val_loss: 0.0182 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 397/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9978\n","Epoch 397: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n","\n","Epoch 397: val_loss did not improve from 0.01802\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0191 - accuracy: 0.9978 - val_loss: 0.0185 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 398/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9978\n","Epoch 398: val_loss did not improve from 0.01802\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0184 - accuracy: 0.9978 - val_loss: 0.0183 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 399/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9978\n","Epoch 399: val_loss did not improve from 0.01802\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0182 - accuracy: 0.9978 - val_loss: 0.0182 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 400/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9977\n","Epoch 400: val_loss did not improve from 0.01802\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0189 - accuracy: 0.9977 - val_loss: 0.0184 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 401/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9978\n","Epoch 401: val_loss did not improve from 0.01802\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0174 - accuracy: 0.9978 - val_loss: 0.0183 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 402/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9978\n","Epoch 402: val_loss improved from 0.01802 to 0.01787, saving model to unet3_hybrid.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0174 - accuracy: 0.9978 - val_loss: 0.0179 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 403/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9978\n","Epoch 403: val_loss did not improve from 0.01787\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0177 - accuracy: 0.9978 - val_loss: 0.0180 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 404/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9977\n","Epoch 404: val_loss did not improve from 0.01787\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0189 - accuracy: 0.9977 - val_loss: 0.0182 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 405/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9978\n","Epoch 405: val_loss did not improve from 0.01787\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0194 - accuracy: 0.9978 - val_loss: 0.0182 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 406/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9978\n","Epoch 406: val_loss did not improve from 0.01787\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0183 - accuracy: 0.9978 - val_loss: 0.0181 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 407/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9978\n","Epoch 407: val_loss did not improve from 0.01787\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0181 - accuracy: 0.9978 - val_loss: 0.0180 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 408/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9978\n","Epoch 408: val_loss did not improve from 0.01787\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0173 - accuracy: 0.9978 - val_loss: 0.0179 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 409/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9968\n","Epoch 409: val_loss did not improve from 0.01787\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0214 - accuracy: 0.9968 - val_loss: 0.0182 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 410/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9978\n","Epoch 410: val_loss did not improve from 0.01787\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0177 - accuracy: 0.9978 - val_loss: 0.0179 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 411/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9978\n","Epoch 411: val_loss did not improve from 0.01787\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0191 - accuracy: 0.9978 - val_loss: 0.0181 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 412/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9978\n","Epoch 412: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n","\n","Epoch 412: val_loss did not improve from 0.01787\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0193 - accuracy: 0.9978 - val_loss: 0.0185 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 413/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9978\n","Epoch 413: val_loss did not improve from 0.01787\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0175 - accuracy: 0.9978 - val_loss: 0.0183 - val_accuracy: 0.9965 - lr: 1.0000e-07\n","Epoch 414/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9978\n","Epoch 414: val_loss did not improve from 0.01787\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0177 - accuracy: 0.9978 - val_loss: 0.0180 - val_accuracy: 0.9965 - lr: 1.0000e-07\n","Epoch 415/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9977\n","Epoch 415: val_loss did not improve from 0.01787\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0183 - accuracy: 0.9977 - val_loss: 0.0182 - val_accuracy: 0.9965 - lr: 1.0000e-07\n","Epoch 416/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9978\n","Epoch 416: val_loss did not improve from 0.01787\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0178 - accuracy: 0.9978 - val_loss: 0.0180 - val_accuracy: 0.9965 - lr: 1.0000e-07\n","Epoch 417/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9978\n","Epoch 417: val_loss did not improve from 0.01787\n","112/112 [==============================] - 46s 410ms/step - loss: 0.0176 - accuracy: 0.9978 - val_loss: 0.0179 - val_accuracy: 0.9965 - lr: 1.0000e-07\n","Epoch 417: early stopping\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7ff86d07e310>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from keras.callbacks import ReduceLROnPlateau\n","reduce_lr=ReduceLROnPlateau(monitor='val_loss',\n","                         factor=0.1,\n","                         patience=10,\n","                         verbose=1,\n","                         mode='auto',\n","                         min_delta=0.00003,\n","                         cooldown=0,\n","                         min_lr=0)\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15,verbose=1,mode='min')\n","save_model= ModelCheckpoint('unet3_hybrid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","unet.fit(images, masks, validation_data=(val_images,val_masks), batch_size=8, epochs=1000,verbose=1,shuffle=True,callbacks=[reduce_lr,save_model,early_stop])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T00:37:04.000593Z","iopub.status.busy":"2023-04-04T00:37:03.999971Z","iopub.status.idle":"2023-04-04T00:37:04.008566Z","shell.execute_reply":"2023-04-04T00:37:04.007333Z","shell.execute_reply.started":"2023-04-04T00:37:04.000548Z"},"trusted":true},"outputs":[],"source":["np.save('unet3_bybrid-history.npy',unet.history.history)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T00:37:07.805700Z","iopub.status.busy":"2023-04-04T00:37:07.804967Z","iopub.status.idle":"2023-04-04T00:37:07.812629Z","shell.execute_reply":"2023-04-04T00:37:07.811412Z","shell.execute_reply.started":"2023-04-04T00:37:07.805554Z"},"trusted":true},"outputs":[],"source":["model_history = np.load('unet3_bybrid-history.npy', allow_pickle='TRUE').item()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T00:37:11.090325Z","iopub.status.busy":"2023-04-04T00:37:11.089721Z","iopub.status.idle":"2023-04-04T00:37:11.574927Z","shell.execute_reply":"2023-04-04T00:37:11.573598Z","shell.execute_reply.started":"2023-04-04T00:37:11.090266Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<matplotlib.legend.Legend at 0x7ff86576de10>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#@title Plot accuracy and loss \n","from matplotlib import pyplot as plt\n","## Accuracy\n","plt.plot(model_history['accuracy'])\n","plt.plot(model_history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')\n","plt.show()\n","\n","## Loss\n","plt.plot(model_history['loss'])\n","plt.plot(model_history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T00:37:47.770679Z","iopub.status.busy":"2023-04-04T00:37:47.769564Z","iopub.status.idle":"2023-04-04T00:38:11.477344Z","shell.execute_reply":"2023-04-04T00:38:11.476136Z","shell.execute_reply.started":"2023-04-04T00:37:47.770643Z"},"trusted":true},"outputs":[],"source":["## Test images\n","base_dir=r'/content/gdrive/MyDrive/mudtest/'\n","test_images_list = os.listdir(r\"{}images/\".format(base_dir))\n","test_masks_list = []\n","test_images = []\n","for n in test_images_list:\n","    test_masks_list.append(n)\n","    a = (np.array(rxr.open_rasterio(r\"{}/images/{}\".format(base_dir,n))))\n","    a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","    test_images.append(a)\n","            \n","## Test masks\n","test_masks = []\n","for n in test_masks_list:\n","    a = (np.array(rxr.open_rasterio(r\"{}/labels/{}\".format(base_dir,n))))\n","    test_masks.append(a)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T00:38:22.307622Z","iopub.status.busy":"2023-04-04T00:38:22.306399Z","iopub.status.idle":"2023-04-04T00:38:22.465321Z","shell.execute_reply":"2023-04-04T00:38:22.464236Z","shell.execute_reply.started":"2023-04-04T00:38:22.307563Z"},"trusted":true},"outputs":[],"source":["for i in range(len(test_images)):\n","  test_images[i] = test_images[i].astype('float32')\n","  test_images[i] = test_images[i].T\n","\n","for i in range(len(test_masks)):\n","  test_masks[i] = test_masks[i].reshape(1,256,256,1)\n","  test_masks[i] = test_masks[i].T\n","\n","for i in range(len(test_images)):\n","  test_images[i] = test_images[i].reshape(-1,256,256,10)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T00:38:24.994810Z","iopub.status.busy":"2023-04-04T00:38:24.994091Z","iopub.status.idle":"2023-04-04T00:38:25.002030Z","shell.execute_reply":"2023-04-04T00:38:25.001002Z","shell.execute_reply.started":"2023-04-04T00:38:24.994772Z"},"trusted":true},"outputs":[],"source":["#@title Returns an image or array plot of mask prediction\n","\n","def reconstruct_image(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","  # Standardise between 0-1\n","  reconstruction = reconstruction/np.max(reconstruction)\n","\n","  # Round to 0-1, binary pixel-by-pixel classification \n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  # Plot reconstructed mask (prediction)\n","  plt.imshow(reconstruction) \n","'''\n","  Returns array of mask prediction, given model and image\n","'''\n","def reconstruct_array(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","\n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  return reconstruction # Returns array"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T00:38:28.315066Z","iopub.status.busy":"2023-04-04T00:38:28.314708Z","iopub.status.idle":"2023-04-04T00:38:28.334267Z","shell.execute_reply":"2023-04-04T00:38:28.333160Z","shell.execute_reply.started":"2023-04-04T00:38:28.315035Z"},"trusted":true},"outputs":[],"source":["#@title Metric functions for evaluation\n","\n","def score_eval(model, image, mask): # Gives score of mask vs prediction\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","    return accuracy_score(mask.flatten(), reconstruction)\n","\n","  else: # If a list of images input, find accuracy for each\n","    scores = []\n","    for i in range(len(image)):\n","      reconstruction = model.predict(image[i].reshape(1, 256, 256, 10))\n","      reconstruction = np.round(reconstruction).flatten()\n","      scores.append(accuracy_score(mask[i].flatten(), reconstruction))\n","\n","    return scores\n","\n","def recall_eval(model, image, mask): # Find recall score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return recall_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    recall = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        recall.append(recall_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return recall\n","\n","def precision_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return precision_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    precision = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        precision.append(precision_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return precision\n","\n","def iou_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return jaccard_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    iou = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        iou.append(jaccard_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return iou\n","\n","def f1_score_eval(model, image, mask): # Find F1-score\n","    prec = np.mean(precision_eval(model, image, mask))\n","    rec = np.mean(recall_eval(model, image, mask))\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def f1_score_eval_basic(precision, recall):\n","    prec = np.mean(precision)\n","    rec = np.mean(recall)\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def produce_mask(image): # Outputs rounded image (binary)\n","  return np.round(image)\n"," \n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T00:38:33.960192Z","iopub.status.busy":"2023-04-04T00:38:33.959100Z","iopub.status.idle":"2023-04-04T00:40:15.323192Z","shell.execute_reply":"2023-04-04T00:40:15.322229Z","shell.execute_reply.started":"2023-04-04T00:38:33.960142Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 727ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 71ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 53ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n"]}],"source":["\n","score = (score_eval(unet, test_images, test_masks))\n","\n","precision = (precision_eval(unet, test_images, test_masks))\n","\n","recall = (recall_eval(unet, test_images, test_masks))\n","\n","iou = (iou_eval(unet, test_images, test_masks))"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T00:40:22.580453Z","iopub.status.busy":"2023-04-04T00:40:22.579503Z","iopub.status.idle":"2023-04-04T00:40:22.586297Z","shell.execute_reply":"2023-04-04T00:40:22.584732Z","shell.execute_reply.started":"2023-04-04T00:40:22.580398Z"},"trusted":true},"outputs":[],"source":["f1_score = (f1_score_eval_basic(precision, recall))"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T00:40:25.730136Z","iopub.status.busy":"2023-04-04T00:40:25.729154Z","iopub.status.idle":"2023-04-04T00:40:25.738630Z","shell.execute_reply":"2023-04-04T00:40:25.737331Z","shell.execute_reply.started":"2023-04-04T00:40:25.730098Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model accuracy:  0.9815479938248942 0.020087597491106245\n","model precision:  0.9442662996113277 0.11089208343673809\n","model recall:  0.974992251372461 0.03653017926958792\n","model F1-score:  0.9593833252755611\n","model iou:  0.9220601264537089 0.11325616352843819\n"]}],"source":["print('model accuracy: ', np.mean(score), np.std(score))\n","print('model precision: ', np.mean(precision), np.std(precision))\n","print('model recall: ', np.mean(recall), np.std(recall))\n","print('model F1-score: ', np.mean(f1_score))\n","print('model iou: ', np.mean(iou), np.std(iou))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
