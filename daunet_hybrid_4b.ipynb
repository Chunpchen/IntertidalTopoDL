{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-06T11:00:17.383240Z","iopub.status.busy":"2023-04-06T11:00:17.382956Z","iopub.status.idle":"2023-04-06T11:00:54.502568Z","shell.execute_reply":"2023-04-06T11:00:54.501518Z","shell.execute_reply.started":"2023-04-06T11:00:17.383212Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/davej23/image-segmentation-keras.git\n","  Cloning https://github.com/davej23/image-segmentation-keras.git to /tmp/pip-req-build-k9m_3lb9\n","  Running command git clone --filter=blob:none --quiet https://github.com/davej23/image-segmentation-keras.git /tmp/pip-req-build-k9m_3lb9\n","  Resolved https://github.com/davej23/image-segmentation-keras.git to commit e01b0a8d5859854cd9d259a618829889166439f5\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting rarfile\n","  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n","Collecting segmentation-models\n","  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n","Collecting rioxarray\n","  Downloading rioxarray-0.9.1.tar.gz (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting efficientnet==1.0.0\n","  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n","Collecting keras-applications<=1.0.8,>=1.0.7\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting image-classifiers==1.0.0\n","  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n","Collecting h5py<=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: Keras>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (2.11.0)\n","Collecting imageio==2.5.0\n","  Downloading imageio-2.5.0-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (0.4.0)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.5.4.60)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.64.1)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (9.4.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (1.21.6)\n","Requirement already satisfied: xarray>=0.17 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (0.20.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from rioxarray) (23.0)\n","Requirement already satisfied: rasterio in /opt/conda/lib/python3.7/site-packages (from rioxarray) (1.2.10)\n","Requirement already satisfied: pyproj>=2.2 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (3.1.0)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py<=2.10.0->keras-segmentation==0.3.0) (1.16.0)\n","Requirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.8.0)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.7.3)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (3.5.3)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from pyproj>=2.2->rioxarray) (2022.12.7)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.11.4)\n","Requirement already satisfied: pandas>=1.1 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (1.3.5)\n","Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.4.0)\n","Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (0.7.2)\n","Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (22.2.0)\n","Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (8.1.3)\n","Requirement already satisfied: click-plugins in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.1.1)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (59.8.0)\n","Requirement already satisfied: affine in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (2.4.0)\n","Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.4.7)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2023.2)\n","Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.11.2)\n","Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio->rioxarray) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->xarray>=0.17->rioxarray) (3.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (4.38.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (0.11.0)\n","Building wheels for collected packages: keras-segmentation, rioxarray\n","  Building wheel for keras-segmentation (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for keras-segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34377 sha256=a050be4235bd6119a42ca322d6d943c58466f5b5048ad4aa08ee213683a686d8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-s8xletqw/wheels/f4/fb/07/8f81ceb3d9fe936f5e4dcd1a64cbc489e42e6e7f9c2f166785\n","  Building wheel for rioxarray (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rioxarray: filename=rioxarray-0.9.1-py3-none-any.whl size=54590 sha256=8c63f4c5938426d6df5d4615322806611c296811fa1f2021e2f14f7dfd2534f9\n","  Stored in directory: /root/.cache/pip/wheels/03/b2/26/2e2cc1797ac99cc070d2cae87c340bd3429bbb583c90b1c780\n","Successfully built keras-segmentation rioxarray\n","Installing collected packages: rarfile, imageio, h5py, keras-applications, image-classifiers, efficientnet, segmentation-models, keras-segmentation, rioxarray\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.25.0\n","    Uninstalling imageio-2.25.0:\n","      Successfully uninstalled imageio-2.25.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n","tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed efficientnet-1.0.0 h5py-2.10.0 image-classifiers-1.0.0 imageio-2.5.0 keras-applications-1.0.8 keras-segmentation-0.3.0 rarfile-4.0 rioxarray-0.9.1 segmentation-models-1.0.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#@title import packages\n","import keras\n","import numpy as np\n","import os\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.losses import *\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint\n","import sys\n","\n","!pip install rarfile segmentation-models git+https://github.com/davej23/image-segmentation-keras.git rioxarray\n","from rarfile import RarFile\n","from sklearn.metrics import *\n","import rioxarray as rxr"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:01:11.235612Z","iopub.status.busy":"2023-04-06T11:01:11.234577Z","iopub.status.idle":"2023-04-06T11:02:40.559741Z","shell.execute_reply":"2023-04-06T11:02:40.558724Z","shell.execute_reply.started":"2023-04-06T11:01:11.235553Z"},"trusted":true},"outputs":[],"source":["base_dir = r\"/content/gdrive/MyDrive/mudtrain/\"\n","#@title Read training images and normalise\n","training_images_list = os.listdir(r\"{}train/images/\".format(base_dir))\n","training_masks_list = []\n","training_images = []\n","for n in training_images_list:\n","  training_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}train/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  training_images.append(a)\n","\n","## Training masks\n","training_masks = []\n","for n in training_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}train/labels/{}\".format(base_dir,n))))\n","  training_masks.append(a)\n","\n","\n","## Validation images\n","validation_images_list = os.listdir(r\"{}val/images/\".format(base_dir))\n","validation_masks_list = []\n","validation_images = []\n","for n in validation_images_list:\n","  validation_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}val/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  validation_images.append(a)\n","\n","## Validation masks\n","validation_masks = []\n","for n in validation_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}val/labels/{}\".format(base_dir,n))))\n","  validation_masks.append(a)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:02:45.435871Z","iopub.status.busy":"2023-04-06T11:02:45.435197Z","iopub.status.idle":"2023-04-06T11:02:46.028408Z","shell.execute_reply":"2023-04-06T11:02:46.027375Z","shell.execute_reply.started":"2023-04-06T11:02:45.435827Z"},"trusted":true},"outputs":[],"source":["#@title Pre-process data, reshaping and transposing\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].astype('float32')\n","  training_images[i] = training_images[i].T\n","\n","for i in range(len(training_masks)):\n","  training_masks[i] = training_masks[i].reshape(1,256,256)\n","  training_masks[i] = training_masks[i].T\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].astype('float32')\n","  validation_images[i] = validation_images[i].T\n","\n","for i in range(len(validation_masks)):\n","  validation_masks[i] = validation_masks[i].reshape(1,256,256)\n","  validation_masks[i] = validation_masks[i].T\n","\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].reshape(256,256,10)\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].reshape(256,256,10)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:02:49.496124Z","iopub.status.busy":"2023-04-06T11:02:49.495747Z","iopub.status.idle":"2023-04-06T11:02:55.022269Z","shell.execute_reply":"2023-04-06T11:02:55.021129Z","shell.execute_reply.started":"2023-04-06T11:02:49.496092Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 10)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["images=np.vstack([training_images])\n","val_images=np.vstack([validation_images])\n","images.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:02:58.228408Z","iopub.status.busy":"2023-04-06T11:02:58.227999Z","iopub.status.idle":"2023-04-06T11:02:58.386016Z","shell.execute_reply":"2023-04-06T11:02:58.385011Z","shell.execute_reply.started":"2023-04-06T11:02:58.228373Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 1)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["masks=np.vstack([training_masks])\n","val_masks=np.vstack([validation_masks])\n","val_masks.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:03:17.761065Z","iopub.status.busy":"2023-04-06T11:03:17.760704Z","iopub.status.idle":"2023-04-06T11:03:18.696493Z","shell.execute_reply":"2023-04-06T11:03:18.695383Z","shell.execute_reply.started":"2023-04-06T11:03:17.761034Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 4)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["bands=[0,1,2,6] # Blue, Green, Red, and NIR band\n","images_4b=images[...,bands]\n","images_4b.shape"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:03:22.975937Z","iopub.status.busy":"2023-04-06T11:03:22.975548Z","iopub.status.idle":"2023-04-06T11:03:23.221476Z","shell.execute_reply":"2023-04-06T11:03:23.220382Z","shell.execute_reply.started":"2023-04-06T11:03:22.975903Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 4)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["val_images_4b=val_images[...,bands]\n","val_images_4b.shape"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:03:27.038624Z","iopub.status.busy":"2023-04-06T11:03:27.037919Z","iopub.status.idle":"2023-04-06T11:03:27.279074Z","shell.execute_reply":"2023-04-06T11:03:27.278005Z","shell.execute_reply.started":"2023-04-06T11:03:27.038583Z"},"trusted":true},"outputs":[{"data":{"text/plain":["935"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del training_images,validation_images,training_masks,validation_masks,training_images_list,validation_images_list,\n","training_masks_list,validation_masks_list\n","gc.collect()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:03:29.573400Z","iopub.status.busy":"2023-04-06T11:03:29.572767Z","iopub.status.idle":"2023-04-06T11:03:29.612007Z","shell.execute_reply":"2023-04-06T11:03:29.610718Z","shell.execute_reply.started":"2023-04-06T11:03:29.573359Z"},"trusted":true},"outputs":[],"source":["del images,val_images"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:03:34.536956Z","iopub.status.busy":"2023-04-06T11:03:34.535959Z","iopub.status.idle":"2023-04-06T11:03:34.548390Z","shell.execute_reply":"2023-04-06T11:03:34.547325Z","shell.execute_reply.started":"2023-04-06T11:03:34.536907Z"},"trusted":true},"outputs":[],"source":["#@title boundary loss\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import models\n","\n","#Shape of semantic segmentation mask\n","OUTPUT_SHAPE = (256, 256, 1)\n","def boundary_loss(y_true, y_pred):\n","\n","    \"\"\"\n","    Paper Implemented : https://arxiv.org/abs/1905.07852\n","    Using Binary Segmentation mask, generates boundary mask on fly and claculates boundary loss.\n","    :param y_true:\n","    :param y_pred:\n","    :return:\n","    \"\"\"\n","    y_true=tf.cast(y_true,tf.float32)\n","    y_pred=tf.cast(y_pred,tf.float32)\n","    \n","    y_pred_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd = y_pred_bd - (1 - y_pred)\n","    y_true_bd = y_true_bd - (1 - y_true)\n","\n","    y_pred_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd_ext = y_pred_bd_ext - (1 - y_pred)\n","    y_true_bd_ext = y_true_bd_ext - (1 - y_true)\n","\n","    P = K.sum(y_pred_bd * y_true_bd_ext) / K.sum(y_pred_bd) + 1e-7\n","    R = K.sum(y_true_bd * y_pred_bd_ext) / K.sum(y_true_bd) + 1e-7\n","    F1_Score = 2 * P * R / (P + R + 1e-7)\n","    loss = K.mean(1 - F1_Score)\n","    \n","    return loss"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:03:39.956860Z","iopub.status.busy":"2023-04-06T11:03:39.956181Z","iopub.status.idle":"2023-04-06T11:03:41.980105Z","shell.execute_reply":"2023-04-06T11:03:41.979080Z","shell.execute_reply.started":"2023-04-06T11:03:39.956822Z"},"trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, Callback\n","class AlphaScheduler(Callback):\n","  def init(self, alpha, update_fn):\n","    self.alpha = alpha\n","    self.update_fn = update_fn\n","  def on_epoch_end(self, epoch, logs=None):\n","    updated_alpha = self.update_fn(K.get_value(self.alpha))\n","\n","alpha = K.variable(1, dtype='float32')\n","\n","def update_alpha(value):\n","  return np.clip(value - 0.005, 0.005, 1)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:03:44.146818Z","iopub.status.busy":"2023-04-06T11:03:44.146405Z","iopub.status.idle":"2023-04-06T11:03:44.153358Z","shell.execute_reply":"2023-04-06T11:03:44.152034Z","shell.execute_reply.started":"2023-04-06T11:03:44.146783Z"},"trusted":true},"outputs":[],"source":["def gl_sl_wrapper(alpha):\n","    def gl_sl(y_true, y_pred):\n","        return alpha*keras.losses.binary_crossentropy(y_true, y_pred) +  (1-alpha)* boundary_loss(y_true, y_pred)\n","    return gl_sl"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:03:48.119828Z","iopub.status.busy":"2023-04-06T11:03:48.119144Z","iopub.status.idle":"2023-04-06T11:03:48.142594Z","shell.execute_reply":"2023-04-06T11:03:48.140913Z","shell.execute_reply.started":"2023-04-06T11:03:48.119792Z"},"trusted":true},"outputs":[],"source":[" \n","def spatial_pool(x, mode, ratio=4):\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n"," \n","    if channel_axis == -1:\n","        batch, height, width, channels = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels//2\n","        input_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        input_x = Reshape((width*height, channel))(input_x)\n"," \n","        context_mask = Conv2D(1, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        context_mask = Reshape((width*height, 1))(context_mask)\n","        context_mask = Softmax(axis=1)(context_mask)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1], transpose_a=True))([input_x, context_mask])\n","        context = Permute((2, 1))(context)\n","        context = Reshape((1, 1, channel))(context)\n"," \n","    else:\n","        batch, channels, height, width = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels // 2\n","        input_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False,\n","                         kernel_initializer='he_normal')(x)\n","        input_x = Reshape((channel, width * height))(input_x)\n"," \n","        context_mask = Conv2D(1, kernel_size=1, strides=1, padding='same', use_bias=False,\n","                              kernel_initializer='he_normal')(x)\n","        context_mask = Reshape((width * height, 1))(context_mask)\n","        context_mask = Softmax(axis=1)(context_mask)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1]))([input_x, context_mask])\n","        context = Reshape((channel, 1, 1))(context)\n"," \n","    if mode == 'p':\n","        context = Conv2D(channels, kernel_size=1, strides=1, padding='same')(context)\n","    else:\n","        context = Conv2D(channel // ratio, kernel_size=1, strides=1, padding='same')(context)\n","        context = LayerNormalization()(context) # pip install keras-layer-normalization\n","        context = Conv2D(channels, kernel_size=1, strides=1, padding='same')(context)\n"," \n","    mask_ch = Activation('sigmoid')(context)\n","    return Multiply()([x, mask_ch])\n"," \n","def channel_pool(x):\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n","    if channel_axis == -1:\n","        batch, height, width, channels = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels//2\n","        g_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        avg_x = GlobalAvgPool2D()(g_x)\n","        avg_x = Softmax()(avg_x)\n","        avg_x = Reshape((channel, 1))(avg_x)\n"," \n","        theta_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        theta_x = Reshape((height*width, channel))(theta_x)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1]))([theta_x, avg_x])\n","        context = Reshape((height*width,))(context)\n","        mask_sp = Activation('sigmoid')(context)\n","        mask_sp = Reshape((height, width, 1))(mask_sp)\n","    else:\n","        batch, channels, height, width = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels//2\n","        g_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        avg_x = GlobalAvgPool2D()(g_x)\n","        avg_x = Softmax()(avg_x)\n","        avg_x = Reshape((1, channel))(avg_x)\n"," \n","        theta_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        theta_x = Reshape((channel, height*width))(theta_x)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1]))([avg_x, theta_x])\n","        context = Reshape((height*width,))(context)\n","        mask_sp = Activation('sigmoid')(context)\n","        mask_sp = Reshape((1, height, width))(mask_sp)\n"," \n","    return Multiply()([x, mask_sp])\n","\n","def PSA(x, mode='p'):\n","    context_channel = spatial_pool(x, mode)\n","    if mode == 'p':\n","        context_spatial = channel_pool(x)\n","        out = Add()([context_spatial, context_channel])\n","    elif mode == 's':\n","        out = channel_pool(context_channel)\n","    else:\n","        out = x\n","    return out"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:03:52.284209Z","iopub.status.busy":"2023-04-06T11:03:52.283253Z","iopub.status.idle":"2023-04-06T11:03:52.549939Z","shell.execute_reply":"2023-04-06T11:03:52.548825Z","shell.execute_reply.started":"2023-04-06T11:03:52.284170Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import models, layers, regularizers\n","from tensorflow.keras import backend as K\n","\n","#convolutional block\n","def conv_block(x, kernelsize, filters, dropout, batchnorm=True): \n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    if dropout > 0:\n","        conv = layers.Dropout(dropout)(conv)\n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(conv)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    return conv\n","\n","def daunet(input_shape, dropout=0, batchnorm=True):    \n","    \n","    filters = [32,64, 128, 256,512]\n","    kernelsize = 3\n","    upsample_size = 2\n","    \n","    inputs = layers.Input(input_shape)    \n","\n","    # Downsampling layers\n","    dn_1 = conv_block(inputs, kernelsize, filters[0], dropout, batchnorm)\n","    c1=PSA(dn_1)\n","    pool_1 = layers.MaxPooling2D(pool_size=(2,2))(dn_1)\n","    \n","    dn_2 = conv_block(pool_1, kernelsize, filters[1], dropout, batchnorm)\n","    c2=PSA(dn_2)\n","    pool_2 = layers.MaxPooling2D(pool_size=(2,2))(dn_2)\n","    \n","    dn_3 = conv_block(pool_2, kernelsize, filters[2], dropout, batchnorm)\n","    c3=PSA(dn_3)\n","    pool_3 = layers.MaxPooling2D(pool_size=(2,2))(dn_3)\n","    \n","    dn_4 = conv_block(pool_3, kernelsize, filters[3], dropout, batchnorm)\n","    c4=PSA(dn_4)\n","    pool_4 = layers.MaxPooling2D(pool_size=(2,2))(dn_4)\n","    \n","    dn_5 = conv_block(pool_4, kernelsize, filters[4], dropout, batchnorm)\n","\n","    # Upsampling layers   \n","    up_5 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(dn_5)\n","    up_5 = layers.concatenate([up_5, c4], axis=3)\n","    up_conv_5 = conv_block(up_5, kernelsize, filters[3], dropout, batchnorm)\n","    \n","    up_4 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_5)\n","    up_4 = layers.concatenate([up_4, c3], axis=3)\n","    up_conv_4 = conv_block(up_4, kernelsize, filters[2], dropout, batchnorm)\n","       \n","    up_3 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_4)\n","    up_3 = layers.concatenate([up_3, c2], axis=3)\n","    up_conv_3 = conv_block(up_3, kernelsize, filters[1], dropout, batchnorm)\n","    \n","    up_2 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_3)\n","    up_2 = layers.concatenate([up_2, c1], axis=3)\n","    up_conv_2 = conv_block(up_2, kernelsize, filters[0], dropout, batchnorm)    \n","   \n","    conv_final = layers.Conv2D(1, kernel_size=(1,1))(up_conv_2)\n","    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n","    outputs = layers.Activation('sigmoid')(conv_final)  \n","\n","    model = models.Model(inputs=[inputs], outputs=[outputs])     \n","    return model"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:03:56.268583Z","iopub.status.busy":"2023-04-06T11:03:56.267605Z","iopub.status.idle":"2023-04-06T11:03:57.613921Z","shell.execute_reply":"2023-04-06T11:03:57.613084Z","shell.execute_reply.started":"2023-04-06T11:03:56.268528Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 256, 256, 4  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 256, 256, 32  1184        ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 256, 256, 32  128        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," activation (Activation)        (None, 256, 256, 32  0           ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 256, 256, 32  9248        ['activation[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 256, 256, 32  128        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_1 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 128, 128, 32  0           ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 128, 128, 64  18496       ['max_pooling2d[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 128, 128, 64  256        ['conv2d_7[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_4 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_2[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 128, 128, 64  36928       ['activation_4[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 128, 128, 64  256        ['conv2d_8[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_5 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_3[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)  0           ['activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 64, 64, 128)  73856       ['max_pooling2d_1[0][0]']        \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_14[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_8 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_8[0][0]']           \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_15[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_9 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0          ['activation_9[0][0]']           \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 32, 32, 256)  295168      ['max_pooling2d_2[0][0]']        \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_21[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_12 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_12[0][0]']          \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_22[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_13 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," conv2d_26 (Conv2D)             (None, 32, 32, 128)  32768       ['activation_13[0][0]']          \n","                                                                                                  \n"," conv2d_24 (Conv2D)             (None, 32, 32, 1)    256         ['activation_13[0][0]']          \n","                                                                                                  \n"," global_average_pooling2d_3 (Gl  (None, 128)         0           ['conv2d_26[0][0]']              \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 32, 32, 128)  32768       ['activation_13[0][0]']          \n","                                                                                                  \n"," reshape_22 (Reshape)           (None, 1024, 1)      0           ['conv2d_24[0][0]']              \n","                                                                                                  \n"," max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0          ['activation_13[0][0]']          \n","                                                                                                  \n"," conv2d_27 (Conv2D)             (None, 32, 32, 128)  32768       ['activation_13[0][0]']          \n","                                                                                                  \n"," softmax_7 (Softmax)            (None, 128)          0           ['global_average_pooling2d_3[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," reshape_21 (Reshape)           (None, 1024, 128)    0           ['conv2d_23[0][0]']              \n","                                                                                                  \n"," softmax_6 (Softmax)            (None, 1024, 1)      0           ['reshape_22[0][0]']             \n","                                                                                                  \n"," conv2d_28 (Conv2D)             (None, 16, 16, 512)  1180160     ['max_pooling2d_3[0][0]']        \n","                                                                                                  \n"," reshape_25 (Reshape)           (None, 1024, 128)    0           ['conv2d_27[0][0]']              \n","                                                                                                  \n"," reshape_24 (Reshape)           (None, 128, 1)       0           ['softmax_7[0][0]']              \n","                                                                                                  \n"," lambda_6 (Lambda)              (None, 128, 1)       0           ['reshape_21[0][0]',             \n","                                                                  'softmax_6[0][0]']              \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_28[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," lambda_7 (Lambda)              (None, 1024, 1)      0           ['reshape_25[0][0]',             \n","                                                                  'reshape_24[0][0]']             \n","                                                                                                  \n"," permute_3 (Permute)            (None, 1, 128)       0           ['lambda_6[0][0]']               \n","                                                                                                  \n"," activation_16 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," reshape_26 (Reshape)           (None, 1024)         0           ['lambda_7[0][0]']               \n","                                                                                                  \n"," reshape_23 (Reshape)           (None, 1, 1, 128)    0           ['permute_3[0][0]']              \n","                                                                                                  \n"," conv2d_29 (Conv2D)             (None, 16, 16, 512)  2359808     ['activation_16[0][0]']          \n","                                                                                                  \n"," activation_15 (Activation)     (None, 1024)         0           ['reshape_26[0][0]']             \n","                                                                                                  \n"," conv2d_25 (Conv2D)             (None, 1, 1, 256)    33024       ['reshape_23[0][0]']             \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_29[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," reshape_27 (Reshape)           (None, 32, 32, 1)    0           ['activation_15[0][0]']          \n","                                                                                                  \n"," activation_14 (Activation)     (None, 1, 1, 256)    0           ['conv2d_25[0][0]']              \n","                                                                                                  \n"," activation_17 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," multiply_7 (Multiply)          (None, 32, 32, 256)  0           ['activation_13[0][0]',          \n","                                                                  'reshape_27[0][0]']             \n","                                                                                                  \n"," multiply_6 (Multiply)          (None, 32, 32, 256)  0           ['activation_13[0][0]',          \n","                                                                  'activation_14[0][0]']          \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 64, 64, 64)   8192        ['activation_9[0][0]']           \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 64, 64, 1)    128         ['activation_9[0][0]']           \n","                                                                                                  \n"," up_sampling2d (UpSampling2D)   (None, 32, 32, 512)  0           ['activation_17[0][0]']          \n","                                                                                                  \n"," add_3 (Add)                    (None, 32, 32, 256)  0           ['multiply_7[0][0]',             \n","                                                                  'multiply_6[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d_2 (Gl  (None, 64)          0           ['conv2d_19[0][0]']              \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 64, 64, 64)   8192        ['activation_9[0][0]']           \n","                                                                                                  \n"," reshape_15 (Reshape)           (None, 4096, 1)      0           ['conv2d_17[0][0]']              \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 32, 32, 768)  0           ['up_sampling2d[0][0]',          \n","                                                                  'add_3[0][0]']                  \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 64, 64, 64)   8192        ['activation_9[0][0]']           \n","                                                                                                  \n"," softmax_5 (Softmax)            (None, 64)           0           ['global_average_pooling2d_2[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," reshape_14 (Reshape)           (None, 4096, 64)     0           ['conv2d_16[0][0]']              \n","                                                                                                  \n"," softmax_4 (Softmax)            (None, 4096, 1)      0           ['reshape_15[0][0]']             \n","                                                                                                  \n"," conv2d_30 (Conv2D)             (None, 32, 32, 256)  1769728     ['concatenate[0][0]']            \n","                                                                                                  \n"," reshape_18 (Reshape)           (None, 4096, 64)     0           ['conv2d_20[0][0]']              \n","                                                                                                  \n"," reshape_17 (Reshape)           (None, 64, 1)        0           ['softmax_5[0][0]']              \n","                                                                                                  \n"," lambda_4 (Lambda)              (None, 64, 1)        0           ['reshape_14[0][0]',             \n","                                                                  'softmax_4[0][0]']              \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_30[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," lambda_5 (Lambda)              (None, 4096, 1)      0           ['reshape_18[0][0]',             \n","                                                                  'reshape_17[0][0]']             \n","                                                                                                  \n"," permute_2 (Permute)            (None, 1, 64)        0           ['lambda_4[0][0]']               \n","                                                                                                  \n"," activation_18 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_10[0][0]'] \n","                                                                                                  \n"," reshape_19 (Reshape)           (None, 4096)         0           ['lambda_5[0][0]']               \n","                                                                                                  \n"," reshape_16 (Reshape)           (None, 1, 1, 64)     0           ['permute_2[0][0]']              \n","                                                                                                  \n"," conv2d_31 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_18[0][0]']          \n","                                                                                                  \n"," activation_11 (Activation)     (None, 4096)         0           ['reshape_19[0][0]']             \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 1, 1, 128)    8320        ['reshape_16[0][0]']             \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_31[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," reshape_20 (Reshape)           (None, 64, 64, 1)    0           ['activation_11[0][0]']          \n","                                                                                                  \n"," activation_10 (Activation)     (None, 1, 1, 128)    0           ['conv2d_18[0][0]']              \n","                                                                                                  \n"," activation_19 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," multiply_5 (Multiply)          (None, 64, 64, 128)  0           ['activation_9[0][0]',           \n","                                                                  'reshape_20[0][0]']             \n","                                                                                                  \n"," multiply_4 (Multiply)          (None, 64, 64, 128)  0           ['activation_9[0][0]',           \n","                                                                  'activation_10[0][0]']          \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 128, 128, 32  2048        ['activation_5[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 128, 128, 1)  64          ['activation_5[0][0]']           \n","                                                                                                  \n"," up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0          ['activation_19[0][0]']          \n","                                                                                                  \n"," add_2 (Add)                    (None, 64, 64, 128)  0           ['multiply_5[0][0]',             \n","                                                                  'multiply_4[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d_1 (Gl  (None, 32)          0           ['conv2d_12[0][0]']              \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 128, 128, 32  2048        ['activation_5[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," reshape_8 (Reshape)            (None, 16384, 1)     0           ['conv2d_10[0][0]']              \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 64, 64, 384)  0           ['up_sampling2d_1[0][0]',        \n","                                                                  'add_2[0][0]']                  \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 128, 128, 32  2048        ['activation_5[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," softmax_3 (Softmax)            (None, 32)           0           ['global_average_pooling2d_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," reshape_7 (Reshape)            (None, 16384, 32)    0           ['conv2d_9[0][0]']               \n","                                                                                                  \n"," softmax_2 (Softmax)            (None, 16384, 1)     0           ['reshape_8[0][0]']              \n","                                                                                                  \n"," conv2d_32 (Conv2D)             (None, 64, 64, 128)  442496      ['concatenate_1[0][0]']          \n","                                                                                                  \n"," reshape_11 (Reshape)           (None, 16384, 32)    0           ['conv2d_13[0][0]']              \n","                                                                                                  \n"," reshape_10 (Reshape)           (None, 32, 1)        0           ['softmax_3[0][0]']              \n","                                                                                                  \n"," lambda_2 (Lambda)              (None, 32, 1)        0           ['reshape_7[0][0]',              \n","                                                                  'softmax_2[0][0]']              \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 64, 64, 128)  512        ['conv2d_32[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," lambda_3 (Lambda)              (None, 16384, 1)     0           ['reshape_11[0][0]',             \n","                                                                  'reshape_10[0][0]']             \n","                                                                                                  \n"," permute_1 (Permute)            (None, 1, 32)        0           ['lambda_2[0][0]']               \n","                                                                                                  \n"," activation_20 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," reshape_12 (Reshape)           (None, 16384)        0           ['lambda_3[0][0]']               \n","                                                                                                  \n"," reshape_9 (Reshape)            (None, 1, 1, 32)     0           ['permute_1[0][0]']              \n","                                                                                                  \n"," conv2d_33 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_20[0][0]']          \n","                                                                                                  \n"," activation_7 (Activation)      (None, 16384)        0           ['reshape_12[0][0]']             \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 1, 1, 64)     2112        ['reshape_9[0][0]']              \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 64, 64, 128)  512        ['conv2d_33[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," reshape_13 (Reshape)           (None, 128, 128, 1)  0           ['activation_7[0][0]']           \n","                                                                                                  \n"," activation_6 (Activation)      (None, 1, 1, 64)     0           ['conv2d_11[0][0]']              \n","                                                                                                  \n"," activation_21 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," multiply_3 (Multiply)          (None, 128, 128, 64  0           ['activation_5[0][0]',           \n","                                )                                 'reshape_13[0][0]']             \n","                                                                                                  \n"," multiply_2 (Multiply)          (None, 128, 128, 64  0           ['activation_5[0][0]',           \n","                                )                                 'activation_6[0][0]']           \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 256, 256, 16  512         ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 256, 256, 1)  32          ['activation_1[0][0]']           \n","                                                                                                  \n"," up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 12  0          ['activation_21[0][0]']          \n","                                8)                                                                \n","                                                                                                  \n"," add_1 (Add)                    (None, 128, 128, 64  0           ['multiply_3[0][0]',             \n","                                )                                 'multiply_2[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d (Glob  (None, 16)          0           ['conv2d_5[0][0]']               \n"," alAveragePooling2D)                                                                              \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 256, 256, 16  512         ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," reshape_1 (Reshape)            (None, 65536, 1)     0           ['conv2d_3[0][0]']               \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 128, 128, 19  0           ['up_sampling2d_2[0][0]',        \n","                                2)                                'add_1[0][0]']                  \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 256, 256, 16  512         ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," softmax_1 (Softmax)            (None, 16)           0           ['global_average_pooling2d[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," reshape (Reshape)              (None, 65536, 16)    0           ['conv2d_2[0][0]']               \n","                                                                                                  \n"," softmax (Softmax)              (None, 65536, 1)     0           ['reshape_1[0][0]']              \n","                                                                                                  \n"," conv2d_34 (Conv2D)             (None, 128, 128, 64  110656      ['concatenate_2[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," reshape_4 (Reshape)            (None, 65536, 16)    0           ['conv2d_6[0][0]']               \n","                                                                                                  \n"," reshape_3 (Reshape)            (None, 16, 1)        0           ['softmax_1[0][0]']              \n","                                                                                                  \n"," lambda (Lambda)                (None, 16, 1)        0           ['reshape[0][0]',                \n","                                                                  'softmax[0][0]']                \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 128, 128, 64  256        ['conv2d_34[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," lambda_1 (Lambda)              (None, 65536, 1)     0           ['reshape_4[0][0]',              \n","                                                                  'reshape_3[0][0]']              \n","                                                                                                  \n"," permute (Permute)              (None, 1, 16)        0           ['lambda[0][0]']                 \n","                                                                                                  \n"," activation_22 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_14[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," reshape_5 (Reshape)            (None, 65536)        0           ['lambda_1[0][0]']               \n","                                                                                                  \n"," reshape_2 (Reshape)            (None, 1, 1, 16)     0           ['permute[0][0]']                \n","                                                                                                  \n"," conv2d_35 (Conv2D)             (None, 128, 128, 64  36928       ['activation_22[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_3 (Activation)      (None, 65536)        0           ['reshape_5[0][0]']              \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 1, 1, 32)     544         ['reshape_2[0][0]']              \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 128, 128, 64  256        ['conv2d_35[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," reshape_6 (Reshape)            (None, 256, 256, 1)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," activation_2 (Activation)      (None, 1, 1, 32)     0           ['conv2d_4[0][0]']               \n","                                                                                                  \n"," activation_23 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_15[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," multiply_1 (Multiply)          (None, 256, 256, 32  0           ['activation_1[0][0]',           \n","                                )                                 'reshape_6[0][0]']              \n","                                                                                                  \n"," multiply (Multiply)            (None, 256, 256, 32  0           ['activation_1[0][0]',           \n","                                )                                 'activation_2[0][0]']           \n","                                                                                                  \n"," up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64  0          ['activation_23[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," add (Add)                      (None, 256, 256, 32  0           ['multiply_1[0][0]',             \n","                                )                                 'multiply[0][0]']               \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 256, 256, 96  0           ['up_sampling2d_3[0][0]',        \n","                                )                                 'add[0][0]']                    \n","                                                                                                  \n"," conv2d_36 (Conv2D)             (None, 256, 256, 32  27680       ['concatenate_3[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 256, 256, 32  128        ['conv2d_36[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_24 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_16[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_37 (Conv2D)             (None, 256, 256, 32  9248        ['activation_24[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 256, 256, 32  128        ['conv2d_37[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_25 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_17[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_38 (Conv2D)             (None, 256, 256, 1)  33          ['activation_25[0][0]']          \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 256, 256, 1)  4          ['conv2d_38[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_26 (Activation)     (None, 256, 256, 1)  0           ['batch_normalization_18[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 8,033,765\n","Trainable params: 8,027,875\n","Non-trainable params: 5,890\n","__________________________________________________________________________________________________\n"]}],"source":["from keras import metrics\n","unet2= daunet(input_shape=(256,256,4))\n","unet2.compile(optimizer = adam_v2.Adam(learning_rate = 1e-4), loss =gl_sl_wrapper(alpha), metrics = ['accuracy'])\n","unet2.summary()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:04:03.594670Z","iopub.status.busy":"2023-04-06T11:04:03.593969Z","iopub.status.idle":"2023-04-06T12:42:07.145211Z","shell.execute_reply":"2023-04-06T12:42:07.144266Z","shell.execute_reply.started":"2023-04-06T11:04:03.594627Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.4608 - accuracy: 0.8619\n","Epoch 1: val_loss improved from inf to 0.60264, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 30s 347ms/step - loss: 0.4608 - accuracy: 0.8619 - val_loss: 0.6026 - val_accuracy: 0.7347 - lr: 1.0000e-04\n","Epoch 2/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.4002 - accuracy: 0.9397\n","Epoch 2: val_loss improved from 0.60264 to 0.56122, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.4002 - accuracy: 0.9397 - val_loss: 0.5612 - val_accuracy: 0.7478 - lr: 1.0000e-04\n","Epoch 3/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3865 - accuracy: 0.9550\n","Epoch 3: val_loss improved from 0.56122 to 0.48334, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3865 - accuracy: 0.9550 - val_loss: 0.4833 - val_accuracy: 0.8915 - lr: 1.0000e-04\n","Epoch 4/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3823 - accuracy: 0.9597\n","Epoch 4: val_loss did not improve from 0.48334\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3823 - accuracy: 0.9597 - val_loss: 0.4892 - val_accuracy: 0.8569 - lr: 1.0000e-04\n","Epoch 5/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3700 - accuracy: 0.9696\n","Epoch 5: val_loss improved from 0.48334 to 0.47025, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.3700 - accuracy: 0.9696 - val_loss: 0.4703 - val_accuracy: 0.8708 - lr: 1.0000e-04\n","Epoch 6/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3728 - accuracy: 0.9642\n","Epoch 6: val_loss improved from 0.47025 to 0.42700, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3728 - accuracy: 0.9642 - val_loss: 0.4270 - val_accuracy: 0.9353 - lr: 1.0000e-04\n","Epoch 7/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3734 - accuracy: 0.9631\n","Epoch 7: val_loss improved from 0.42700 to 0.39768, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3734 - accuracy: 0.9631 - val_loss: 0.3977 - val_accuracy: 0.9353 - lr: 1.0000e-04\n","Epoch 8/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.9647\n","Epoch 8: val_loss improved from 0.39768 to 0.37178, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3689 - accuracy: 0.9647 - val_loss: 0.3718 - val_accuracy: 0.9703 - lr: 1.0000e-04\n","Epoch 9/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3618 - accuracy: 0.9722\n","Epoch 9: val_loss improved from 0.37178 to 0.36610, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 281ms/step - loss: 0.3618 - accuracy: 0.9722 - val_loss: 0.3661 - val_accuracy: 0.9777 - lr: 1.0000e-04\n","Epoch 10/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3615 - accuracy: 0.9688\n","Epoch 10: val_loss improved from 0.36610 to 0.34907, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3615 - accuracy: 0.9688 - val_loss: 0.3491 - val_accuracy: 0.9800 - lr: 1.0000e-04\n","Epoch 11/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.9735\n","Epoch 11: val_loss did not improve from 0.34907\n","56/56 [==============================] - 15s 272ms/step - loss: 0.3564 - accuracy: 0.9735 - val_loss: 0.3565 - val_accuracy: 0.9760 - lr: 1.0000e-04\n","Epoch 12/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3525 - accuracy: 0.9738\n","Epoch 12: val_loss improved from 0.34907 to 0.34396, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3525 - accuracy: 0.9738 - val_loss: 0.3440 - val_accuracy: 0.9795 - lr: 1.0000e-04\n","Epoch 13/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3480 - accuracy: 0.9773\n","Epoch 13: val_loss did not improve from 0.34396\n","56/56 [==============================] - 15s 271ms/step - loss: 0.3480 - accuracy: 0.9773 - val_loss: 0.3510 - val_accuracy: 0.9689 - lr: 1.0000e-04\n","Epoch 14/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.9776\n","Epoch 14: val_loss did not improve from 0.34396\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3468 - accuracy: 0.9776 - val_loss: 0.3459 - val_accuracy: 0.9815 - lr: 1.0000e-04\n","Epoch 15/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.9780\n","Epoch 15: val_loss did not improve from 0.34396\n","56/56 [==============================] - 15s 272ms/step - loss: 0.3431 - accuracy: 0.9780 - val_loss: 0.3495 - val_accuracy: 0.9761 - lr: 1.0000e-04\n","Epoch 16/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3475 - accuracy: 0.9732\n","Epoch 16: val_loss improved from 0.34396 to 0.33745, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3475 - accuracy: 0.9732 - val_loss: 0.3374 - val_accuracy: 0.9788 - lr: 1.0000e-04\n","Epoch 17/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.9799\n","Epoch 17: val_loss did not improve from 0.33745\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3375 - accuracy: 0.9799 - val_loss: 0.3379 - val_accuracy: 0.9794 - lr: 1.0000e-04\n","Epoch 18/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.9796\n","Epoch 18: val_loss did not improve from 0.33745\n","56/56 [==============================] - 15s 271ms/step - loss: 0.3378 - accuracy: 0.9796 - val_loss: 0.3390 - val_accuracy: 0.9768 - lr: 1.0000e-04\n","Epoch 19/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3362 - accuracy: 0.9790\n","Epoch 19: val_loss did not improve from 0.33745\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3362 - accuracy: 0.9790 - val_loss: 0.3833 - val_accuracy: 0.9353 - lr: 1.0000e-04\n","Epoch 20/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3356 - accuracy: 0.9777\n","Epoch 20: val_loss improved from 0.33745 to 0.33556, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3356 - accuracy: 0.9777 - val_loss: 0.3356 - val_accuracy: 0.9792 - lr: 1.0000e-04\n","Epoch 21/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3286 - accuracy: 0.9832\n","Epoch 21: val_loss did not improve from 0.33556\n","56/56 [==============================] - 15s 271ms/step - loss: 0.3286 - accuracy: 0.9832 - val_loss: 0.3361 - val_accuracy: 0.9835 - lr: 1.0000e-04\n","Epoch 22/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3304 - accuracy: 0.9797\n","Epoch 22: val_loss improved from 0.33556 to 0.33371, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.3304 - accuracy: 0.9797 - val_loss: 0.3337 - val_accuracy: 0.9817 - lr: 1.0000e-04\n","Epoch 23/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3294 - accuracy: 0.9785\n","Epoch 23: val_loss improved from 0.33371 to 0.32476, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3294 - accuracy: 0.9785 - val_loss: 0.3248 - val_accuracy: 0.9795 - lr: 1.0000e-04\n","Epoch 24/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.9779\n","Epoch 24: val_loss improved from 0.32476 to 0.32304, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3306 - accuracy: 0.9779 - val_loss: 0.3230 - val_accuracy: 0.9814 - lr: 1.0000e-04\n","Epoch 25/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3199 - accuracy: 0.9823\n","Epoch 25: val_loss did not improve from 0.32304\n","56/56 [==============================] - 15s 272ms/step - loss: 0.3199 - accuracy: 0.9823 - val_loss: 0.3251 - val_accuracy: 0.9850 - lr: 1.0000e-04\n","Epoch 26/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3177 - accuracy: 0.9841\n","Epoch 26: val_loss did not improve from 0.32304\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3177 - accuracy: 0.9841 - val_loss: 0.3240 - val_accuracy: 0.9853 - lr: 1.0000e-04\n","Epoch 27/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3194 - accuracy: 0.9803\n","Epoch 27: val_loss improved from 0.32304 to 0.31935, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.3194 - accuracy: 0.9803 - val_loss: 0.3193 - val_accuracy: 0.9852 - lr: 1.0000e-04\n","Epoch 28/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3204 - accuracy: 0.9781\n","Epoch 28: val_loss improved from 0.31935 to 0.30646, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.3204 - accuracy: 0.9781 - val_loss: 0.3065 - val_accuracy: 0.9848 - lr: 1.0000e-04\n","Epoch 29/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3148 - accuracy: 0.9830\n","Epoch 29: val_loss did not improve from 0.30646\n","56/56 [==============================] - 15s 274ms/step - loss: 0.3148 - accuracy: 0.9830 - val_loss: 0.3159 - val_accuracy: 0.9822 - lr: 1.0000e-04\n","Epoch 30/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.9828\n","Epoch 30: val_loss did not improve from 0.30646\n","56/56 [==============================] - 15s 270ms/step - loss: 0.3138 - accuracy: 0.9828 - val_loss: 0.3162 - val_accuracy: 0.9850 - lr: 1.0000e-04\n","Epoch 31/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3091 - accuracy: 0.9837\n","Epoch 31: val_loss did not improve from 0.30646\n","56/56 [==============================] - 15s 271ms/step - loss: 0.3091 - accuracy: 0.9837 - val_loss: 0.3140 - val_accuracy: 0.9844 - lr: 1.0000e-04\n","Epoch 32/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.9847\n","Epoch 32: val_loss did not improve from 0.30646\n","56/56 [==============================] - 15s 270ms/step - loss: 0.3068 - accuracy: 0.9847 - val_loss: 0.3109 - val_accuracy: 0.9844 - lr: 1.0000e-04\n","Epoch 33/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3050 - accuracy: 0.9862\n","Epoch 33: val_loss did not improve from 0.30646\n","56/56 [==============================] - 15s 274ms/step - loss: 0.3050 - accuracy: 0.9862 - val_loss: 0.3067 - val_accuracy: 0.9863 - lr: 1.0000e-04\n","Epoch 34/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3017 - accuracy: 0.9870\n","Epoch 34: val_loss improved from 0.30646 to 0.30473, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3017 - accuracy: 0.9870 - val_loss: 0.3047 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 35/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.9868\n","Epoch 35: val_loss improved from 0.30473 to 0.30250, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.3003 - accuracy: 0.9868 - val_loss: 0.3025 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 36/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.9876\n","Epoch 36: val_loss improved from 0.30250 to 0.30021, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2973 - accuracy: 0.9876 - val_loss: 0.3002 - val_accuracy: 0.9865 - lr: 1.0000e-04\n","Epoch 37/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.9874\n","Epoch 37: val_loss did not improve from 0.30021\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2960 - accuracy: 0.9874 - val_loss: 0.3038 - val_accuracy: 0.9859 - lr: 1.0000e-04\n","Epoch 38/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.9877\n","Epoch 38: val_loss did not improve from 0.30021\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2923 - accuracy: 0.9877 - val_loss: 0.3015 - val_accuracy: 0.9844 - lr: 1.0000e-04\n","Epoch 39/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.9810\n","Epoch 39: val_loss improved from 0.30021 to 0.29741, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.2988 - accuracy: 0.9810 - val_loss: 0.2974 - val_accuracy: 0.9805 - lr: 1.0000e-04\n","Epoch 40/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2944 - accuracy: 0.9830\n","Epoch 40: val_loss improved from 0.29741 to 0.28888, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.2944 - accuracy: 0.9830 - val_loss: 0.2889 - val_accuracy: 0.9821 - lr: 1.0000e-04\n","Epoch 41/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.9841\n","Epoch 41: val_loss improved from 0.28888 to 0.28747, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2923 - accuracy: 0.9841 - val_loss: 0.2875 - val_accuracy: 0.9852 - lr: 1.0000e-04\n","Epoch 42/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2889 - accuracy: 0.9864\n","Epoch 42: val_loss did not improve from 0.28747\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2889 - accuracy: 0.9864 - val_loss: 0.2899 - val_accuracy: 0.9858 - lr: 1.0000e-04\n","Epoch 43/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9858\n","Epoch 43: val_loss did not improve from 0.28747\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2895 - accuracy: 0.9858 - val_loss: 0.2898 - val_accuracy: 0.9833 - lr: 1.0000e-04\n","Epoch 44/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.9878\n","Epoch 44: val_loss did not improve from 0.28747\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2865 - accuracy: 0.9878 - val_loss: 0.2879 - val_accuracy: 0.9859 - lr: 1.0000e-04\n","Epoch 45/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.9864\n","Epoch 45: val_loss improved from 0.28747 to 0.28503, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2854 - accuracy: 0.9864 - val_loss: 0.2850 - val_accuracy: 0.9860 - lr: 1.0000e-04\n","Epoch 46/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.9879\n","Epoch 46: val_loss did not improve from 0.28503\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2808 - accuracy: 0.9879 - val_loss: 0.2889 - val_accuracy: 0.9844 - lr: 1.0000e-04\n","Epoch 47/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2777 - accuracy: 0.9886\n","Epoch 47: val_loss improved from 0.28503 to 0.28307, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2777 - accuracy: 0.9886 - val_loss: 0.2831 - val_accuracy: 0.9852 - lr: 1.0000e-04\n","Epoch 48/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.9885\n","Epoch 48: val_loss did not improve from 0.28307\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2775 - accuracy: 0.9885 - val_loss: 0.2833 - val_accuracy: 0.9838 - lr: 1.0000e-04\n","Epoch 49/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2761 - accuracy: 0.9881\n","Epoch 49: val_loss improved from 0.28307 to 0.28111, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.2761 - accuracy: 0.9881 - val_loss: 0.2811 - val_accuracy: 0.9854 - lr: 1.0000e-04\n","Epoch 50/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.9867\n","Epoch 50: val_loss did not improve from 0.28111\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2757 - accuracy: 0.9867 - val_loss: 0.2964 - val_accuracy: 0.9685 - lr: 1.0000e-04\n","Epoch 51/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2722 - accuracy: 0.9881\n","Epoch 51: val_loss improved from 0.28111 to 0.27880, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.2722 - accuracy: 0.9881 - val_loss: 0.2788 - val_accuracy: 0.9869 - lr: 1.0000e-04\n","Epoch 52/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.9890\n","Epoch 52: val_loss improved from 0.27880 to 0.26830, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.2721 - accuracy: 0.9890 - val_loss: 0.2683 - val_accuracy: 0.9875 - lr: 1.0000e-04\n","Epoch 53/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2708 - accuracy: 0.9882\n","Epoch 53: val_loss did not improve from 0.26830\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2708 - accuracy: 0.9882 - val_loss: 0.2713 - val_accuracy: 0.9870 - lr: 1.0000e-04\n","Epoch 54/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.9887\n","Epoch 54: val_loss improved from 0.26830 to 0.26636, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2677 - accuracy: 0.9887 - val_loss: 0.2664 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 55/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2661 - accuracy: 0.9881\n","Epoch 55: val_loss did not improve from 0.26636\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2661 - accuracy: 0.9881 - val_loss: 0.2708 - val_accuracy: 0.9824 - lr: 1.0000e-04\n","Epoch 56/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.9879\n","Epoch 56: val_loss improved from 0.26636 to 0.26489, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2642 - accuracy: 0.9879 - val_loss: 0.2649 - val_accuracy: 0.9880 - lr: 1.0000e-04\n","Epoch 57/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2656 - accuracy: 0.9881\n","Epoch 57: val_loss did not improve from 0.26489\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2656 - accuracy: 0.9881 - val_loss: 0.2697 - val_accuracy: 0.9861 - lr: 1.0000e-04\n","Epoch 58/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9867\n","Epoch 58: val_loss improved from 0.26489 to 0.25792, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2653 - accuracy: 0.9867 - val_loss: 0.2579 - val_accuracy: 0.9878 - lr: 1.0000e-04\n","Epoch 59/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9882\n","Epoch 59: val_loss did not improve from 0.25792\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2610 - accuracy: 0.9882 - val_loss: 0.2625 - val_accuracy: 0.9869 - lr: 1.0000e-04\n","Epoch 60/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9900\n","Epoch 60: val_loss did not improve from 0.25792\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2560 - accuracy: 0.9900 - val_loss: 0.2602 - val_accuracy: 0.9883 - lr: 1.0000e-04\n","Epoch 61/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.9901\n","Epoch 61: val_loss did not improve from 0.25792\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2551 - accuracy: 0.9901 - val_loss: 0.2589 - val_accuracy: 0.9873 - lr: 1.0000e-04\n","Epoch 62/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.9901\n","Epoch 62: val_loss did not improve from 0.25792\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2529 - accuracy: 0.9901 - val_loss: 0.2627 - val_accuracy: 0.9850 - lr: 1.0000e-04\n","Epoch 63/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.9903\n","Epoch 63: val_loss improved from 0.25792 to 0.25687, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2534 - accuracy: 0.9903 - val_loss: 0.2569 - val_accuracy: 0.9883 - lr: 1.0000e-04\n","Epoch 64/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.9897\n","Epoch 64: val_loss improved from 0.25687 to 0.25554, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.2537 - accuracy: 0.9897 - val_loss: 0.2555 - val_accuracy: 0.9864 - lr: 1.0000e-04\n","Epoch 65/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9857\n","Epoch 65: val_loss improved from 0.25554 to 0.25370, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2580 - accuracy: 0.9857 - val_loss: 0.2537 - val_accuracy: 0.9815 - lr: 1.0000e-04\n","Epoch 66/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.9869\n","Epoch 66: val_loss improved from 0.25370 to 0.24977, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2532 - accuracy: 0.9869 - val_loss: 0.2498 - val_accuracy: 0.9818 - lr: 1.0000e-04\n","Epoch 67/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9891\n","Epoch 67: val_loss did not improve from 0.24977\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2495 - accuracy: 0.9891 - val_loss: 0.2521 - val_accuracy: 0.9832 - lr: 1.0000e-04\n","Epoch 68/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.9901\n","Epoch 68: val_loss improved from 0.24977 to 0.24696, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.2465 - accuracy: 0.9901 - val_loss: 0.2470 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 69/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9873\n","Epoch 69: val_loss improved from 0.24696 to 0.24276, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.2487 - accuracy: 0.9873 - val_loss: 0.2428 - val_accuracy: 0.9885 - lr: 1.0000e-04\n","Epoch 70/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9901\n","Epoch 70: val_loss did not improve from 0.24276\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2448 - accuracy: 0.9901 - val_loss: 0.2484 - val_accuracy: 0.9875 - lr: 1.0000e-04\n","Epoch 71/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.9911\n","Epoch 71: val_loss did not improve from 0.24276\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2413 - accuracy: 0.9911 - val_loss: 0.2493 - val_accuracy: 0.9861 - lr: 1.0000e-04\n","Epoch 72/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9914\n","Epoch 72: val_loss did not improve from 0.24276\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2405 - accuracy: 0.9914 - val_loss: 0.2482 - val_accuracy: 0.9854 - lr: 1.0000e-04\n","Epoch 73/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2386 - accuracy: 0.9912\n","Epoch 73: val_loss did not improve from 0.24276\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2386 - accuracy: 0.9912 - val_loss: 0.2449 - val_accuracy: 0.9868 - lr: 1.0000e-04\n","Epoch 74/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2344 - accuracy: 0.9917\n","Epoch 74: val_loss improved from 0.24276 to 0.24123, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.2344 - accuracy: 0.9917 - val_loss: 0.2412 - val_accuracy: 0.9872 - lr: 1.0000e-04\n","Epoch 75/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2377 - accuracy: 0.9917\n","Epoch 75: val_loss improved from 0.24123 to 0.23770, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2377 - accuracy: 0.9917 - val_loss: 0.2377 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 76/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2330 - accuracy: 0.9914\n","Epoch 76: val_loss improved from 0.23770 to 0.23307, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2330 - accuracy: 0.9914 - val_loss: 0.2331 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 77/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.9915\n","Epoch 77: val_loss did not improve from 0.23307\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2347 - accuracy: 0.9915 - val_loss: 0.2397 - val_accuracy: 0.9860 - lr: 1.0000e-04\n","Epoch 78/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9920\n","Epoch 78: val_loss did not improve from 0.23307\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2316 - accuracy: 0.9920 - val_loss: 0.2346 - val_accuracy: 0.9878 - lr: 1.0000e-04\n","Epoch 79/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.9879\n","Epoch 79: val_loss did not improve from 0.23307\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2335 - accuracy: 0.9879 - val_loss: 0.2503 - val_accuracy: 0.9772 - lr: 1.0000e-04\n","Epoch 80/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9870\n","Epoch 80: val_loss improved from 0.23307 to 0.22682, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2332 - accuracy: 0.9870 - val_loss: 0.2268 - val_accuracy: 0.9881 - lr: 1.0000e-04\n","Epoch 81/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2360 - accuracy: 0.9840\n","Epoch 81: val_loss improved from 0.22682 to 0.22296, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2360 - accuracy: 0.9840 - val_loss: 0.2230 - val_accuracy: 0.9843 - lr: 1.0000e-04\n","Epoch 82/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2318 - accuracy: 0.9878\n","Epoch 82: val_loss did not improve from 0.22296\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2318 - accuracy: 0.9878 - val_loss: 0.2691 - val_accuracy: 0.9546 - lr: 1.0000e-04\n","Epoch 83/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2287 - accuracy: 0.9893\n","Epoch 83: val_loss did not improve from 0.22296\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2287 - accuracy: 0.9893 - val_loss: 0.2354 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 84/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2247 - accuracy: 0.9906\n","Epoch 84: val_loss did not improve from 0.22296\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2247 - accuracy: 0.9906 - val_loss: 0.2275 - val_accuracy: 0.9872 - lr: 1.0000e-04\n","Epoch 85/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2222 - accuracy: 0.9914\n","Epoch 85: val_loss did not improve from 0.22296\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2222 - accuracy: 0.9914 - val_loss: 0.2294 - val_accuracy: 0.9869 - lr: 1.0000e-04\n","Epoch 86/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.9918\n","Epoch 86: val_loss did not improve from 0.22296\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2211 - accuracy: 0.9918 - val_loss: 0.2253 - val_accuracy: 0.9875 - lr: 1.0000e-04\n","Epoch 87/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2213 - accuracy: 0.9918\n","Epoch 87: val_loss did not improve from 0.22296\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2213 - accuracy: 0.9918 - val_loss: 0.2250 - val_accuracy: 0.9870 - lr: 1.0000e-04\n","Epoch 88/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2209 - accuracy: 0.9907\n","Epoch 88: val_loss did not improve from 0.22296\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2209 - accuracy: 0.9907 - val_loss: 0.2280 - val_accuracy: 0.9825 - lr: 1.0000e-04\n","Epoch 89/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2213 - accuracy: 0.9903\n","Epoch 89: val_loss did not improve from 0.22296\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2213 - accuracy: 0.9903 - val_loss: 0.2287 - val_accuracy: 0.9859 - lr: 1.0000e-04\n","Epoch 90/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2152 - accuracy: 0.9914\n","Epoch 90: val_loss improved from 0.22296 to 0.22183, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2152 - accuracy: 0.9914 - val_loss: 0.2218 - val_accuracy: 0.9876 - lr: 1.0000e-04\n","Epoch 91/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2160 - accuracy: 0.9911\n","Epoch 91: val_loss improved from 0.22183 to 0.21739, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.2160 - accuracy: 0.9911 - val_loss: 0.2174 - val_accuracy: 0.9873 - lr: 1.0000e-04\n","Epoch 92/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2139 - accuracy: 0.9923\n","Epoch 92: val_loss did not improve from 0.21739\n","56/56 [==============================] - 15s 271ms/step - loss: 0.2139 - accuracy: 0.9923 - val_loss: 0.2182 - val_accuracy: 0.9875 - lr: 1.0000e-04\n","Epoch 93/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2108 - accuracy: 0.9927\n","Epoch 93: val_loss improved from 0.21739 to 0.21537, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.2108 - accuracy: 0.9927 - val_loss: 0.2154 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 94/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2124 - accuracy: 0.9920\n","Epoch 94: val_loss improved from 0.21537 to 0.21460, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 281ms/step - loss: 0.2124 - accuracy: 0.9920 - val_loss: 0.2146 - val_accuracy: 0.9874 - lr: 1.0000e-04\n","Epoch 95/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.9924\n","Epoch 95: val_loss improved from 0.21460 to 0.21190, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.2117 - accuracy: 0.9924 - val_loss: 0.2119 - val_accuracy: 0.9891 - lr: 1.0000e-04\n","Epoch 96/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2144 - accuracy: 0.9884\n","Epoch 96: val_loss improved from 0.21190 to 0.20656, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.2144 - accuracy: 0.9884 - val_loss: 0.2066 - val_accuracy: 0.9877 - lr: 1.0000e-04\n","Epoch 97/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9901\n","Epoch 97: val_loss did not improve from 0.20656\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2132 - accuracy: 0.9901 - val_loss: 0.2105 - val_accuracy: 0.9887 - lr: 1.0000e-04\n","Epoch 98/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2057 - accuracy: 0.9921\n","Epoch 98: val_loss did not improve from 0.20656\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2057 - accuracy: 0.9921 - val_loss: 0.2095 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 99/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2067 - accuracy: 0.9918\n","Epoch 99: val_loss improved from 0.20656 to 0.20476, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2067 - accuracy: 0.9918 - val_loss: 0.2048 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 100/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2067 - accuracy: 0.9911\n","Epoch 100: val_loss did not improve from 0.20476\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2067 - accuracy: 0.9911 - val_loss: 0.2107 - val_accuracy: 0.9874 - lr: 1.0000e-04\n","Epoch 101/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9925\n","Epoch 101: val_loss did not improve from 0.20476\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2024 - accuracy: 0.9925 - val_loss: 0.2087 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 102/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2033 - accuracy: 0.9924\n","Epoch 102: val_loss did not improve from 0.20476\n","56/56 [==============================] - 15s 270ms/step - loss: 0.2033 - accuracy: 0.9924 - val_loss: 0.2056 - val_accuracy: 0.9874 - lr: 1.0000e-04\n","Epoch 103/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2006 - accuracy: 0.9932\n","Epoch 103: val_loss improved from 0.20476 to 0.20339, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.2006 - accuracy: 0.9932 - val_loss: 0.2034 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 104/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1985 - accuracy: 0.9932\n","Epoch 104: val_loss improved from 0.20339 to 0.20308, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.1985 - accuracy: 0.9932 - val_loss: 0.2031 - val_accuracy: 0.9880 - lr: 1.0000e-04\n","Epoch 105/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1966 - accuracy: 0.9934\n","Epoch 105: val_loss improved from 0.20308 to 0.20227, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 281ms/step - loss: 0.1966 - accuracy: 0.9934 - val_loss: 0.2023 - val_accuracy: 0.9882 - lr: 1.0000e-04\n","Epoch 106/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9933\n","Epoch 106: val_loss improved from 0.20227 to 0.20097, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1973 - accuracy: 0.9933 - val_loss: 0.2010 - val_accuracy: 0.9878 - lr: 1.0000e-04\n","Epoch 107/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.9935\n","Epoch 107: val_loss improved from 0.20097 to 0.19983, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1959 - accuracy: 0.9935 - val_loss: 0.1998 - val_accuracy: 0.9883 - lr: 1.0000e-04\n","Epoch 108/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9934\n","Epoch 108: val_loss did not improve from 0.19983\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1958 - accuracy: 0.9934 - val_loss: 0.1998 - val_accuracy: 0.9885 - lr: 1.0000e-04\n","Epoch 109/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9936\n","Epoch 109: val_loss improved from 0.19983 to 0.19548, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1918 - accuracy: 0.9936 - val_loss: 0.1955 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 110/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9925\n","Epoch 110: val_loss did not improve from 0.19548\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1925 - accuracy: 0.9925 - val_loss: 0.2279 - val_accuracy: 0.9674 - lr: 1.0000e-04\n","Epoch 111/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9928\n","Epoch 111: val_loss did not improve from 0.19548\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1937 - accuracy: 0.9928 - val_loss: 0.1963 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 112/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1921 - accuracy: 0.9931\n","Epoch 112: val_loss improved from 0.19548 to 0.19228, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1921 - accuracy: 0.9931 - val_loss: 0.1923 - val_accuracy: 0.9886 - lr: 1.0000e-04\n","Epoch 113/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.9933\n","Epoch 113: val_loss did not improve from 0.19228\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1887 - accuracy: 0.9933 - val_loss: 0.1927 - val_accuracy: 0.9880 - lr: 1.0000e-04\n","Epoch 114/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1899 - accuracy: 0.9934\n","Epoch 114: val_loss improved from 0.19228 to 0.19194, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1899 - accuracy: 0.9934 - val_loss: 0.1919 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 115/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.9933\n","Epoch 115: val_loss improved from 0.19194 to 0.18806, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1869 - accuracy: 0.9933 - val_loss: 0.1881 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 116/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1866 - accuracy: 0.9933\n","Epoch 116: val_loss improved from 0.18806 to 0.18684, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.1866 - accuracy: 0.9933 - val_loss: 0.1868 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 117/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9938\n","Epoch 117: val_loss did not improve from 0.18684\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1852 - accuracy: 0.9938 - val_loss: 0.1888 - val_accuracy: 0.9891 - lr: 1.0000e-04\n","Epoch 118/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1854 - accuracy: 0.9938\n","Epoch 118: val_loss improved from 0.18684 to 0.18640, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1854 - accuracy: 0.9938 - val_loss: 0.1864 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 119/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1830 - accuracy: 0.9938\n","Epoch 119: val_loss improved from 0.18640 to 0.18357, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1830 - accuracy: 0.9938 - val_loss: 0.1836 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 120/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1839 - accuracy: 0.9937\n","Epoch 120: val_loss did not improve from 0.18357\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1839 - accuracy: 0.9937 - val_loss: 0.1849 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 121/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9938\n","Epoch 121: val_loss improved from 0.18357 to 0.18271, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1796 - accuracy: 0.9938 - val_loss: 0.1827 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 122/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9933\n","Epoch 122: val_loss improved from 0.18271 to 0.18166, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1800 - accuracy: 0.9933 - val_loss: 0.1817 - val_accuracy: 0.9879 - lr: 1.0000e-04\n","Epoch 123/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9908\n","Epoch 123: val_loss did not improve from 0.18166\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1840 - accuracy: 0.9908 - val_loss: 0.1833 - val_accuracy: 0.9891 - lr: 1.0000e-04\n","Epoch 124/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1784 - accuracy: 0.9932\n","Epoch 124: val_loss improved from 0.18166 to 0.18152, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1784 - accuracy: 0.9932 - val_loss: 0.1815 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 125/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9937\n","Epoch 125: val_loss improved from 0.18152 to 0.18026, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1770 - accuracy: 0.9937 - val_loss: 0.1803 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 126/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.9940\n","Epoch 126: val_loss improved from 0.18026 to 0.17897, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1755 - accuracy: 0.9940 - val_loss: 0.1790 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 127/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1750 - accuracy: 0.9941\n","Epoch 127: val_loss did not improve from 0.17897\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1750 - accuracy: 0.9941 - val_loss: 0.1792 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 128/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1762 - accuracy: 0.9941\n","Epoch 128: val_loss improved from 0.17897 to 0.17681, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1762 - accuracy: 0.9941 - val_loss: 0.1768 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 129/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.9942\n","Epoch 129: val_loss improved from 0.17681 to 0.17605, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1733 - accuracy: 0.9942 - val_loss: 0.1761 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 130/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1738 - accuracy: 0.9943\n","Epoch 130: val_loss improved from 0.17605 to 0.17471, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1738 - accuracy: 0.9943 - val_loss: 0.1747 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 131/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9943\n","Epoch 131: val_loss improved from 0.17471 to 0.17322, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1684 - accuracy: 0.9943 - val_loss: 0.1732 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 132/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9943\n","Epoch 132: val_loss improved from 0.17322 to 0.17275, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1691 - accuracy: 0.9943 - val_loss: 0.1727 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 133/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9941\n","Epoch 133: val_loss did not improve from 0.17275\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1707 - accuracy: 0.9941 - val_loss: 0.1746 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 134/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9946\n","Epoch 134: val_loss improved from 0.17275 to 0.16916, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1672 - accuracy: 0.9946 - val_loss: 0.1692 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 135/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1655 - accuracy: 0.9945\n","Epoch 135: val_loss improved from 0.16916 to 0.16904, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1655 - accuracy: 0.9945 - val_loss: 0.1690 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 136/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9944\n","Epoch 136: val_loss improved from 0.16904 to 0.16835, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1672 - accuracy: 0.9944 - val_loss: 0.1683 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 137/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1655 - accuracy: 0.9943\n","Epoch 137: val_loss did not improve from 0.16835\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1655 - accuracy: 0.9943 - val_loss: 0.1684 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 138/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9928\n","Epoch 138: val_loss improved from 0.16835 to 0.16502, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1665 - accuracy: 0.9928 - val_loss: 0.1650 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 139/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9914\n","Epoch 139: val_loss did not improve from 0.16502\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1669 - accuracy: 0.9914 - val_loss: 0.1854 - val_accuracy: 0.9764 - lr: 1.0000e-04\n","Epoch 140/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9915\n","Epoch 140: val_loss did not improve from 0.16502\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1683 - accuracy: 0.9915 - val_loss: 0.1727 - val_accuracy: 0.9849 - lr: 1.0000e-04\n","Epoch 141/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9899\n","Epoch 141: val_loss improved from 0.16502 to 0.16252, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1689 - accuracy: 0.9899 - val_loss: 0.1625 - val_accuracy: 0.9876 - lr: 1.0000e-04\n","Epoch 142/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1661 - accuracy: 0.9904\n","Epoch 142: val_loss did not improve from 0.16252\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1661 - accuracy: 0.9904 - val_loss: 0.1683 - val_accuracy: 0.9860 - lr: 1.0000e-04\n","Epoch 143/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1619 - accuracy: 0.9933\n","Epoch 143: val_loss did not improve from 0.16252\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1619 - accuracy: 0.9933 - val_loss: 0.1681 - val_accuracy: 0.9875 - lr: 1.0000e-04\n","Epoch 144/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9939\n","Epoch 144: val_loss did not improve from 0.16252\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1613 - accuracy: 0.9939 - val_loss: 0.1650 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 145/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1584 - accuracy: 0.9941\n","Epoch 145: val_loss improved from 0.16252 to 0.16137, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1584 - accuracy: 0.9941 - val_loss: 0.1614 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 146/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9942\n","Epoch 146: val_loss did not improve from 0.16137\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1575 - accuracy: 0.9942 - val_loss: 0.1615 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 147/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1568 - accuracy: 0.9945\n","Epoch 147: val_loss improved from 0.16137 to 0.15951, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1568 - accuracy: 0.9945 - val_loss: 0.1595 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 148/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1563 - accuracy: 0.9945\n","Epoch 148: val_loss did not improve from 0.15951\n","56/56 [==============================] - 15s 269ms/step - loss: 0.1563 - accuracy: 0.9945 - val_loss: 0.1597 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 149/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9947\n","Epoch 149: val_loss improved from 0.15951 to 0.15750, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1540 - accuracy: 0.9947 - val_loss: 0.1575 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 150/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1524 - accuracy: 0.9948\n","Epoch 150: val_loss improved from 0.15750 to 0.15629, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1524 - accuracy: 0.9948 - val_loss: 0.1563 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 151/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1516 - accuracy: 0.9948\n","Epoch 151: val_loss improved from 0.15629 to 0.15542, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1516 - accuracy: 0.9948 - val_loss: 0.1554 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 152/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1536 - accuracy: 0.9948\n","Epoch 152: val_loss improved from 0.15542 to 0.15461, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1536 - accuracy: 0.9948 - val_loss: 0.1546 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 153/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9950\n","Epoch 153: val_loss improved from 0.15461 to 0.15435, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1515 - accuracy: 0.9950 - val_loss: 0.1543 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 154/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.9949\n","Epoch 154: val_loss improved from 0.15435 to 0.15379, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1502 - accuracy: 0.9949 - val_loss: 0.1538 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 155/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1510 - accuracy: 0.9948\n","Epoch 155: val_loss improved from 0.15379 to 0.15173, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1510 - accuracy: 0.9948 - val_loss: 0.1517 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 156/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.9951\n","Epoch 156: val_loss improved from 0.15173 to 0.15134, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1473 - accuracy: 0.9951 - val_loss: 0.1513 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 157/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.9951\n","Epoch 157: val_loss did not improve from 0.15134\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1472 - accuracy: 0.9951 - val_loss: 0.1515 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 158/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9950\n","Epoch 158: val_loss improved from 0.15134 to 0.15050, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1503 - accuracy: 0.9950 - val_loss: 0.1505 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 159/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1450 - accuracy: 0.9951\n","Epoch 159: val_loss improved from 0.15050 to 0.14911, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1450 - accuracy: 0.9951 - val_loss: 0.1491 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 160/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9953\n","Epoch 160: val_loss improved from 0.14911 to 0.14829, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1452 - accuracy: 0.9953 - val_loss: 0.1483 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 161/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1458 - accuracy: 0.9949\n","Epoch 161: val_loss did not improve from 0.14829\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1458 - accuracy: 0.9949 - val_loss: 0.1507 - val_accuracy: 0.9885 - lr: 1.0000e-04\n","Epoch 162/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9952\n","Epoch 162: val_loss improved from 0.14829 to 0.14749, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1412 - accuracy: 0.9952 - val_loss: 0.1475 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 163/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9951\n","Epoch 163: val_loss did not improve from 0.14749\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1446 - accuracy: 0.9951 - val_loss: 0.1488 - val_accuracy: 0.9887 - lr: 1.0000e-04\n","Epoch 164/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9951\n","Epoch 164: val_loss improved from 0.14749 to 0.14671, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1433 - accuracy: 0.9951 - val_loss: 0.1467 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 165/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 0.9952\n","Epoch 165: val_loss improved from 0.14671 to 0.14594, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1416 - accuracy: 0.9952 - val_loss: 0.1459 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 166/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9945\n","Epoch 166: val_loss improved from 0.14594 to 0.14128, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1452 - accuracy: 0.9945 - val_loss: 0.1413 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 167/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9952\n","Epoch 167: val_loss improved from 0.14128 to 0.14031, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1379 - accuracy: 0.9952 - val_loss: 0.1403 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 168/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1385 - accuracy: 0.9954\n","Epoch 168: val_loss improved from 0.14031 to 0.14001, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1385 - accuracy: 0.9954 - val_loss: 0.1400 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 169/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9954\n","Epoch 169: val_loss improved from 0.14001 to 0.13878, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1366 - accuracy: 0.9954 - val_loss: 0.1388 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 170/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.9954\n","Epoch 170: val_loss improved from 0.13878 to 0.13753, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1372 - accuracy: 0.9954 - val_loss: 0.1375 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 171/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9955\n","Epoch 171: val_loss did not improve from 0.13753\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1363 - accuracy: 0.9955 - val_loss: 0.1394 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 172/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9956\n","Epoch 172: val_loss did not improve from 0.13753\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1355 - accuracy: 0.9956 - val_loss: 0.1378 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 173/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9957\n","Epoch 173: val_loss did not improve from 0.13753\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1355 - accuracy: 0.9957 - val_loss: 0.1378 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 174/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9956\n","Epoch 174: val_loss did not improve from 0.13753\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1363 - accuracy: 0.9956 - val_loss: 0.1376 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 175/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9957\n","Epoch 175: val_loss improved from 0.13753 to 0.13605, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1342 - accuracy: 0.9957 - val_loss: 0.1360 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 176/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9950\n","Epoch 176: val_loss did not improve from 0.13605\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1335 - accuracy: 0.9950 - val_loss: 0.1372 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 177/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9950\n","Epoch 177: val_loss did not improve from 0.13605\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1337 - accuracy: 0.9950 - val_loss: 0.1383 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 178/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.9955\n","Epoch 178: val_loss did not improve from 0.13605\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1321 - accuracy: 0.9955 - val_loss: 0.1363 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 179/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9955\n","Epoch 179: val_loss improved from 0.13605 to 0.13522, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1309 - accuracy: 0.9955 - val_loss: 0.1352 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 180/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9958\n","Epoch 180: val_loss improved from 0.13522 to 0.13498, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1299 - accuracy: 0.9958 - val_loss: 0.1350 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 181/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1302 - accuracy: 0.9957\n","Epoch 181: val_loss did not improve from 0.13498\n","56/56 [==============================] - 15s 269ms/step - loss: 0.1302 - accuracy: 0.9957 - val_loss: 0.1352 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 182/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1471 - accuracy: 0.9844\n","Epoch 182: val_loss did not improve from 0.13498\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1471 - accuracy: 0.9844 - val_loss: 0.2264 - val_accuracy: 0.9275 - lr: 1.0000e-04\n","Epoch 183/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.9826\n","Epoch 183: val_loss did not improve from 0.13498\n","56/56 [==============================] - 15s 269ms/step - loss: 0.1486 - accuracy: 0.9826 - val_loss: 0.1374 - val_accuracy: 0.9803 - lr: 1.0000e-04\n","Epoch 184/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1466 - accuracy: 0.9843\n","Epoch 184: val_loss improved from 0.13498 to 0.12631, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1466 - accuracy: 0.9843 - val_loss: 0.1263 - val_accuracy: 0.9869 - lr: 1.0000e-04\n","Epoch 185/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9902\n","Epoch 185: val_loss did not improve from 0.12631\n","56/56 [==============================] - 15s 269ms/step - loss: 0.1354 - accuracy: 0.9902 - val_loss: 0.1345 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 186/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1338 - accuracy: 0.9910\n","Epoch 186: val_loss did not improve from 0.12631\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1338 - accuracy: 0.9910 - val_loss: 0.1348 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 187/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1290 - accuracy: 0.9934\n","Epoch 187: val_loss did not improve from 0.12631\n","56/56 [==============================] - 15s 269ms/step - loss: 0.1290 - accuracy: 0.9934 - val_loss: 0.1341 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 188/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9936\n","Epoch 188: val_loss did not improve from 0.12631\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1296 - accuracy: 0.9936 - val_loss: 0.1336 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 189/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9943\n","Epoch 189: val_loss did not improve from 0.12631\n","56/56 [==============================] - 15s 273ms/step - loss: 0.1253 - accuracy: 0.9943 - val_loss: 0.1350 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 190/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9945\n","Epoch 190: val_loss did not improve from 0.12631\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1251 - accuracy: 0.9945 - val_loss: 0.1336 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 191/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1245 - accuracy: 0.9949\n","Epoch 191: val_loss did not improve from 0.12631\n","56/56 [==============================] - 15s 269ms/step - loss: 0.1245 - accuracy: 0.9949 - val_loss: 0.1296 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 192/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1245 - accuracy: 0.9950\n","Epoch 192: val_loss did not improve from 0.12631\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1245 - accuracy: 0.9950 - val_loss: 0.1270 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 193/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9951\n","Epoch 193: val_loss improved from 0.12631 to 0.12628, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1216 - accuracy: 0.9951 - val_loss: 0.1263 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 194/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9954\n","Epoch 194: val_loss improved from 0.12628 to 0.12579, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1235 - accuracy: 0.9954 - val_loss: 0.1258 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 195/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1207 - accuracy: 0.9949\n","Epoch 195: val_loss did not improve from 0.12579\n","56/56 [==============================] - 15s 269ms/step - loss: 0.1207 - accuracy: 0.9949 - val_loss: 0.1279 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 196/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9931\n","Epoch 196: val_loss improved from 0.12579 to 0.12512, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1250 - accuracy: 0.9931 - val_loss: 0.1251 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 197/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1222 - accuracy: 0.9949\n","Epoch 197: val_loss improved from 0.12512 to 0.12459, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1222 - accuracy: 0.9949 - val_loss: 0.1246 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 198/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9954\n","Epoch 198: val_loss improved from 0.12459 to 0.12412, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1200 - accuracy: 0.9954 - val_loss: 0.1241 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 199/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1188 - accuracy: 0.9953\n","Epoch 199: val_loss improved from 0.12412 to 0.12276, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1188 - accuracy: 0.9953 - val_loss: 0.1228 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 200/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 0.9954\n","Epoch 200: val_loss did not improve from 0.12276\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1185 - accuracy: 0.9954 - val_loss: 0.1235 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 201/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 0.9952\n","Epoch 201: val_loss improved from 0.12276 to 0.12000, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1182 - accuracy: 0.9952 - val_loss: 0.1200 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 202/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1157 - accuracy: 0.9955\n","Epoch 202: val_loss improved from 0.12000 to 0.11835, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1157 - accuracy: 0.9955 - val_loss: 0.1183 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 203/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9956\n","Epoch 203: val_loss did not improve from 0.11835\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1173 - accuracy: 0.9956 - val_loss: 0.1222 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 204/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9956\n","Epoch 204: val_loss did not improve from 0.11835\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1169 - accuracy: 0.9956 - val_loss: 0.1232 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 205/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9959\n","Epoch 205: val_loss did not improve from 0.11835\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1154 - accuracy: 0.9959 - val_loss: 0.1209 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 206/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9957\n","Epoch 206: val_loss did not improve from 0.11835\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1169 - accuracy: 0.9957 - val_loss: 0.1191 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 207/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9948\n","Epoch 207: val_loss improved from 0.11835 to 0.11759, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1195 - accuracy: 0.9948 - val_loss: 0.1176 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 208/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1150 - accuracy: 0.9952\n","Epoch 208: val_loss did not improve from 0.11759\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1150 - accuracy: 0.9952 - val_loss: 0.1183 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 209/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1124 - accuracy: 0.9959\n","Epoch 209: val_loss did not improve from 0.11759\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1124 - accuracy: 0.9959 - val_loss: 0.1188 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 210/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9960\n","Epoch 210: val_loss did not improve from 0.11759\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1112 - accuracy: 0.9960 - val_loss: 0.1179 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 211/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9960\n","Epoch 211: val_loss improved from 0.11759 to 0.11658, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1102 - accuracy: 0.9960 - val_loss: 0.1166 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 212/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9961\n","Epoch 212: val_loss improved from 0.11658 to 0.11658, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1123 - accuracy: 0.9961 - val_loss: 0.1166 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 213/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.9960\n","Epoch 213: val_loss improved from 0.11658 to 0.11569, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1094 - accuracy: 0.9960 - val_loss: 0.1157 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 214/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9960\n","Epoch 214: val_loss improved from 0.11569 to 0.11475, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1089 - accuracy: 0.9960 - val_loss: 0.1148 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 215/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.9961\n","Epoch 215: val_loss did not improve from 0.11475\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1091 - accuracy: 0.9961 - val_loss: 0.1155 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 216/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9959\n","Epoch 216: val_loss improved from 0.11475 to 0.11454, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1098 - accuracy: 0.9959 - val_loss: 0.1145 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 217/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9959\n","Epoch 217: val_loss did not improve from 0.11454\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1109 - accuracy: 0.9959 - val_loss: 0.1160 - val_accuracy: 0.9893 - lr: 1.0000e-04\n","Epoch 218/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1079 - accuracy: 0.9960\n","Epoch 218: val_loss improved from 0.11454 to 0.11259, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1079 - accuracy: 0.9960 - val_loss: 0.1126 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 219/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 0.9961\n","Epoch 219: val_loss improved from 0.11259 to 0.11039, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1064 - accuracy: 0.9961 - val_loss: 0.1104 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 220/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1072 - accuracy: 0.9962\n","Epoch 220: val_loss did not improve from 0.11039\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1072 - accuracy: 0.9962 - val_loss: 0.1109 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 221/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9959\n","Epoch 221: val_loss improved from 0.11039 to 0.10907, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1082 - accuracy: 0.9959 - val_loss: 0.1091 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 222/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1063 - accuracy: 0.9961\n","Epoch 222: val_loss did not improve from 0.10907\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1063 - accuracy: 0.9961 - val_loss: 0.1096 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 223/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9961\n","Epoch 223: val_loss did not improve from 0.10907\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1057 - accuracy: 0.9961 - val_loss: 0.1097 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 224/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.9961\n","Epoch 224: val_loss did not improve from 0.10907\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1066 - accuracy: 0.9961 - val_loss: 0.1096 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 225/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9962\n","Epoch 225: val_loss improved from 0.10907 to 0.10821, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.1013 - accuracy: 0.9962 - val_loss: 0.1082 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 226/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1031 - accuracy: 0.9962\n","Epoch 226: val_loss improved from 0.10821 to 0.10774, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1031 - accuracy: 0.9962 - val_loss: 0.1077 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 227/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.9961\n","Epoch 227: val_loss did not improve from 0.10774\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1041 - accuracy: 0.9961 - val_loss: 0.1090 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 228/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9962\n","Epoch 228: val_loss improved from 0.10774 to 0.10683, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1033 - accuracy: 0.9962 - val_loss: 0.1068 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 229/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1006 - accuracy: 0.9963\n","Epoch 229: val_loss did not improve from 0.10683\n","56/56 [==============================] - 15s 271ms/step - loss: 0.1006 - accuracy: 0.9963 - val_loss: 0.1070 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 230/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0994 - accuracy: 0.9963\n","Epoch 230: val_loss improved from 0.10683 to 0.10484, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0994 - accuracy: 0.9963 - val_loss: 0.1048 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 231/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9962\n","Epoch 231: val_loss improved from 0.10484 to 0.10340, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1010 - accuracy: 0.9962 - val_loss: 0.1034 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 232/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.9962\n","Epoch 232: val_loss did not improve from 0.10340\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1032 - accuracy: 0.9962 - val_loss: 0.1060 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 233/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0994 - accuracy: 0.9963\n","Epoch 233: val_loss did not improve from 0.10340\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0994 - accuracy: 0.9963 - val_loss: 0.1066 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 234/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9965\n","Epoch 234: val_loss did not improve from 0.10340\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0977 - accuracy: 0.9965 - val_loss: 0.1051 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 235/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1006 - accuracy: 0.9964\n","Epoch 235: val_loss did not improve from 0.10340\n","56/56 [==============================] - 15s 271ms/step - loss: 0.1006 - accuracy: 0.9964 - val_loss: 0.1049 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 236/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9963\n","Epoch 236: val_loss did not improve from 0.10340\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0978 - accuracy: 0.9963 - val_loss: 0.1035 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 237/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9963\n","Epoch 237: val_loss did not improve from 0.10340\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0986 - accuracy: 0.9963 - val_loss: 0.1039 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 238/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9965\n","Epoch 238: val_loss improved from 0.10340 to 0.10340, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0948 - accuracy: 0.9965 - val_loss: 0.1034 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 239/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9964\n","Epoch 239: val_loss did not improve from 0.10340\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0960 - accuracy: 0.9964 - val_loss: 0.1048 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 240/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9964\n","Epoch 240: val_loss improved from 0.10340 to 0.10139, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0962 - accuracy: 0.9964 - val_loss: 0.1014 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 241/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9966\n","Epoch 241: val_loss did not improve from 0.10139\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0955 - accuracy: 0.9966 - val_loss: 0.1030 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 242/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9965\n","Epoch 242: val_loss improved from 0.10139 to 0.10111, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0935 - accuracy: 0.9965 - val_loss: 0.1011 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 243/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9966\n","Epoch 243: val_loss improved from 0.10111 to 0.10056, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0928 - accuracy: 0.9966 - val_loss: 0.1006 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 244/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9966\n","Epoch 244: val_loss improved from 0.10056 to 0.09893, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0927 - accuracy: 0.9966 - val_loss: 0.0989 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 245/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 0.9966\n","Epoch 245: val_loss did not improve from 0.09893\n","56/56 [==============================] - 15s 270ms/step - loss: 0.0926 - accuracy: 0.9966 - val_loss: 0.0990 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 246/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9955\n","Epoch 246: val_loss did not improve from 0.09893\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0929 - accuracy: 0.9955 - val_loss: 0.1001 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 247/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9959\n","Epoch 247: val_loss did not improve from 0.09893\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0918 - accuracy: 0.9959 - val_loss: 0.0996 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 248/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9961\n","Epoch 248: val_loss did not improve from 0.09893\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0944 - accuracy: 0.9961 - val_loss: 0.1034 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 249/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.9956\n","Epoch 249: val_loss did not improve from 0.09893\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0961 - accuracy: 0.9956 - val_loss: 0.1056 - val_accuracy: 0.9866 - lr: 1.0000e-04\n","Epoch 250/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9958\n","Epoch 250: val_loss did not improve from 0.09893\n","56/56 [==============================] - 15s 270ms/step - loss: 0.0917 - accuracy: 0.9958 - val_loss: 0.1006 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 251/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9963\n","Epoch 251: val_loss did not improve from 0.09893\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0904 - accuracy: 0.9963 - val_loss: 0.1013 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 252/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9965\n","Epoch 252: val_loss did not improve from 0.09893\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0903 - accuracy: 0.9965 - val_loss: 0.0996 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 253/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9966\n","Epoch 253: val_loss improved from 0.09893 to 0.09585, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0880 - accuracy: 0.9966 - val_loss: 0.0958 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 254/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9966\n","Epoch 254: val_loss did not improve from 0.09585\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0917 - accuracy: 0.9966 - val_loss: 0.0965 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 255/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9967\n","Epoch 255: val_loss did not improve from 0.09585\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0882 - accuracy: 0.9967 - val_loss: 0.0967 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 256/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0867 - accuracy: 0.9967\n","Epoch 256: val_loss improved from 0.09585 to 0.09574, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.0867 - accuracy: 0.9967 - val_loss: 0.0957 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 257/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9966\n","Epoch 257: val_loss did not improve from 0.09574\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0880 - accuracy: 0.9966 - val_loss: 0.1021 - val_accuracy: 0.9880 - lr: 1.0000e-04\n","Epoch 258/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9966\n","Epoch 258: val_loss did not improve from 0.09574\n","56/56 [==============================] - 15s 270ms/step - loss: 0.0869 - accuracy: 0.9966 - val_loss: 0.0986 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 259/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9912\n","Epoch 259: val_loss did not improve from 0.09574\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0997 - accuracy: 0.9912 - val_loss: 0.1039 - val_accuracy: 0.9789 - lr: 1.0000e-04\n","Epoch 260/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9821\n","Epoch 260: val_loss did not improve from 0.09574\n","56/56 [==============================] - 15s 270ms/step - loss: 0.1126 - accuracy: 0.9821 - val_loss: 0.1084 - val_accuracy: 0.9783 - lr: 1.0000e-04\n","Epoch 261/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9876\n","Epoch 261: val_loss did not improve from 0.09574\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1014 - accuracy: 0.9876 - val_loss: 0.0965 - val_accuracy: 0.9865 - lr: 1.0000e-04\n","Epoch 262/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9907\n","Epoch 262: val_loss improved from 0.09574 to 0.09281, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0973 - accuracy: 0.9907 - val_loss: 0.0928 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 263/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9937\n","Epoch 263: val_loss did not improve from 0.09281\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0884 - accuracy: 0.9937 - val_loss: 0.0954 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 264/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9946\n","Epoch 264: val_loss did not improve from 0.09281\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0881 - accuracy: 0.9946 - val_loss: 0.0950 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 265/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9945\n","Epoch 265: val_loss did not improve from 0.09281\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0882 - accuracy: 0.9945 - val_loss: 0.0959 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 266/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9944\n","Epoch 266: val_loss did not improve from 0.09281\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0859 - accuracy: 0.9944 - val_loss: 0.1057 - val_accuracy: 0.9860 - lr: 1.0000e-04\n","Epoch 267/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9947\n","Epoch 267: val_loss improved from 0.09281 to 0.08932, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0874 - accuracy: 0.9947 - val_loss: 0.0893 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 268/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9956\n","Epoch 268: val_loss improved from 0.08932 to 0.08657, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0838 - accuracy: 0.9956 - val_loss: 0.0866 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 269/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9959\n","Epoch 269: val_loss improved from 0.08657 to 0.08402, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0831 - accuracy: 0.9959 - val_loss: 0.0840 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 270/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9957\n","Epoch 270: val_loss did not improve from 0.08402\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0851 - accuracy: 0.9957 - val_loss: 0.0843 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 271/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9926\n","Epoch 271: val_loss did not improve from 0.08402\n","56/56 [==============================] - 15s 270ms/step - loss: 0.0901 - accuracy: 0.9926 - val_loss: 0.0898 - val_accuracy: 0.9879 - lr: 1.0000e-04\n","Epoch 272/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9951\n","Epoch 272: val_loss did not improve from 0.08402\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0837 - accuracy: 0.9951 - val_loss: 0.0892 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 273/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9958\n","Epoch 273: val_loss did not improve from 0.08402\n","56/56 [==============================] - 15s 270ms/step - loss: 0.0834 - accuracy: 0.9958 - val_loss: 0.0851 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 274/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9962\n","Epoch 274: val_loss did not improve from 0.08402\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0818 - accuracy: 0.9962 - val_loss: 0.0847 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 275/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9963\n","Epoch 275: val_loss did not improve from 0.08402\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0819 - accuracy: 0.9963 - val_loss: 0.0843 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 276/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9963\n","Epoch 276: val_loss improved from 0.08402 to 0.08346, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0821 - accuracy: 0.9963 - val_loss: 0.0835 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 277/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9963\n","Epoch 277: val_loss improved from 0.08346 to 0.08326, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0803 - accuracy: 0.9963 - val_loss: 0.0833 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 278/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9966\n","Epoch 278: val_loss improved from 0.08326 to 0.08213, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0778 - accuracy: 0.9966 - val_loss: 0.0821 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 279/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9965\n","Epoch 279: val_loss improved from 0.08213 to 0.08171, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0815 - accuracy: 0.9965 - val_loss: 0.0817 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 280/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9966\n","Epoch 280: val_loss improved from 0.08171 to 0.08156, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0801 - accuracy: 0.9966 - val_loss: 0.0816 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 281/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9966\n","Epoch 281: val_loss improved from 0.08156 to 0.08139, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.0802 - accuracy: 0.9966 - val_loss: 0.0814 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 282/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9966\n","Epoch 282: val_loss improved from 0.08139 to 0.07934, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0788 - accuracy: 0.9966 - val_loss: 0.0793 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 283/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9967\n","Epoch 283: val_loss improved from 0.07934 to 0.07915, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0776 - accuracy: 0.9967 - val_loss: 0.0791 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 284/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9968\n","Epoch 284: val_loss improved from 0.07915 to 0.07851, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0764 - accuracy: 0.9968 - val_loss: 0.0785 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 285/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9967\n","Epoch 285: val_loss improved from 0.07851 to 0.07733, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0759 - accuracy: 0.9967 - val_loss: 0.0773 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 286/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9966\n","Epoch 286: val_loss did not improve from 0.07733\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0795 - accuracy: 0.9966 - val_loss: 0.0775 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 287/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9967\n","Epoch 287: val_loss did not improve from 0.07733\n","56/56 [==============================] - 15s 270ms/step - loss: 0.0778 - accuracy: 0.9967 - val_loss: 0.0779 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 288/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9967\n","Epoch 288: val_loss did not improve from 0.07733\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0764 - accuracy: 0.9967 - val_loss: 0.0777 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 289/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9968\n","Epoch 289: val_loss did not improve from 0.07733\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0752 - accuracy: 0.9968 - val_loss: 0.0777 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 290/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9968\n","Epoch 290: val_loss improved from 0.07733 to 0.07634, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0769 - accuracy: 0.9968 - val_loss: 0.0763 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 291/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9969\n","Epoch 291: val_loss improved from 0.07634 to 0.07490, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0733 - accuracy: 0.9969 - val_loss: 0.0749 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 292/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9969\n","Epoch 292: val_loss did not improve from 0.07490\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0728 - accuracy: 0.9969 - val_loss: 0.0751 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 293/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9969\n","Epoch 293: val_loss improved from 0.07490 to 0.07460, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0743 - accuracy: 0.9969 - val_loss: 0.0746 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 294/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9969\n","Epoch 294: val_loss improved from 0.07460 to 0.07455, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0741 - accuracy: 0.9969 - val_loss: 0.0746 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 295/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9968\n","Epoch 295: val_loss improved from 0.07455 to 0.07432, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0747 - accuracy: 0.9968 - val_loss: 0.0743 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 296/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9969\n","Epoch 296: val_loss improved from 0.07432 to 0.07386, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0720 - accuracy: 0.9969 - val_loss: 0.0739 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 297/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9969\n","Epoch 297: val_loss improved from 0.07386 to 0.07361, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0740 - accuracy: 0.9969 - val_loss: 0.0736 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 298/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9970\n","Epoch 298: val_loss did not improve from 0.07361\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0718 - accuracy: 0.9970 - val_loss: 0.0744 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 299/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9970\n","Epoch 299: val_loss did not improve from 0.07361\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0708 - accuracy: 0.9970 - val_loss: 0.0739 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 300/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9970\n","Epoch 300: val_loss improved from 0.07361 to 0.07300, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0698 - accuracy: 0.9970 - val_loss: 0.0730 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 301/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9970\n","Epoch 301: val_loss improved from 0.07300 to 0.07281, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0720 - accuracy: 0.9970 - val_loss: 0.0728 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 302/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9970\n","Epoch 302: val_loss improved from 0.07281 to 0.07167, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0691 - accuracy: 0.9970 - val_loss: 0.0717 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 303/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9959\n","Epoch 303: val_loss did not improve from 0.07167\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0725 - accuracy: 0.9959 - val_loss: 0.0723 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 304/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9953\n","Epoch 304: val_loss did not improve from 0.07167\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0726 - accuracy: 0.9953 - val_loss: 0.0771 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 305/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9962\n","Epoch 305: val_loss did not improve from 0.07167\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0717 - accuracy: 0.9962 - val_loss: 0.0728 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 306/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9968\n","Epoch 306: val_loss did not improve from 0.07167\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0685 - accuracy: 0.9968 - val_loss: 0.0721 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 307/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9969\n","Epoch 307: val_loss improved from 0.07167 to 0.07091, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0682 - accuracy: 0.9969 - val_loss: 0.0709 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 308/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9969\n","Epoch 308: val_loss improved from 0.07091 to 0.06930, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0685 - accuracy: 0.9969 - val_loss: 0.0693 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 309/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9965\n","Epoch 309: val_loss did not improve from 0.06930\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0699 - accuracy: 0.9965 - val_loss: 0.0699 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 310/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9968\n","Epoch 310: val_loss did not improve from 0.06930\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0687 - accuracy: 0.9968 - val_loss: 0.0696 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 311/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9971\n","Epoch 311: val_loss improved from 0.06930 to 0.06847, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 278ms/step - loss: 0.0672 - accuracy: 0.9971 - val_loss: 0.0685 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 312/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9969\n","Epoch 312: val_loss did not improve from 0.06847\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0693 - accuracy: 0.9969 - val_loss: 0.0701 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 313/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9967\n","Epoch 313: val_loss did not improve from 0.06847\n","56/56 [==============================] - 15s 273ms/step - loss: 0.0676 - accuracy: 0.9967 - val_loss: 0.0698 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 314/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9970\n","Epoch 314: val_loss improved from 0.06847 to 0.06842, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0686 - accuracy: 0.9970 - val_loss: 0.0684 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 315/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9968\n","Epoch 315: val_loss improved from 0.06842 to 0.06625, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0686 - accuracy: 0.9968 - val_loss: 0.0662 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 316/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9962\n","Epoch 316: val_loss did not improve from 0.06625\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0693 - accuracy: 0.9962 - val_loss: 0.0683 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 317/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9969\n","Epoch 317: val_loss did not improve from 0.06625\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0658 - accuracy: 0.9969 - val_loss: 0.0686 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 318/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9951\n","Epoch 318: val_loss did not improve from 0.06625\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0703 - accuracy: 0.9951 - val_loss: 0.0720 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 319/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9956\n","Epoch 319: val_loss did not improve from 0.06625\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0673 - accuracy: 0.9956 - val_loss: 0.0684 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 320/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9964\n","Epoch 320: val_loss did not improve from 0.06625\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0674 - accuracy: 0.9964 - val_loss: 0.0678 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 321/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9961\n","Epoch 321: val_loss did not improve from 0.06625\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0684 - accuracy: 0.9961 - val_loss: 0.0705 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 322/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9966\n","Epoch 322: val_loss did not improve from 0.06625\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0647 - accuracy: 0.9966 - val_loss: 0.0696 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 323/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9969\n","Epoch 323: val_loss did not improve from 0.06625\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0662 - accuracy: 0.9969 - val_loss: 0.0683 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 324/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9968\n","Epoch 324: val_loss did not improve from 0.06625\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0631 - accuracy: 0.9968 - val_loss: 0.0669 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 325/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9971\n","Epoch 325: val_loss did not improve from 0.06625\n","\n","Epoch 325: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0629 - accuracy: 0.9971 - val_loss: 0.0663 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 326/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9972\n","Epoch 326: val_loss improved from 0.06625 to 0.06579, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0665 - accuracy: 0.9972 - val_loss: 0.0658 - val_accuracy: 0.9943 - lr: 1.0000e-05\n","Epoch 327/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9972\n","Epoch 327: val_loss improved from 0.06579 to 0.06527, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0622 - accuracy: 0.9972 - val_loss: 0.0653 - val_accuracy: 0.9944 - lr: 1.0000e-05\n","Epoch 328/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9973\n","Epoch 328: val_loss improved from 0.06527 to 0.06495, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.0633 - accuracy: 0.9973 - val_loss: 0.0650 - val_accuracy: 0.9944 - lr: 1.0000e-05\n","Epoch 329/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9973\n","Epoch 329: val_loss improved from 0.06495 to 0.06479, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.0629 - accuracy: 0.9973 - val_loss: 0.0648 - val_accuracy: 0.9945 - lr: 1.0000e-05\n","Epoch 330/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9973\n","Epoch 330: val_loss did not improve from 0.06479\n","56/56 [==============================] - 15s 270ms/step - loss: 0.0640 - accuracy: 0.9973 - val_loss: 0.0648 - val_accuracy: 0.9945 - lr: 1.0000e-05\n","Epoch 331/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9973\n","Epoch 331: val_loss did not improve from 0.06479\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0642 - accuracy: 0.9973 - val_loss: 0.0649 - val_accuracy: 0.9945 - lr: 1.0000e-05\n","Epoch 332/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9972\n","Epoch 332: val_loss improved from 0.06479 to 0.06474, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0649 - accuracy: 0.9972 - val_loss: 0.0647 - val_accuracy: 0.9945 - lr: 1.0000e-05\n","Epoch 333/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9972\n","Epoch 333: val_loss did not improve from 0.06474\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0648 - accuracy: 0.9972 - val_loss: 0.0648 - val_accuracy: 0.9945 - lr: 1.0000e-05\n","Epoch 334/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0635 - accuracy: 0.9973\n","Epoch 334: val_loss improved from 0.06474 to 0.06458, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.0635 - accuracy: 0.9973 - val_loss: 0.0646 - val_accuracy: 0.9946 - lr: 1.0000e-05\n","Epoch 335/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9973\n","Epoch 335: val_loss did not improve from 0.06458\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0643 - accuracy: 0.9973 - val_loss: 0.0646 - val_accuracy: 0.9946 - lr: 1.0000e-05\n","Epoch 336/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9973\n","Epoch 336: val_loss improved from 0.06458 to 0.06435, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0622 - accuracy: 0.9973 - val_loss: 0.0643 - val_accuracy: 0.9946 - lr: 1.0000e-05\n","Epoch 337/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9973\n","Epoch 337: val_loss did not improve from 0.06435\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0633 - accuracy: 0.9973 - val_loss: 0.0644 - val_accuracy: 0.9946 - lr: 1.0000e-05\n","Epoch 338/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9973\n","Epoch 338: val_loss improved from 0.06435 to 0.06426, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.0631 - accuracy: 0.9973 - val_loss: 0.0643 - val_accuracy: 0.9947 - lr: 1.0000e-05\n","Epoch 339/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9973\n","Epoch 339: val_loss improved from 0.06426 to 0.06389, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0631 - accuracy: 0.9973 - val_loss: 0.0639 - val_accuracy: 0.9946 - lr: 1.0000e-05\n","Epoch 340/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9973\n","Epoch 340: val_loss did not improve from 0.06389\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0645 - accuracy: 0.9973 - val_loss: 0.0639 - val_accuracy: 0.9948 - lr: 1.0000e-05\n","Epoch 341/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9973\n","Epoch 341: val_loss did not improve from 0.06389\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0657 - accuracy: 0.9973 - val_loss: 0.0643 - val_accuracy: 0.9947 - lr: 1.0000e-05\n","Epoch 342/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9974\n","Epoch 342: val_loss did not improve from 0.06389\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0630 - accuracy: 0.9974 - val_loss: 0.0643 - val_accuracy: 0.9947 - lr: 1.0000e-05\n","Epoch 343/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9973\n","Epoch 343: val_loss did not improve from 0.06389\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0646 - accuracy: 0.9973 - val_loss: 0.0644 - val_accuracy: 0.9947 - lr: 1.0000e-05\n","Epoch 344/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9974\n","Epoch 344: val_loss improved from 0.06389 to 0.06389, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.0614 - accuracy: 0.9974 - val_loss: 0.0639 - val_accuracy: 0.9947 - lr: 1.0000e-05\n","Epoch 345/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9973\n","Epoch 345: val_loss improved from 0.06389 to 0.06384, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.0630 - accuracy: 0.9973 - val_loss: 0.0638 - val_accuracy: 0.9947 - lr: 1.0000e-05\n","Epoch 346/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9974\n","Epoch 346: val_loss did not improve from 0.06384\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0618 - accuracy: 0.9974 - val_loss: 0.0639 - val_accuracy: 0.9947 - lr: 1.0000e-05\n","Epoch 347/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9974\n","Epoch 347: val_loss improved from 0.06384 to 0.06383, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.0631 - accuracy: 0.9974 - val_loss: 0.0638 - val_accuracy: 0.9947 - lr: 1.0000e-05\n","Epoch 348/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9974\n","Epoch 348: val_loss improved from 0.06383 to 0.06361, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0620 - accuracy: 0.9974 - val_loss: 0.0636 - val_accuracy: 0.9947 - lr: 1.0000e-05\n","Epoch 349/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9974\n","Epoch 349: val_loss did not improve from 0.06361\n","56/56 [==============================] - 15s 270ms/step - loss: 0.0632 - accuracy: 0.9974 - val_loss: 0.0638 - val_accuracy: 0.9947 - lr: 1.0000e-05\n","Epoch 350/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9974\n","Epoch 350: val_loss improved from 0.06361 to 0.06360, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0608 - accuracy: 0.9974 - val_loss: 0.0636 - val_accuracy: 0.9946 - lr: 1.0000e-05\n","Epoch 351/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9974\n","Epoch 351: val_loss did not improve from 0.06360\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0623 - accuracy: 0.9974 - val_loss: 0.0636 - val_accuracy: 0.9946 - lr: 1.0000e-05\n","Epoch 352/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9974\n","Epoch 352: val_loss improved from 0.06360 to 0.06353, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0625 - accuracy: 0.9974 - val_loss: 0.0635 - val_accuracy: 0.9948 - lr: 1.0000e-05\n","Epoch 353/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9974\n","Epoch 353: val_loss improved from 0.06353 to 0.06344, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0630 - accuracy: 0.9974 - val_loss: 0.0634 - val_accuracy: 0.9948 - lr: 1.0000e-05\n","Epoch 354/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9974\n","Epoch 354: val_loss improved from 0.06344 to 0.06331, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0618 - accuracy: 0.9974 - val_loss: 0.0633 - val_accuracy: 0.9948 - lr: 1.0000e-05\n","Epoch 355/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9973\n","Epoch 355: val_loss did not improve from 0.06331\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0629 - accuracy: 0.9973 - val_loss: 0.0635 - val_accuracy: 0.9948 - lr: 1.0000e-05\n","Epoch 356/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9974\n","Epoch 356: val_loss did not improve from 0.06331\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0618 - accuracy: 0.9974 - val_loss: 0.0634 - val_accuracy: 0.9948 - lr: 1.0000e-05\n","Epoch 357/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9974\n","Epoch 357: val_loss did not improve from 0.06331\n","56/56 [==============================] - 15s 272ms/step - loss: 0.0627 - accuracy: 0.9974 - val_loss: 0.0636 - val_accuracy: 0.9948 - lr: 1.0000e-05\n","Epoch 358/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9974\n","Epoch 358: val_loss did not improve from 0.06331\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0631 - accuracy: 0.9974 - val_loss: 0.0635 - val_accuracy: 0.9948 - lr: 1.0000e-05\n","Epoch 359/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9974\n","Epoch 359: val_loss improved from 0.06331 to 0.06299, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.0618 - accuracy: 0.9974 - val_loss: 0.0630 - val_accuracy: 0.9950 - lr: 1.0000e-05\n","Epoch 360/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9974\n","Epoch 360: val_loss improved from 0.06299 to 0.06270, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 280ms/step - loss: 0.0614 - accuracy: 0.9974 - val_loss: 0.0627 - val_accuracy: 0.9950 - lr: 1.0000e-05\n","Epoch 361/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9975\n","Epoch 361: val_loss did not improve from 0.06270\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0615 - accuracy: 0.9975 - val_loss: 0.0627 - val_accuracy: 0.9950 - lr: 1.0000e-05\n","Epoch 362/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9975\n","Epoch 362: val_loss did not improve from 0.06270\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0629 - accuracy: 0.9975 - val_loss: 0.0630 - val_accuracy: 0.9950 - lr: 1.0000e-05\n","Epoch 363/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9974\n","Epoch 363: val_loss improved from 0.06270 to 0.06249, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0610 - accuracy: 0.9974 - val_loss: 0.0625 - val_accuracy: 0.9951 - lr: 1.0000e-05\n","Epoch 364/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9975\n","Epoch 364: val_loss improved from 0.06249 to 0.06242, saving model to daunet_hybird_4b.hdf5\n","56/56 [==============================] - 16s 279ms/step - loss: 0.0605 - accuracy: 0.9975 - val_loss: 0.0624 - val_accuracy: 0.9951 - lr: 1.0000e-05\n","Epoch 365/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9975\n","Epoch 365: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0611 - accuracy: 0.9975 - val_loss: 0.0625 - val_accuracy: 0.9950 - lr: 1.0000e-05\n","Epoch 366/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9974\n","Epoch 366: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0624 - accuracy: 0.9974 - val_loss: 0.0625 - val_accuracy: 0.9950 - lr: 1.0000e-05\n","Epoch 367/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9974\n","Epoch 367: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0639 - accuracy: 0.9974 - val_loss: 0.0628 - val_accuracy: 0.9950 - lr: 1.0000e-05\n","Epoch 368/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9974\n","Epoch 368: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0608 - accuracy: 0.9974 - val_loss: 0.0627 - val_accuracy: 0.9949 - lr: 1.0000e-05\n","Epoch 369/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9974\n","Epoch 369: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 269ms/step - loss: 0.0593 - accuracy: 0.9974 - val_loss: 0.0654 - val_accuracy: 0.9934 - lr: 1.0000e-05\n","Epoch 370/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9974\n","Epoch 370: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0619 - accuracy: 0.9974 - val_loss: 0.0661 - val_accuracy: 0.9932 - lr: 1.0000e-05\n","Epoch 371/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9975\n","Epoch 371: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0625 - accuracy: 0.9975 - val_loss: 0.0665 - val_accuracy: 0.9932 - lr: 1.0000e-05\n","Epoch 372/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 0.9975\n","Epoch 372: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 270ms/step - loss: 0.0602 - accuracy: 0.9975 - val_loss: 0.0661 - val_accuracy: 0.9933 - lr: 1.0000e-05\n","Epoch 373/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9975\n","Epoch 373: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0647 - accuracy: 0.9975 - val_loss: 0.0661 - val_accuracy: 0.9934 - lr: 1.0000e-05\n","Epoch 374/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9975\n","Epoch 374: val_loss did not improve from 0.06242\n","\n","Epoch 374: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0608 - accuracy: 0.9975 - val_loss: 0.0658 - val_accuracy: 0.9934 - lr: 1.0000e-05\n","Epoch 375/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9975\n","Epoch 375: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 270ms/step - loss: 0.0609 - accuracy: 0.9975 - val_loss: 0.0656 - val_accuracy: 0.9935 - lr: 1.0000e-06\n","Epoch 376/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9974\n","Epoch 376: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0621 - accuracy: 0.9974 - val_loss: 0.0653 - val_accuracy: 0.9936 - lr: 1.0000e-06\n","Epoch 377/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9975\n","Epoch 377: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0620 - accuracy: 0.9975 - val_loss: 0.0653 - val_accuracy: 0.9936 - lr: 1.0000e-06\n","Epoch 378/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9974\n","Epoch 378: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0616 - accuracy: 0.9974 - val_loss: 0.0651 - val_accuracy: 0.9936 - lr: 1.0000e-06\n","Epoch 379/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9975\n","Epoch 379: val_loss did not improve from 0.06242\n","56/56 [==============================] - 15s 271ms/step - loss: 0.0614 - accuracy: 0.9975 - val_loss: 0.0650 - val_accuracy: 0.9937 - lr: 1.0000e-06\n","Epoch 379: early stopping\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7db029806250>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Train model\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr=ReduceLROnPlateau(monitor='val_loss',\n","                         factor=0.1,\n","                         patience=10,\n","                         verbose=1,\n","                         mode='auto',\n","                         min_delta=0.00003,\n","                         cooldown=0,\n","                         min_lr=0)\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15,verbose=1,mode='min')\n","save_model= ModelCheckpoint('daunet_hybird_4b.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","unet2.fit(images_4b, masks, validation_data=(val_images_4b,val_masks), batch_size=16, epochs=1000,verbose=1,shuffle=True,callbacks=[save_model,reduce_lr,early_stop])"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T12:44:50.827797Z","iopub.status.busy":"2023-04-06T12:44:50.826737Z","iopub.status.idle":"2023-04-06T12:44:50.835135Z","shell.execute_reply":"2023-04-06T12:44:50.834122Z","shell.execute_reply.started":"2023-04-06T12:44:50.827740Z"},"trusted":true},"outputs":[],"source":["np.save('daunet_hybird_4b-history.npy',unet2.history.history)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T12:44:59.710545Z","iopub.status.busy":"2023-04-06T12:44:59.710076Z","iopub.status.idle":"2023-04-06T12:44:59.717895Z","shell.execute_reply":"2023-04-06T12:44:59.716107Z","shell.execute_reply.started":"2023-04-06T12:44:59.710509Z"},"trusted":true},"outputs":[],"source":["model_history = np.load('daunet_hybird_4b-history.npy', allow_pickle='TRUE').item()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T12:45:03.181115Z","iopub.status.busy":"2023-04-06T12:45:03.180394Z","iopub.status.idle":"2023-04-06T12:45:33.597533Z","shell.execute_reply":"2023-04-06T12:45:33.596522Z","shell.execute_reply.started":"2023-04-06T12:45:03.181075Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<matplotlib.legend.Legend at 0x7db0096bfe10>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#@title Plot accuracy and loss \n","from matplotlib import pyplot as plt\n","## Accuracy\n","plt.plot(model_history['accuracy'])\n","plt.plot(model_history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')\n","plt.show()\n","\n","## Loss\n","plt.plot(model_history['loss'])\n","plt.plot(model_history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T12:46:14.083640Z","iopub.status.busy":"2023-04-06T12:46:14.082956Z","iopub.status.idle":"2023-04-06T12:46:14.091749Z","shell.execute_reply":"2023-04-06T12:46:14.090613Z","shell.execute_reply.started":"2023-04-06T12:46:14.083600Z"},"trusted":true},"outputs":[],"source":["del images_4b,val_images_4b"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T12:46:17.510235Z","iopub.status.busy":"2023-04-06T12:46:17.509452Z","iopub.status.idle":"2023-04-06T12:46:41.263212Z","shell.execute_reply":"2023-04-06T12:46:41.262189Z","shell.execute_reply.started":"2023-04-06T12:46:17.510199Z"},"trusted":true},"outputs":[],"source":["## Test images\n","test_dir=r'/content/gdrive/MyDrive/mudtest/'\n","test_images_list = os.listdir(r\"{}/images/\".format(test_dir))\n","test_masks_list = []\n","test_images = []\n","for n in test_images_list:\n","  test_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}/images/{}\".format(test_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  test_images.append(a)\n","\n","## Test masks\n","test_masks = []\n","for n in test_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}/labels/{}\".format(test_dir,n))))\n","  test_masks.append(a)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T12:47:37.276283Z","iopub.status.busy":"2023-04-06T12:47:37.275906Z","iopub.status.idle":"2023-04-06T12:47:37.424568Z","shell.execute_reply":"2023-04-06T12:47:37.423493Z","shell.execute_reply.started":"2023-04-06T12:47:37.276251Z"},"trusted":true},"outputs":[],"source":["for i in range(len(test_images)):\n","  test_images[i] = test_images[i].astype('float32')\n","  test_images[i] = test_images[i].T\n","\n","for i in range(len(test_masks)):\n","  test_masks[i] = test_masks[i].reshape(1,256,256,1)\n","  test_masks[i] = test_masks[i].T\n","for i in range(len(test_images)):\n","  test_images[i] = test_images[i].reshape(-1,256,256,10)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T12:50:09.473510Z","iopub.status.busy":"2023-04-06T12:50:09.472582Z","iopub.status.idle":"2023-04-06T12:50:09.482603Z","shell.execute_reply":"2023-04-06T12:50:09.481369Z","shell.execute_reply.started":"2023-04-06T12:50:09.473467Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(1, 256, 256, 10)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["test_images[0].shape"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T13:02:43.500060Z","iopub.status.busy":"2023-04-06T13:02:43.499705Z","iopub.status.idle":"2023-04-06T13:02:43.570545Z","shell.execute_reply":"2023-04-06T13:02:43.569381Z","shell.execute_reply.started":"2023-04-06T13:02:43.500030Z"},"trusted":true},"outputs":[],"source":["test_images_4b=[]\n","for i in range(len(test_images)):\n","  test_images_4b.append(test_images[i][...,bands])"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T13:03:01.248161Z","iopub.status.busy":"2023-04-06T13:03:01.247788Z","iopub.status.idle":"2023-04-06T13:03:01.256955Z","shell.execute_reply":"2023-04-06T13:03:01.255890Z","shell.execute_reply.started":"2023-04-06T13:03:01.248129Z"},"trusted":true},"outputs":[{"data":{"text/plain":["266"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["len(test_images_4b)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T13:03:16.586704Z","iopub.status.busy":"2023-04-06T13:03:16.586337Z","iopub.status.idle":"2023-04-06T13:03:16.595771Z","shell.execute_reply":"2023-04-06T13:03:16.594536Z","shell.execute_reply.started":"2023-04-06T13:03:16.586667Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(1, 256, 256, 4)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["test_images_4b[0].shape"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T13:03:30.158649Z","iopub.status.busy":"2023-04-06T13:03:30.157722Z","iopub.status.idle":"2023-04-06T13:03:30.166624Z","shell.execute_reply":"2023-04-06T13:03:30.165387Z","shell.execute_reply.started":"2023-04-06T13:03:30.158597Z"},"trusted":true},"outputs":[],"source":["#@title Returns an image or array plot of mask prediction\n","\n","def reconstruct_image(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","  # Standardise between 0-1\n","  reconstruction = reconstruction/np.max(reconstruction)\n","\n","  # Round to 0-1, binary pixel-by-pixel classification \n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  # Plot reconstructed mask (prediction)\n","  plt.imshow(reconstruction) \n","'''\n","  Returns array of mask prediction, given model and image\n","'''\n","def reconstruct_array(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","\n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  return reconstruction # Returns array"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T13:04:39.561268Z","iopub.status.busy":"2023-04-06T13:04:39.560920Z","iopub.status.idle":"2023-04-06T13:04:39.580349Z","shell.execute_reply":"2023-04-06T13:04:39.579246Z","shell.execute_reply.started":"2023-04-06T13:04:39.561236Z"},"trusted":true},"outputs":[],"source":["#@title Metric functions for evaluation\n","def accuracy_eval(model, image, mask): # Gives score of mask vs prediction\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","    return accuracy_score(mask.flatten(), reconstruction)\n","\n","  else: # If a list of images input, find accuracy for each\n","    accuracy = []\n","    for i in range(len(image)):\n","#         reconstruction = model.predict(image[i].reshape(1, 256, 256, 10))\n","      reconstruction = model.predict(image[i].reshape(1, 256, 256, 4))\n","      reconstruction = np.round(reconstruction).flatten()\n","      accuracy.append(accuracy_score(mask[i].flatten(), reconstruction))\n","    return accuracy\n","\n","def recall_eval(model, image, mask): # Find recall score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return recall_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    recall = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        recall.append(recall_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return recall\n","\n","def precision_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return precision_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    precision = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        precision.append(precision_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return precision\n","\n","def iou_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return jaccard_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    iou = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        iou.append(jaccard_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return iou\n","\n","def f1_score_eval_basic(precision, recall):\n","    prec = np.mean(precision)\n","    rec = np.mean(recall)\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def produce_mask(image): # Outputs rounded image (binary)\n","  return np.round(image)\n","\n"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T13:04:43.953347Z","iopub.status.busy":"2023-04-06T13:04:43.952469Z","iopub.status.idle":"2023-04-06T13:06:12.698825Z","shell.execute_reply":"2023-04-06T13:06:12.697788Z","shell.execute_reply.started":"2023-04-06T13:04:43.953279Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 53ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 49ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 47ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 57ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 55ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 57ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 56ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 54ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 50ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 51ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n"]}],"source":["accuracy = (accuracy_eval(unet2, test_images_4b, test_masks))\n","precision = (precision_eval(unet2, test_images_4b, test_masks))\n","recall = (recall_eval(unet2, test_images_4b, test_masks))\n","iou = (iou_eval(unet2, test_images_4b, test_masks))"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T13:06:27.553456Z","iopub.status.busy":"2023-04-06T13:06:27.552520Z","iopub.status.idle":"2023-04-06T13:06:27.560257Z","shell.execute_reply":"2023-04-06T13:06:27.559196Z","shell.execute_reply.started":"2023-04-06T13:06:27.553403Z"},"trusted":true},"outputs":[],"source":["f1_score = (f1_score_eval_basic(precision, recall))"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T13:06:30.215087Z","iopub.status.busy":"2023-04-06T13:06:30.214483Z","iopub.status.idle":"2023-04-06T13:06:30.223026Z","shell.execute_reply":"2023-04-06T13:06:30.221905Z","shell.execute_reply.started":"2023-04-06T13:06:30.215048Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model accuracy:  0.9669645495880815 0.05074961361547062\n","model precision:  0.9317837611868868 0.10775096599243812\n","model recall:  0.9398641200542128 0.11880967911754911\n","model F1-score:  0.9358064981838397\n","model iou:  0.8852543167893862\n"]}],"source":["\n","# Print score eval results for each model\n","print('model accuracy: ', np.mean(accuracy), np.std(accuracy))\n","# Print precision eval results for each model\n","print('model precision: ', np.mean(precision), np.std(precision))\n","# Print recall eval results for each model\n","print('model recall: ', np.mean(recall), np.std(recall))\n","# Print f1-score eval results for each model\n","print('model F1-score: ', np.mean(f1_score))\n","print('model iou: ', np.mean(iou))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
