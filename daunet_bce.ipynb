{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-05T10:31:51.407168Z","iopub.status.busy":"2023-04-05T10:31:51.406809Z","iopub.status.idle":"2023-04-05T10:32:30.275208Z","shell.execute_reply":"2023-04-05T10:32:30.274145Z","shell.execute_reply.started":"2023-04-05T10:31:51.407135Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/davej23/image-segmentation-keras.git\n","  Cloning https://github.com/davej23/image-segmentation-keras.git to /tmp/pip-req-build-eshrzve8\n","  Running command git clone --filter=blob:none --quiet https://github.com/davej23/image-segmentation-keras.git /tmp/pip-req-build-eshrzve8\n","  Resolved https://github.com/davej23/image-segmentation-keras.git to commit e01b0a8d5859854cd9d259a618829889166439f5\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting rarfile\n","  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n","Collecting segmentation-models\n","  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n","Collecting rioxarray\n","  Downloading rioxarray-0.9.1.tar.gz (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting keras-applications<=1.0.8,>=1.0.7\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting efficientnet==1.0.0\n","  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n","Collecting image-classifiers==1.0.0\n","  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n","Collecting h5py<=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: Keras>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (2.11.0)\n","Collecting imageio==2.5.0\n","  Downloading imageio-2.5.0-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (0.4.0)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.5.4.60)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.64.1)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (9.4.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (1.21.6)\n","Requirement already satisfied: xarray>=0.17 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (0.20.2)\n","Requirement already satisfied: pyproj>=2.2 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (3.1.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from rioxarray) (23.0)\n","Requirement already satisfied: rasterio in /opt/conda/lib/python3.7/site-packages (from rioxarray) (1.2.10)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py<=2.10.0->keras-segmentation==0.3.0) (1.16.0)\n","Requirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.8.0)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.7.3)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (3.5.3)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from pyproj>=2.2->rioxarray) (2022.12.7)\n","Requirement already satisfied: pandas>=1.1 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (1.3.5)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.11.4)\n","Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.4.0)\n","Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (22.2.0)\n","Requirement already satisfied: affine in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (2.4.0)\n","Requirement already satisfied: click-plugins in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.1.1)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (59.8.0)\n","Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (0.7.2)\n","Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (8.1.3)\n","Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.4.7)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2023.2)\n","Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.11.2)\n","Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio->rioxarray) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->xarray>=0.17->rioxarray) (3.11.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (4.38.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (1.4.4)\n","Building wheels for collected packages: keras-segmentation, rioxarray\n","  Building wheel for keras-segmentation (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for keras-segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34377 sha256=b2bcba404047619fdf4e88044e717ad634a0e5bf44dfe98224bf51d82634f119\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-_po_y7kc/wheels/f4/fb/07/8f81ceb3d9fe936f5e4dcd1a64cbc489e42e6e7f9c2f166785\n","  Building wheel for rioxarray (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rioxarray: filename=rioxarray-0.9.1-py3-none-any.whl size=54590 sha256=e696ebea8ee66d5a344bd12b8add10651f37d66df5c46ba01feb678eb7fa3be4\n","  Stored in directory: /root/.cache/pip/wheels/03/b2/26/2e2cc1797ac99cc070d2cae87c340bd3429bbb583c90b1c780\n","Successfully built keras-segmentation rioxarray\n","Installing collected packages: rarfile, imageio, h5py, keras-applications, image-classifiers, efficientnet, segmentation-models, keras-segmentation, rioxarray\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.25.0\n","    Uninstalling imageio-2.25.0:\n","      Successfully uninstalled imageio-2.25.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n","tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed efficientnet-1.0.0 h5py-2.10.0 image-classifiers-1.0.0 imageio-2.5.0 keras-applications-1.0.8 keras-segmentation-0.3.0 rarfile-4.0 rioxarray-0.9.1 segmentation-models-1.0.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#@title import packages\n","import keras\n","import numpy as np\n","import os\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.losses import *\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint\n","import sys\n","\n","!pip install rarfile segmentation-models git+https://github.com/davej23/image-segmentation-keras.git rioxarray\n","from rarfile import RarFile\n","from sklearn.metrics import *\n","import rioxarray as rxr"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:32:34.127358Z","iopub.status.busy":"2023-04-05T10:32:34.126753Z","iopub.status.idle":"2023-04-05T10:34:15.084614Z","shell.execute_reply":"2023-04-05T10:34:15.083556Z","shell.execute_reply.started":"2023-04-05T10:32:34.127323Z"},"trusted":true},"outputs":[],"source":["base_dir = r\"/content/gdrive/MyDrive/mudtrain/\"\n","#@title Read training images and normalise\n","training_images_list = os.listdir(r\"{}train/images/\".format(base_dir))\n","training_masks_list = []\n","training_images = []\n","for n in training_images_list:\n","  training_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}train/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  training_images.append(a)\n","\n","## Training masks\n","training_masks = []\n","for n in training_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}train/labels/{}\".format(base_dir,n))))\n","  training_masks.append(a)\n","\n","\n","## Validation images\n","validation_images_list = os.listdir(r\"{}val/images/\".format(base_dir))\n","validation_masks_list = []\n","validation_images = []\n","for n in validation_images_list:\n","  validation_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}val/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  validation_images.append(a)\n","\n","## Validation masks\n","validation_masks = []\n","for n in validation_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}val/labels/{}\".format(base_dir,n))))\n","  validation_masks.append(a)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:34:52.570977Z","iopub.status.busy":"2023-04-05T10:34:52.570165Z","iopub.status.idle":"2023-04-05T10:34:53.222797Z","shell.execute_reply":"2023-04-05T10:34:53.221374Z","shell.execute_reply.started":"2023-04-05T10:34:52.570939Z"},"trusted":true},"outputs":[],"source":["#@title Pre-process data, reshaping and transposing\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].astype('float32')\n","  training_images[i] = training_images[i].T\n","\n","for i in range(len(training_masks)):\n","  training_masks[i] = training_masks[i].reshape(1,256,256)\n","  training_masks[i] = training_masks[i].T\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].astype('float32')\n","  validation_images[i] = validation_images[i].T\n","\n","for i in range(len(validation_masks)):\n","  validation_masks[i] = validation_masks[i].reshape(1,256,256)\n","  validation_masks[i] = validation_masks[i].T\n","\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].reshape(256,256,10)\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].reshape(256,256,10)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:34:56.060965Z","iopub.status.busy":"2023-04-05T10:34:56.060471Z","iopub.status.idle":"2023-04-05T10:35:01.199840Z","shell.execute_reply":"2023-04-05T10:35:01.198720Z","shell.execute_reply.started":"2023-04-05T10:34:56.060928Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 10)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["images=np.vstack([training_images])\n","val_images=np.vstack([validation_images])\n","images.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:35:01.202197Z","iopub.status.busy":"2023-04-05T10:35:01.201839Z","iopub.status.idle":"2023-04-05T10:35:01.382556Z","shell.execute_reply":"2023-04-05T10:35:01.380923Z","shell.execute_reply.started":"2023-04-05T10:35:01.202158Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 1)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["masks=np.vstack([training_masks])\n","val_masks=np.vstack([validation_masks])\n","val_masks.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:35:06.799377Z","iopub.status.busy":"2023-04-05T10:35:06.798649Z","iopub.status.idle":"2023-04-05T10:35:07.037991Z","shell.execute_reply":"2023-04-05T10:35:07.037003Z","shell.execute_reply.started":"2023-04-05T10:35:06.799339Z"},"trusted":true},"outputs":[{"data":{"text/plain":["904"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del training_images,validation_images,training_masks,validation_masks,training_images_list,validation_images_list,\n","training_masks_list,validation_masks_list\n","gc.collect()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T12:41:39.023180Z","iopub.status.busy":"2023-04-05T12:41:39.022440Z","iopub.status.idle":"2023-04-05T12:41:39.036345Z","shell.execute_reply":"2023-04-05T12:41:39.035288Z","shell.execute_reply.started":"2023-04-05T12:41:39.023141Z"},"trusted":true},"outputs":[],"source":["del images,masks,val_images,val_masks"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:26.642341Z","iopub.status.busy":"2023-04-04T19:50:26.641074Z","iopub.status.idle":"2023-04-04T19:50:26.653425Z","shell.execute_reply":"2023-04-04T19:50:26.652093Z","shell.execute_reply.started":"2023-04-04T19:50:26.642291Z"},"trusted":true},"outputs":[],"source":["#@title boundary loss\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import models\n","\n","#Shape of semantic segmentation mask\n","OUTPUT_SHAPE = (256, 256, 1)\n","def boundary_loss(y_true, y_pred):\n","\n","    \"\"\"\n","    Paper Implemented : https://arxiv.org/abs/1905.07852\n","    Using Binary Segmentation mask, generates boundary mask on fly and claculates boundary loss.\n","    :param y_true:\n","    :param y_pred:\n","    :return:\n","    \"\"\"\n","    y_true=tf.cast(y_true,tf.float32)\n","    y_pred=tf.cast(y_pred,tf.float32)\n","    \n","    y_pred_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd = y_pred_bd - (1 - y_pred)\n","    y_true_bd = y_true_bd - (1 - y_true)\n","\n","    y_pred_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd_ext = y_pred_bd_ext - (1 - y_pred)\n","    y_true_bd_ext = y_true_bd_ext - (1 - y_true)\n","\n","    P = K.sum(y_pred_bd * y_true_bd_ext) / K.sum(y_pred_bd) + 1e-7\n","    R = K.sum(y_true_bd * y_pred_bd_ext) / K.sum(y_true_bd) + 1e-7\n","    F1_Score = 2 * P * R / (P + R + 1e-7)\n","    loss = K.mean(1 - F1_Score)\n","    \n","    return loss"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:29.939912Z","iopub.status.busy":"2023-04-04T19:50:29.939199Z","iopub.status.idle":"2023-04-04T19:50:32.206391Z","shell.execute_reply":"2023-04-04T19:50:32.205381Z","shell.execute_reply.started":"2023-04-04T19:50:29.939872Z"},"trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, Callback\n","class AlphaScheduler(Callback):\n","  def init(self, alpha, update_fn):\n","    self.alpha = alpha\n","    self.update_fn = update_fn\n","  def on_epoch_end(self, epoch, logs=None):\n","    updated_alpha = self.update_fn(K.get_value(self.alpha))\n","\n","alpha = K.variable(1, dtype='float32')\n","\n","def update_alpha(value):\n","  return np.clip(value - 0.005, 0.005, 1)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:32.349836Z","iopub.status.busy":"2023-04-04T19:50:32.349500Z","iopub.status.idle":"2023-04-04T19:50:32.356389Z","shell.execute_reply":"2023-04-04T19:50:32.354437Z","shell.execute_reply.started":"2023-04-04T19:50:32.349804Z"},"trusted":true},"outputs":[],"source":["def gl_sl_wrapper(alpha):\n","    def gl_sl(y_true, y_pred):\n","        return alpha*keras.losses.binary_crossentropy(y_true, y_pred) +  (1-alpha)* boundary_loss(y_true, y_pred)\n","    return gl_sl"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:35:16.375272Z","iopub.status.busy":"2023-04-05T10:35:16.374844Z","iopub.status.idle":"2023-04-05T10:35:16.398539Z","shell.execute_reply":"2023-04-05T10:35:16.397249Z","shell.execute_reply.started":"2023-04-05T10:35:16.375239Z"},"trusted":true},"outputs":[],"source":[" \n","def spatial_pool(x, mode, ratio=4):\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n"," \n","    if channel_axis == -1:\n","        batch, height, width, channels = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels//2\n","        input_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        input_x = Reshape((width*height, channel))(input_x)\n"," \n","        context_mask = Conv2D(1, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        context_mask = Reshape((width*height, 1))(context_mask)\n","        context_mask = Softmax(axis=1)(context_mask)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1], transpose_a=True))([input_x, context_mask])\n","        context = Permute((2, 1))(context)\n","        context = Reshape((1, 1, channel))(context)\n"," \n","    else:\n","        batch, channels, height, width = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels // 2\n","        input_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False,\n","                         kernel_initializer='he_normal')(x)\n","        input_x = Reshape((channel, width * height))(input_x)\n"," \n","        context_mask = Conv2D(1, kernel_size=1, strides=1, padding='same', use_bias=False,\n","                              kernel_initializer='he_normal')(x)\n","        context_mask = Reshape((width * height, 1))(context_mask)\n","        context_mask = Softmax(axis=1)(context_mask)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1]))([input_x, context_mask])\n","        context = Reshape((channel, 1, 1))(context)\n"," \n","    if mode == 'p':\n","        context = Conv2D(channels, kernel_size=1, strides=1, padding='same')(context)\n","    else:\n","        context = Conv2D(channel // ratio, kernel_size=1, strides=1, padding='same')(context)\n","        context = LayerNormalization()(context) # pip install keras-layer-normalization\n","        context = Conv2D(channels, kernel_size=1, strides=1, padding='same')(context)\n"," \n","    mask_ch = Activation('sigmoid')(context)\n","    return Multiply()([x, mask_ch])\n"," \n","def channel_pool(x):\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n","    if channel_axis == -1:\n","        batch, height, width, channels = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels//2\n","        g_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        avg_x = GlobalAvgPool2D()(g_x)\n","        avg_x = Softmax()(avg_x)\n","        avg_x = Reshape((channel, 1))(avg_x)\n"," \n","        theta_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        theta_x = Reshape((height*width, channel))(theta_x)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1]))([theta_x, avg_x])\n","        context = Reshape((height*width,))(context)\n","        mask_sp = Activation('sigmoid')(context)\n","        mask_sp = Reshape((height, width, 1))(mask_sp)\n","    else:\n","        batch, channels, height, width = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels//2\n","        g_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        avg_x = GlobalAvgPool2D()(g_x)\n","        avg_x = Softmax()(avg_x)\n","        avg_x = Reshape((1, channel))(avg_x)\n"," \n","        theta_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        theta_x = Reshape((channel, height*width))(theta_x)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1]))([avg_x, theta_x])\n","        context = Reshape((height*width,))(context)\n","        mask_sp = Activation('sigmoid')(context)\n","        mask_sp = Reshape((1, height, width))(mask_sp)\n"," \n","    return Multiply()([x, mask_sp])\n","\n","def PSA(x, mode='p'):\n","    context_channel = spatial_pool(x, mode)\n","    if mode == 'p':\n","        context_spatial = channel_pool(x)\n","        out = Add()([context_spatial, context_channel])\n","    elif mode == 's':\n","        out = channel_pool(context_channel)\n","    else:\n","        out = x\n","    return out"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:35:23.531699Z","iopub.status.busy":"2023-04-05T10:35:23.531337Z","iopub.status.idle":"2023-04-05T10:35:23.845355Z","shell.execute_reply":"2023-04-05T10:35:23.844347Z","shell.execute_reply.started":"2023-04-05T10:35:23.531666Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import models, layers, regularizers\n","from tensorflow.keras import backend as K\n","\n","#convolutional block\n","def conv_block(x, kernelsize, filters, dropout, batchnorm=True): \n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    if dropout > 0:\n","        conv = layers.Dropout(dropout)(conv)\n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(conv)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    return conv\n","\n","def daunet(input_shape, dropout=0, batchnorm=True):    \n","    \n","    filters = [32,64, 128, 256,512]\n","    kernelsize = 3\n","    upsample_size = 2\n","    \n","    inputs = layers.Input(input_shape)    \n","\n","    # Downsampling layers\n","    dn_1 = conv_block(inputs, kernelsize, filters[0], dropout, batchnorm)\n","    c1=PSA(dn_1)\n","    pool_1 = layers.MaxPooling2D(pool_size=(2,2))(dn_1)\n","    \n","    dn_2 = conv_block(pool_1, kernelsize, filters[1], dropout, batchnorm)\n","    c2=PSA(dn_2)\n","    pool_2 = layers.MaxPooling2D(pool_size=(2,2))(dn_2)\n","    \n","    dn_3 = conv_block(pool_2, kernelsize, filters[2], dropout, batchnorm)\n","    c3=PSA(dn_3)\n","    pool_3 = layers.MaxPooling2D(pool_size=(2,2))(dn_3)\n","    \n","    dn_4 = conv_block(pool_3, kernelsize, filters[3], dropout, batchnorm)\n","    c4=PSA(dn_4)\n","    pool_4 = layers.MaxPooling2D(pool_size=(2,2))(dn_4)\n","    \n","    dn_5 = conv_block(pool_4, kernelsize, filters[4], dropout, batchnorm)\n","\n","    # Upsampling layers   \n","    up_5 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(dn_5)\n","    up_5 = layers.concatenate([up_5, c4], axis=3)\n","    up_conv_5 = conv_block(up_5, kernelsize, filters[3], dropout, batchnorm)\n","    \n","    up_4 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_5)\n","    up_4 = layers.concatenate([up_4, c3], axis=3)\n","    up_conv_4 = conv_block(up_4, kernelsize, filters[2], dropout, batchnorm)\n","       \n","    up_3 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_4)\n","    up_3 = layers.concatenate([up_3, c2], axis=3)\n","    up_conv_3 = conv_block(up_3, kernelsize, filters[1], dropout, batchnorm)\n","    \n","    up_2 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_3)\n","    up_2 = layers.concatenate([up_2, c1], axis=3)\n","    up_conv_2 = conv_block(up_2, kernelsize, filters[0], dropout, batchnorm)    \n","   \n","    conv_final = layers.Conv2D(1, kernel_size=(1,1))(up_conv_2)\n","    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n","    outputs = layers.Activation('sigmoid')(conv_final)  \n","\n","    model = models.Model(inputs=[inputs], outputs=[outputs])     \n","    return model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:35:37.203209Z","iopub.status.busy":"2023-04-05T10:35:37.202295Z","iopub.status.idle":"2023-04-05T10:35:40.927475Z","shell.execute_reply":"2023-04-05T10:35:40.926626Z","shell.execute_reply.started":"2023-04-05T10:35:37.203155Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 256, 256, 1  0           []                               \n","                                0)]                                                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 256, 256, 32  2912        ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 256, 256, 32  128        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," activation (Activation)        (None, 256, 256, 32  0           ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 256, 256, 32  9248        ['activation[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 256, 256, 32  128        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_1 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 128, 128, 32  0           ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 128, 128, 64  18496       ['max_pooling2d[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 128, 128, 64  256        ['conv2d_7[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_4 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_2[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 128, 128, 64  36928       ['activation_4[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 128, 128, 64  256        ['conv2d_8[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_5 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_3[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)  0           ['activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 64, 64, 128)  73856       ['max_pooling2d_1[0][0]']        \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_14[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_8 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_8[0][0]']           \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_15[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_9 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0          ['activation_9[0][0]']           \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 32, 32, 256)  295168      ['max_pooling2d_2[0][0]']        \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_21[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_12 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_12[0][0]']          \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_22[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_13 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," conv2d_26 (Conv2D)             (None, 32, 32, 128)  32768       ['activation_13[0][0]']          \n","                                                                                                  \n"," conv2d_24 (Conv2D)             (None, 32, 32, 1)    256         ['activation_13[0][0]']          \n","                                                                                                  \n"," global_average_pooling2d_3 (Gl  (None, 128)         0           ['conv2d_26[0][0]']              \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 32, 32, 128)  32768       ['activation_13[0][0]']          \n","                                                                                                  \n"," reshape_22 (Reshape)           (None, 1024, 1)      0           ['conv2d_24[0][0]']              \n","                                                                                                  \n"," max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0          ['activation_13[0][0]']          \n","                                                                                                  \n"," conv2d_27 (Conv2D)             (None, 32, 32, 128)  32768       ['activation_13[0][0]']          \n","                                                                                                  \n"," softmax_7 (Softmax)            (None, 128)          0           ['global_average_pooling2d_3[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," reshape_21 (Reshape)           (None, 1024, 128)    0           ['conv2d_23[0][0]']              \n","                                                                                                  \n"," softmax_6 (Softmax)            (None, 1024, 1)      0           ['reshape_22[0][0]']             \n","                                                                                                  \n"," conv2d_28 (Conv2D)             (None, 16, 16, 512)  1180160     ['max_pooling2d_3[0][0]']        \n","                                                                                                  \n"," reshape_25 (Reshape)           (None, 1024, 128)    0           ['conv2d_27[0][0]']              \n","                                                                                                  \n"," reshape_24 (Reshape)           (None, 128, 1)       0           ['softmax_7[0][0]']              \n","                                                                                                  \n"," lambda_6 (Lambda)              (None, 128, 1)       0           ['reshape_21[0][0]',             \n","                                                                  'softmax_6[0][0]']              \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_28[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," lambda_7 (Lambda)              (None, 1024, 1)      0           ['reshape_25[0][0]',             \n","                                                                  'reshape_24[0][0]']             \n","                                                                                                  \n"," permute_3 (Permute)            (None, 1, 128)       0           ['lambda_6[0][0]']               \n","                                                                                                  \n"," activation_16 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," reshape_26 (Reshape)           (None, 1024)         0           ['lambda_7[0][0]']               \n","                                                                                                  \n"," reshape_23 (Reshape)           (None, 1, 1, 128)    0           ['permute_3[0][0]']              \n","                                                                                                  \n"," conv2d_29 (Conv2D)             (None, 16, 16, 512)  2359808     ['activation_16[0][0]']          \n","                                                                                                  \n"," activation_15 (Activation)     (None, 1024)         0           ['reshape_26[0][0]']             \n","                                                                                                  \n"," conv2d_25 (Conv2D)             (None, 1, 1, 256)    33024       ['reshape_23[0][0]']             \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_29[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," reshape_27 (Reshape)           (None, 32, 32, 1)    0           ['activation_15[0][0]']          \n","                                                                                                  \n"," activation_14 (Activation)     (None, 1, 1, 256)    0           ['conv2d_25[0][0]']              \n","                                                                                                  \n"," activation_17 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," multiply_7 (Multiply)          (None, 32, 32, 256)  0           ['activation_13[0][0]',          \n","                                                                  'reshape_27[0][0]']             \n","                                                                                                  \n"," multiply_6 (Multiply)          (None, 32, 32, 256)  0           ['activation_13[0][0]',          \n","                                                                  'activation_14[0][0]']          \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 64, 64, 64)   8192        ['activation_9[0][0]']           \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 64, 64, 1)    128         ['activation_9[0][0]']           \n","                                                                                                  \n"," up_sampling2d (UpSampling2D)   (None, 32, 32, 512)  0           ['activation_17[0][0]']          \n","                                                                                                  \n"," add_3 (Add)                    (None, 32, 32, 256)  0           ['multiply_7[0][0]',             \n","                                                                  'multiply_6[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d_2 (Gl  (None, 64)          0           ['conv2d_19[0][0]']              \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 64, 64, 64)   8192        ['activation_9[0][0]']           \n","                                                                                                  \n"," reshape_15 (Reshape)           (None, 4096, 1)      0           ['conv2d_17[0][0]']              \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 32, 32, 768)  0           ['up_sampling2d[0][0]',          \n","                                                                  'add_3[0][0]']                  \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 64, 64, 64)   8192        ['activation_9[0][0]']           \n","                                                                                                  \n"," softmax_5 (Softmax)            (None, 64)           0           ['global_average_pooling2d_2[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," reshape_14 (Reshape)           (None, 4096, 64)     0           ['conv2d_16[0][0]']              \n","                                                                                                  \n"," softmax_4 (Softmax)            (None, 4096, 1)      0           ['reshape_15[0][0]']             \n","                                                                                                  \n"," conv2d_30 (Conv2D)             (None, 32, 32, 256)  1769728     ['concatenate[0][0]']            \n","                                                                                                  \n"," reshape_18 (Reshape)           (None, 4096, 64)     0           ['conv2d_20[0][0]']              \n","                                                                                                  \n"," reshape_17 (Reshape)           (None, 64, 1)        0           ['softmax_5[0][0]']              \n","                                                                                                  \n"," lambda_4 (Lambda)              (None, 64, 1)        0           ['reshape_14[0][0]',             \n","                                                                  'softmax_4[0][0]']              \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_30[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," lambda_5 (Lambda)              (None, 4096, 1)      0           ['reshape_18[0][0]',             \n","                                                                  'reshape_17[0][0]']             \n","                                                                                                  \n"," permute_2 (Permute)            (None, 1, 64)        0           ['lambda_4[0][0]']               \n","                                                                                                  \n"," activation_18 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_10[0][0]'] \n","                                                                                                  \n"," reshape_19 (Reshape)           (None, 4096)         0           ['lambda_5[0][0]']               \n","                                                                                                  \n"," reshape_16 (Reshape)           (None, 1, 1, 64)     0           ['permute_2[0][0]']              \n","                                                                                                  \n"," conv2d_31 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_18[0][0]']          \n","                                                                                                  \n"," activation_11 (Activation)     (None, 4096)         0           ['reshape_19[0][0]']             \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 1, 1, 128)    8320        ['reshape_16[0][0]']             \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_31[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," reshape_20 (Reshape)           (None, 64, 64, 1)    0           ['activation_11[0][0]']          \n","                                                                                                  \n"," activation_10 (Activation)     (None, 1, 1, 128)    0           ['conv2d_18[0][0]']              \n","                                                                                                  \n"," activation_19 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," multiply_5 (Multiply)          (None, 64, 64, 128)  0           ['activation_9[0][0]',           \n","                                                                  'reshape_20[0][0]']             \n","                                                                                                  \n"," multiply_4 (Multiply)          (None, 64, 64, 128)  0           ['activation_9[0][0]',           \n","                                                                  'activation_10[0][0]']          \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 128, 128, 32  2048        ['activation_5[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 128, 128, 1)  64          ['activation_5[0][0]']           \n","                                                                                                  \n"," up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0          ['activation_19[0][0]']          \n","                                                                                                  \n"," add_2 (Add)                    (None, 64, 64, 128)  0           ['multiply_5[0][0]',             \n","                                                                  'multiply_4[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d_1 (Gl  (None, 32)          0           ['conv2d_12[0][0]']              \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 128, 128, 32  2048        ['activation_5[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," reshape_8 (Reshape)            (None, 16384, 1)     0           ['conv2d_10[0][0]']              \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 64, 64, 384)  0           ['up_sampling2d_1[0][0]',        \n","                                                                  'add_2[0][0]']                  \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 128, 128, 32  2048        ['activation_5[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," softmax_3 (Softmax)            (None, 32)           0           ['global_average_pooling2d_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," reshape_7 (Reshape)            (None, 16384, 32)    0           ['conv2d_9[0][0]']               \n","                                                                                                  \n"," softmax_2 (Softmax)            (None, 16384, 1)     0           ['reshape_8[0][0]']              \n","                                                                                                  \n"," conv2d_32 (Conv2D)             (None, 64, 64, 128)  442496      ['concatenate_1[0][0]']          \n","                                                                                                  \n"," reshape_11 (Reshape)           (None, 16384, 32)    0           ['conv2d_13[0][0]']              \n","                                                                                                  \n"," reshape_10 (Reshape)           (None, 32, 1)        0           ['softmax_3[0][0]']              \n","                                                                                                  \n"," lambda_2 (Lambda)              (None, 32, 1)        0           ['reshape_7[0][0]',              \n","                                                                  'softmax_2[0][0]']              \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 64, 64, 128)  512        ['conv2d_32[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," lambda_3 (Lambda)              (None, 16384, 1)     0           ['reshape_11[0][0]',             \n","                                                                  'reshape_10[0][0]']             \n","                                                                                                  \n"," permute_1 (Permute)            (None, 1, 32)        0           ['lambda_2[0][0]']               \n","                                                                                                  \n"," activation_20 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," reshape_12 (Reshape)           (None, 16384)        0           ['lambda_3[0][0]']               \n","                                                                                                  \n"," reshape_9 (Reshape)            (None, 1, 1, 32)     0           ['permute_1[0][0]']              \n","                                                                                                  \n"," conv2d_33 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_20[0][0]']          \n","                                                                                                  \n"," activation_7 (Activation)      (None, 16384)        0           ['reshape_12[0][0]']             \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 1, 1, 64)     2112        ['reshape_9[0][0]']              \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 64, 64, 128)  512        ['conv2d_33[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," reshape_13 (Reshape)           (None, 128, 128, 1)  0           ['activation_7[0][0]']           \n","                                                                                                  \n"," activation_6 (Activation)      (None, 1, 1, 64)     0           ['conv2d_11[0][0]']              \n","                                                                                                  \n"," activation_21 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," multiply_3 (Multiply)          (None, 128, 128, 64  0           ['activation_5[0][0]',           \n","                                )                                 'reshape_13[0][0]']             \n","                                                                                                  \n"," multiply_2 (Multiply)          (None, 128, 128, 64  0           ['activation_5[0][0]',           \n","                                )                                 'activation_6[0][0]']           \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 256, 256, 16  512         ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 256, 256, 1)  32          ['activation_1[0][0]']           \n","                                                                                                  \n"," up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 12  0          ['activation_21[0][0]']          \n","                                8)                                                                \n","                                                                                                  \n"," add_1 (Add)                    (None, 128, 128, 64  0           ['multiply_3[0][0]',             \n","                                )                                 'multiply_2[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d (Glob  (None, 16)          0           ['conv2d_5[0][0]']               \n"," alAveragePooling2D)                                                                              \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 256, 256, 16  512         ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," reshape_1 (Reshape)            (None, 65536, 1)     0           ['conv2d_3[0][0]']               \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 128, 128, 19  0           ['up_sampling2d_2[0][0]',        \n","                                2)                                'add_1[0][0]']                  \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 256, 256, 16  512         ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," softmax_1 (Softmax)            (None, 16)           0           ['global_average_pooling2d[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," reshape (Reshape)              (None, 65536, 16)    0           ['conv2d_2[0][0]']               \n","                                                                                                  \n"," softmax (Softmax)              (None, 65536, 1)     0           ['reshape_1[0][0]']              \n","                                                                                                  \n"," conv2d_34 (Conv2D)             (None, 128, 128, 64  110656      ['concatenate_2[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," reshape_4 (Reshape)            (None, 65536, 16)    0           ['conv2d_6[0][0]']               \n","                                                                                                  \n"," reshape_3 (Reshape)            (None, 16, 1)        0           ['softmax_1[0][0]']              \n","                                                                                                  \n"," lambda (Lambda)                (None, 16, 1)        0           ['reshape[0][0]',                \n","                                                                  'softmax[0][0]']                \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 128, 128, 64  256        ['conv2d_34[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," lambda_1 (Lambda)              (None, 65536, 1)     0           ['reshape_4[0][0]',              \n","                                                                  'reshape_3[0][0]']              \n","                                                                                                  \n"," permute (Permute)              (None, 1, 16)        0           ['lambda[0][0]']                 \n","                                                                                                  \n"," activation_22 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_14[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," reshape_5 (Reshape)            (None, 65536)        0           ['lambda_1[0][0]']               \n","                                                                                                  \n"," reshape_2 (Reshape)            (None, 1, 1, 16)     0           ['permute[0][0]']                \n","                                                                                                  \n"," conv2d_35 (Conv2D)             (None, 128, 128, 64  36928       ['activation_22[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_3 (Activation)      (None, 65536)        0           ['reshape_5[0][0]']              \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 1, 1, 32)     544         ['reshape_2[0][0]']              \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 128, 128, 64  256        ['conv2d_35[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," reshape_6 (Reshape)            (None, 256, 256, 1)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," activation_2 (Activation)      (None, 1, 1, 32)     0           ['conv2d_4[0][0]']               \n","                                                                                                  \n"," activation_23 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_15[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," multiply_1 (Multiply)          (None, 256, 256, 32  0           ['activation_1[0][0]',           \n","                                )                                 'reshape_6[0][0]']              \n","                                                                                                  \n"," multiply (Multiply)            (None, 256, 256, 32  0           ['activation_1[0][0]',           \n","                                )                                 'activation_2[0][0]']           \n","                                                                                                  \n"," up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64  0          ['activation_23[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," add (Add)                      (None, 256, 256, 32  0           ['multiply_1[0][0]',             \n","                                )                                 'multiply[0][0]']               \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 256, 256, 96  0           ['up_sampling2d_3[0][0]',        \n","                                )                                 'add[0][0]']                    \n","                                                                                                  \n"," conv2d_36 (Conv2D)             (None, 256, 256, 32  27680       ['concatenate_3[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 256, 256, 32  128        ['conv2d_36[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_24 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_16[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_37 (Conv2D)             (None, 256, 256, 32  9248        ['activation_24[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 256, 256, 32  128        ['conv2d_37[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_25 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_17[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_38 (Conv2D)             (None, 256, 256, 1)  33          ['activation_25[0][0]']          \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 256, 256, 1)  4          ['conv2d_38[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_26 (Activation)     (None, 256, 256, 1)  0           ['batch_normalization_18[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 8,035,493\n","Trainable params: 8,029,603\n","Non-trainable params: 5,890\n","__________________________________________________________________________________________________\n"]}],"source":["from keras import metrics\n","unet2= daunet(input_shape=(256,256,10))#binary_crossentropy\n","unet2.compile(optimizer = adam_v2.Adam(learning_rate = 1e-4), loss =binary_crossentropy, metrics = ['accuracy'])\n","unet2.summary()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:36:05.764376Z","iopub.status.busy":"2023-04-05T10:36:05.763666Z","iopub.status.idle":"2023-04-05T12:40:28.380861Z","shell.execute_reply":"2023-04-05T12:40:28.379648Z","shell.execute_reply.started":"2023-04-05T10:36:05.764337Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.4505 - accuracy: 0.8893\n","Epoch 1: val_loss improved from inf to 0.66153, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 32s 377ms/step - loss: 0.4505 - accuracy: 0.8893 - val_loss: 0.6615 - val_accuracy: 0.6561 - lr: 1.0000e-04\n","Epoch 2/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3934 - accuracy: 0.9505\n","Epoch 2: val_loss did not improve from 0.66153\n","56/56 [==============================] - 15s 273ms/step - loss: 0.3934 - accuracy: 0.9505 - val_loss: 0.6697 - val_accuracy: 0.6840 - lr: 1.0000e-04\n","Epoch 3/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3879 - accuracy: 0.9526\n","Epoch 3: val_loss improved from 0.66153 to 0.62410, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3879 - accuracy: 0.9526 - val_loss: 0.6241 - val_accuracy: 0.7675 - lr: 1.0000e-04\n","Epoch 4/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3821 - accuracy: 0.9591\n","Epoch 4: val_loss improved from 0.62410 to 0.60472, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.3821 - accuracy: 0.9591 - val_loss: 0.6047 - val_accuracy: 0.7880 - lr: 1.0000e-04\n","Epoch 5/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3727 - accuracy: 0.9681\n","Epoch 5: val_loss improved from 0.60472 to 0.55584, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3727 - accuracy: 0.9681 - val_loss: 0.5558 - val_accuracy: 0.8282 - lr: 1.0000e-04\n","Epoch 6/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3683 - accuracy: 0.9691\n","Epoch 6: val_loss improved from 0.55584 to 0.50242, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3683 - accuracy: 0.9691 - val_loss: 0.5024 - val_accuracy: 0.8772 - lr: 1.0000e-04\n","Epoch 7/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3662 - accuracy: 0.9697\n","Epoch 7: val_loss improved from 0.50242 to 0.46856, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3662 - accuracy: 0.9697 - val_loss: 0.4686 - val_accuracy: 0.8831 - lr: 1.0000e-04\n","Epoch 8/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3609 - accuracy: 0.9748\n","Epoch 8: val_loss improved from 0.46856 to 0.38625, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3609 - accuracy: 0.9748 - val_loss: 0.3863 - val_accuracy: 0.9375 - lr: 1.0000e-04\n","Epoch 9/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3567 - accuracy: 0.9755\n","Epoch 9: val_loss did not improve from 0.38625\n","56/56 [==============================] - 15s 276ms/step - loss: 0.3567 - accuracy: 0.9755 - val_loss: 0.4033 - val_accuracy: 0.9151 - lr: 1.0000e-04\n","Epoch 10/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3582 - accuracy: 0.9739\n","Epoch 10: val_loss improved from 0.38625 to 0.34739, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3582 - accuracy: 0.9739 - val_loss: 0.3474 - val_accuracy: 0.9814 - lr: 1.0000e-04\n","Epoch 11/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3573 - accuracy: 0.9741\n","Epoch 11: val_loss improved from 0.34739 to 0.33967, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3573 - accuracy: 0.9741 - val_loss: 0.3397 - val_accuracy: 0.9822 - lr: 1.0000e-04\n","Epoch 12/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3518 - accuracy: 0.9754\n","Epoch 12: val_loss improved from 0.33967 to 0.33911, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3518 - accuracy: 0.9754 - val_loss: 0.3391 - val_accuracy: 0.9791 - lr: 1.0000e-04\n","Epoch 13/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3473 - accuracy: 0.9788\n","Epoch 13: val_loss did not improve from 0.33911\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3473 - accuracy: 0.9788 - val_loss: 0.3412 - val_accuracy: 0.9832 - lr: 1.0000e-04\n","Epoch 14/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3425 - accuracy: 0.9824\n","Epoch 14: val_loss did not improve from 0.33911\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3425 - accuracy: 0.9824 - val_loss: 0.3393 - val_accuracy: 0.9847 - lr: 1.0000e-04\n","Epoch 15/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3429 - accuracy: 0.9790\n","Epoch 15: val_loss improved from 0.33911 to 0.33547, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3429 - accuracy: 0.9790 - val_loss: 0.3355 - val_accuracy: 0.9805 - lr: 1.0000e-04\n","Epoch 16/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3393 - accuracy: 0.9817\n","Epoch 16: val_loss improved from 0.33547 to 0.33479, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3393 - accuracy: 0.9817 - val_loss: 0.3348 - val_accuracy: 0.9804 - lr: 1.0000e-04\n","Epoch 17/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3363 - accuracy: 0.9823\n","Epoch 17: val_loss improved from 0.33479 to 0.33200, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3363 - accuracy: 0.9823 - val_loss: 0.3320 - val_accuracy: 0.9853 - lr: 1.0000e-04\n","Epoch 18/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.9832\n","Epoch 18: val_loss improved from 0.33200 to 0.33162, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.3340 - accuracy: 0.9832 - val_loss: 0.3316 - val_accuracy: 0.9855 - lr: 1.0000e-04\n","Epoch 19/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3337 - accuracy: 0.9831\n","Epoch 19: val_loss improved from 0.33162 to 0.31896, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3337 - accuracy: 0.9831 - val_loss: 0.3190 - val_accuracy: 0.9853 - lr: 1.0000e-04\n","Epoch 20/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.9835\n","Epoch 20: val_loss did not improve from 0.31896\n","56/56 [==============================] - 15s 274ms/step - loss: 0.3296 - accuracy: 0.9835 - val_loss: 0.3215 - val_accuracy: 0.9844 - lr: 1.0000e-04\n","Epoch 21/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3255 - accuracy: 0.9863\n","Epoch 21: val_loss did not improve from 0.31896\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3255 - accuracy: 0.9863 - val_loss: 0.3231 - val_accuracy: 0.9854 - lr: 1.0000e-04\n","Epoch 22/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3212 - accuracy: 0.9865\n","Epoch 22: val_loss did not improve from 0.31896\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3212 - accuracy: 0.9865 - val_loss: 0.3205 - val_accuracy: 0.9864 - lr: 1.0000e-04\n","Epoch 23/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.9867\n","Epoch 23: val_loss did not improve from 0.31896\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3209 - accuracy: 0.9867 - val_loss: 0.3274 - val_accuracy: 0.9828 - lr: 1.0000e-04\n","Epoch 24/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3246 - accuracy: 0.9812\n","Epoch 24: val_loss did not improve from 0.31896\n","56/56 [==============================] - 15s 274ms/step - loss: 0.3246 - accuracy: 0.9812 - val_loss: 0.3260 - val_accuracy: 0.9787 - lr: 1.0000e-04\n","Epoch 25/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3202 - accuracy: 0.9832\n","Epoch 25: val_loss improved from 0.31896 to 0.31646, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3202 - accuracy: 0.9832 - val_loss: 0.3165 - val_accuracy: 0.9844 - lr: 1.0000e-04\n","Epoch 26/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3187 - accuracy: 0.9832\n","Epoch 26: val_loss did not improve from 0.31646\n","56/56 [==============================] - 15s 274ms/step - loss: 0.3187 - accuracy: 0.9832 - val_loss: 0.3190 - val_accuracy: 0.9868 - lr: 1.0000e-04\n","Epoch 27/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.9849\n","Epoch 27: val_loss improved from 0.31646 to 0.30898, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.3146 - accuracy: 0.9849 - val_loss: 0.3090 - val_accuracy: 0.9863 - lr: 1.0000e-04\n","Epoch 28/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3143 - accuracy: 0.9844\n","Epoch 28: val_loss improved from 0.30898 to 0.30625, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3143 - accuracy: 0.9844 - val_loss: 0.3063 - val_accuracy: 0.9846 - lr: 1.0000e-04\n","Epoch 29/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.9870\n","Epoch 29: val_loss did not improve from 0.30625\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3094 - accuracy: 0.9870 - val_loss: 0.3087 - val_accuracy: 0.9872 - lr: 1.0000e-04\n","Epoch 30/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.9869\n","Epoch 30: val_loss improved from 0.30625 to 0.30617, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3075 - accuracy: 0.9869 - val_loss: 0.3062 - val_accuracy: 0.9869 - lr: 1.0000e-04\n","Epoch 31/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3054 - accuracy: 0.9886\n","Epoch 31: val_loss improved from 0.30617 to 0.30545, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3054 - accuracy: 0.9886 - val_loss: 0.3055 - val_accuracy: 0.9875 - lr: 1.0000e-04\n","Epoch 32/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.9886\n","Epoch 32: val_loss improved from 0.30545 to 0.30099, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3046 - accuracy: 0.9886 - val_loss: 0.3010 - val_accuracy: 0.9878 - lr: 1.0000e-04\n","Epoch 33/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3028 - accuracy: 0.9882\n","Epoch 33: val_loss improved from 0.30099 to 0.29775, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3028 - accuracy: 0.9882 - val_loss: 0.2978 - val_accuracy: 0.9870 - lr: 1.0000e-04\n","Epoch 34/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3016 - accuracy: 0.9871\n","Epoch 34: val_loss did not improve from 0.29775\n","56/56 [==============================] - 15s 276ms/step - loss: 0.3016 - accuracy: 0.9871 - val_loss: 0.3140 - val_accuracy: 0.9752 - lr: 1.0000e-04\n","Epoch 35/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.9841\n","Epoch 35: val_loss improved from 0.29775 to 0.29772, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3021 - accuracy: 0.9841 - val_loss: 0.2977 - val_accuracy: 0.9877 - lr: 1.0000e-04\n","Epoch 36/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.9876\n","Epoch 36: val_loss did not improve from 0.29772\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2974 - accuracy: 0.9876 - val_loss: 0.3099 - val_accuracy: 0.9793 - lr: 1.0000e-04\n","Epoch 37/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.9870\n","Epoch 37: val_loss did not improve from 0.29772\n","56/56 [==============================] - 15s 276ms/step - loss: 0.2951 - accuracy: 0.9870 - val_loss: 0.3027 - val_accuracy: 0.9867 - lr: 1.0000e-04\n","Epoch 38/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.9875\n","Epoch 38: val_loss improved from 0.29772 to 0.28929, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.2961 - accuracy: 0.9875 - val_loss: 0.2893 - val_accuracy: 0.9869 - lr: 1.0000e-04\n","Epoch 39/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.9895\n","Epoch 39: val_loss improved from 0.28929 to 0.28897, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.2897 - accuracy: 0.9895 - val_loss: 0.2890 - val_accuracy: 0.9872 - lr: 1.0000e-04\n","Epoch 40/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9889\n","Epoch 40: val_loss improved from 0.28897 to 0.28417, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2876 - accuracy: 0.9889 - val_loss: 0.2842 - val_accuracy: 0.9887 - lr: 1.0000e-04\n","Epoch 41/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2860 - accuracy: 0.9894\n","Epoch 41: val_loss did not improve from 0.28417\n","56/56 [==============================] - 15s 277ms/step - loss: 0.2860 - accuracy: 0.9894 - val_loss: 0.3003 - val_accuracy: 0.9776 - lr: 1.0000e-04\n","Epoch 42/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2875 - accuracy: 0.9892\n","Epoch 42: val_loss did not improve from 0.28417\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2875 - accuracy: 0.9892 - val_loss: 0.2880 - val_accuracy: 0.9875 - lr: 1.0000e-04\n","Epoch 43/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 0.9902\n","Epoch 43: val_loss improved from 0.28417 to 0.28382, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2825 - accuracy: 0.9902 - val_loss: 0.2838 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 44/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2837 - accuracy: 0.9880\n","Epoch 44: val_loss improved from 0.28382 to 0.28138, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2837 - accuracy: 0.9880 - val_loss: 0.2814 - val_accuracy: 0.9882 - lr: 1.0000e-04\n","Epoch 45/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2793 - accuracy: 0.9906\n","Epoch 45: val_loss improved from 0.28138 to 0.27919, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.2793 - accuracy: 0.9906 - val_loss: 0.2792 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 46/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.9902\n","Epoch 46: val_loss improved from 0.27919 to 0.27742, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2788 - accuracy: 0.9902 - val_loss: 0.2774 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 47/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.9906\n","Epoch 47: val_loss improved from 0.27742 to 0.27725, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2757 - accuracy: 0.9906 - val_loss: 0.2773 - val_accuracy: 0.9879 - lr: 1.0000e-04\n","Epoch 48/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.9912\n","Epoch 48: val_loss improved from 0.27725 to 0.27477, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2749 - accuracy: 0.9912 - val_loss: 0.2748 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 49/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2732 - accuracy: 0.9909\n","Epoch 49: val_loss improved from 0.27477 to 0.26998, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.2732 - accuracy: 0.9909 - val_loss: 0.2700 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 50/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.9918\n","Epoch 50: val_loss did not improve from 0.26998\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2703 - accuracy: 0.9918 - val_loss: 0.2712 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 51/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2713 - accuracy: 0.9917\n","Epoch 51: val_loss did not improve from 0.26998\n","56/56 [==============================] - 15s 276ms/step - loss: 0.2713 - accuracy: 0.9917 - val_loss: 0.2704 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 52/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.9911\n","Epoch 52: val_loss did not improve from 0.26998\n","56/56 [==============================] - 15s 276ms/step - loss: 0.2673 - accuracy: 0.9911 - val_loss: 0.2723 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 53/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9908\n","Epoch 53: val_loss did not improve from 0.26998\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2699 - accuracy: 0.9908 - val_loss: 0.2706 - val_accuracy: 0.9883 - lr: 1.0000e-04\n","Epoch 54/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2632 - accuracy: 0.9917\n","Epoch 54: val_loss improved from 0.26998 to 0.26544, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2632 - accuracy: 0.9917 - val_loss: 0.2654 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 55/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.9915\n","Epoch 55: val_loss improved from 0.26544 to 0.26367, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2638 - accuracy: 0.9915 - val_loss: 0.2637 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 56/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9893\n","Epoch 56: val_loss did not improve from 0.26367\n","56/56 [==============================] - 15s 276ms/step - loss: 0.2647 - accuracy: 0.9893 - val_loss: 0.2654 - val_accuracy: 0.9878 - lr: 1.0000e-04\n","Epoch 57/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.9904\n","Epoch 57: val_loss improved from 0.26367 to 0.25711, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2630 - accuracy: 0.9904 - val_loss: 0.2571 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 58/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9891\n","Epoch 58: val_loss improved from 0.25711 to 0.25645, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2610 - accuracy: 0.9891 - val_loss: 0.2564 - val_accuracy: 0.9882 - lr: 1.0000e-04\n","Epoch 59/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2598 - accuracy: 0.9908\n","Epoch 59: val_loss improved from 0.25645 to 0.25465, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2598 - accuracy: 0.9908 - val_loss: 0.2546 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 60/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.9905\n","Epoch 60: val_loss did not improve from 0.25465\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2559 - accuracy: 0.9905 - val_loss: 0.2628 - val_accuracy: 0.9846 - lr: 1.0000e-04\n","Epoch 61/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.9911\n","Epoch 61: val_loss did not improve from 0.25465\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2539 - accuracy: 0.9911 - val_loss: 0.2554 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 62/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.9922\n","Epoch 62: val_loss improved from 0.25465 to 0.25157, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2513 - accuracy: 0.9922 - val_loss: 0.2516 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 63/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.9917\n","Epoch 63: val_loss did not improve from 0.25157\n","56/56 [==============================] - 15s 276ms/step - loss: 0.2513 - accuracy: 0.9917 - val_loss: 0.2558 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 64/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.9914\n","Epoch 64: val_loss improved from 0.25157 to 0.25000, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2518 - accuracy: 0.9914 - val_loss: 0.2500 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 65/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.9921\n","Epoch 65: val_loss improved from 0.25000 to 0.24760, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2475 - accuracy: 0.9921 - val_loss: 0.2476 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 66/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.9906\n","Epoch 66: val_loss improved from 0.24760 to 0.24652, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.2482 - accuracy: 0.9906 - val_loss: 0.2465 - val_accuracy: 0.9893 - lr: 1.0000e-04\n","Epoch 67/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.9917\n","Epoch 67: val_loss did not improve from 0.24652\n","56/56 [==============================] - 15s 277ms/step - loss: 0.2468 - accuracy: 0.9917 - val_loss: 0.2465 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 68/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.9917\n","Epoch 68: val_loss improved from 0.24652 to 0.24253, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2442 - accuracy: 0.9917 - val_loss: 0.2425 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 69/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.9925\n","Epoch 69: val_loss improved from 0.24253 to 0.24177, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2418 - accuracy: 0.9925 - val_loss: 0.2418 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 70/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2401 - accuracy: 0.9929\n","Epoch 70: val_loss did not improve from 0.24177\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2401 - accuracy: 0.9929 - val_loss: 0.2432 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 71/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.9874\n","Epoch 71: val_loss did not improve from 0.24177\n","56/56 [==============================] - 15s 276ms/step - loss: 0.2443 - accuracy: 0.9874 - val_loss: 0.2543 - val_accuracy: 0.9797 - lr: 1.0000e-04\n","Epoch 72/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2412 - accuracy: 0.9885\n","Epoch 72: val_loss improved from 0.24177 to 0.23780, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2412 - accuracy: 0.9885 - val_loss: 0.2378 - val_accuracy: 0.9876 - lr: 1.0000e-04\n","Epoch 73/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2412 - accuracy: 0.9897\n","Epoch 73: val_loss did not improve from 0.23780\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2412 - accuracy: 0.9897 - val_loss: 0.2401 - val_accuracy: 0.9870 - lr: 1.0000e-04\n","Epoch 74/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.9918\n","Epoch 74: val_loss improved from 0.23780 to 0.23707, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2385 - accuracy: 0.9918 - val_loss: 0.2371 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 75/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2353 - accuracy: 0.9918\n","Epoch 75: val_loss improved from 0.23707 to 0.23596, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.2353 - accuracy: 0.9918 - val_loss: 0.2360 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 76/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2359 - accuracy: 0.9900\n","Epoch 76: val_loss improved from 0.23596 to 0.23524, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.2359 - accuracy: 0.9900 - val_loss: 0.2352 - val_accuracy: 0.9891 - lr: 1.0000e-04\n","Epoch 77/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.9917\n","Epoch 77: val_loss improved from 0.23524 to 0.23377, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2345 - accuracy: 0.9917 - val_loss: 0.2338 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 78/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2298 - accuracy: 0.9924\n","Epoch 78: val_loss improved from 0.23377 to 0.22838, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2298 - accuracy: 0.9924 - val_loss: 0.2284 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 79/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9928\n","Epoch 79: val_loss did not improve from 0.22838\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2303 - accuracy: 0.9928 - val_loss: 0.2289 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 80/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2285 - accuracy: 0.9929\n","Epoch 80: val_loss did not improve from 0.22838\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2285 - accuracy: 0.9929 - val_loss: 0.2293 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 81/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2268 - accuracy: 0.9932\n","Epoch 81: val_loss did not improve from 0.22838\n","56/56 [==============================] - 15s 276ms/step - loss: 0.2268 - accuracy: 0.9932 - val_loss: 0.2298 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 82/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9932\n","Epoch 82: val_loss improved from 0.22838 to 0.22571, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2244 - accuracy: 0.9932 - val_loss: 0.2257 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 83/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.9927\n","Epoch 83: val_loss improved from 0.22571 to 0.22216, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2248 - accuracy: 0.9927 - val_loss: 0.2222 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 84/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2202 - accuracy: 0.9935\n","Epoch 84: val_loss did not improve from 0.22216\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2202 - accuracy: 0.9935 - val_loss: 0.2227 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 85/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2203 - accuracy: 0.9930\n","Epoch 85: val_loss improved from 0.22216 to 0.22048, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2203 - accuracy: 0.9930 - val_loss: 0.2205 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 86/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.9933\n","Epoch 86: val_loss did not improve from 0.22048\n","56/56 [==============================] - 15s 276ms/step - loss: 0.2200 - accuracy: 0.9933 - val_loss: 0.2207 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 87/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2162 - accuracy: 0.9935\n","Epoch 87: val_loss improved from 0.22048 to 0.21783, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.2162 - accuracy: 0.9935 - val_loss: 0.2178 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 88/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2177 - accuracy: 0.9936\n","Epoch 88: val_loss improved from 0.21783 to 0.21544, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2177 - accuracy: 0.9936 - val_loss: 0.2154 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 89/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2171 - accuracy: 0.9937\n","Epoch 89: val_loss improved from 0.21544 to 0.21535, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2171 - accuracy: 0.9937 - val_loss: 0.2154 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 90/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2180 - accuracy: 0.9925\n","Epoch 90: val_loss improved from 0.21535 to 0.21315, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2180 - accuracy: 0.9925 - val_loss: 0.2132 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 91/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2318 - accuracy: 0.9794\n","Epoch 91: val_loss did not improve from 0.21315\n","56/56 [==============================] - 15s 277ms/step - loss: 0.2318 - accuracy: 0.9794 - val_loss: 0.2966 - val_accuracy: 0.9178 - lr: 1.0000e-04\n","Epoch 92/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.9853\n","Epoch 92: val_loss improved from 0.21315 to 0.20934, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2217 - accuracy: 0.9853 - val_loss: 0.2093 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 93/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2163 - accuracy: 0.9887\n","Epoch 93: val_loss did not improve from 0.20934\n","56/56 [==============================] - 15s 277ms/step - loss: 0.2163 - accuracy: 0.9887 - val_loss: 0.2143 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 94/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9911\n","Epoch 94: val_loss did not improve from 0.20934\n","56/56 [==============================] - 15s 276ms/step - loss: 0.2120 - accuracy: 0.9911 - val_loss: 0.2119 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 95/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2100 - accuracy: 0.9916\n","Epoch 95: val_loss improved from 0.20934 to 0.20674, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2100 - accuracy: 0.9916 - val_loss: 0.2067 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 96/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9923\n","Epoch 96: val_loss did not improve from 0.20674\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2106 - accuracy: 0.9923 - val_loss: 0.2068 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 97/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2083 - accuracy: 0.9925\n","Epoch 97: val_loss did not improve from 0.20674\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2083 - accuracy: 0.9925 - val_loss: 0.2090 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 98/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9929\n","Epoch 98: val_loss improved from 0.20674 to 0.20591, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2031 - accuracy: 0.9929 - val_loss: 0.2059 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 99/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.9930\n","Epoch 99: val_loss improved from 0.20591 to 0.20427, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2069 - accuracy: 0.9930 - val_loss: 0.2043 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 100/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2021 - accuracy: 0.9934\n","Epoch 100: val_loss improved from 0.20427 to 0.20362, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2021 - accuracy: 0.9934 - val_loss: 0.2036 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 101/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2018 - accuracy: 0.9933\n","Epoch 101: val_loss improved from 0.20362 to 0.20138, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2018 - accuracy: 0.9933 - val_loss: 0.2014 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 102/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2032 - accuracy: 0.9934\n","Epoch 102: val_loss improved from 0.20138 to 0.19868, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 288ms/step - loss: 0.2032 - accuracy: 0.9934 - val_loss: 0.1987 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 103/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2001 - accuracy: 0.9937\n","Epoch 103: val_loss improved from 0.19868 to 0.19780, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2001 - accuracy: 0.9937 - val_loss: 0.1978 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 104/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2008 - accuracy: 0.9931\n","Epoch 104: val_loss did not improve from 0.19780\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2008 - accuracy: 0.9931 - val_loss: 0.1998 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 105/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1991 - accuracy: 0.9917\n","Epoch 105: val_loss improved from 0.19780 to 0.19582, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1991 - accuracy: 0.9917 - val_loss: 0.1958 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 106/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2019 - accuracy: 0.9928\n","Epoch 106: val_loss did not improve from 0.19582\n","56/56 [==============================] - 15s 276ms/step - loss: 0.2019 - accuracy: 0.9928 - val_loss: 0.1963 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 107/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1957 - accuracy: 0.9932\n","Epoch 107: val_loss improved from 0.19582 to 0.19461, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1957 - accuracy: 0.9932 - val_loss: 0.1946 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 108/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1947 - accuracy: 0.9936\n","Epoch 108: val_loss improved from 0.19461 to 0.19435, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1947 - accuracy: 0.9936 - val_loss: 0.1943 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 109/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9937\n","Epoch 109: val_loss improved from 0.19435 to 0.19265, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1938 - accuracy: 0.9937 - val_loss: 0.1926 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 110/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.9942\n","Epoch 110: val_loss improved from 0.19265 to 0.19129, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1894 - accuracy: 0.9942 - val_loss: 0.1913 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 111/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9940\n","Epoch 111: val_loss improved from 0.19129 to 0.18971, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1909 - accuracy: 0.9940 - val_loss: 0.1897 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 112/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9941\n","Epoch 112: val_loss improved from 0.18971 to 0.18890, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1898 - accuracy: 0.9941 - val_loss: 0.1889 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 113/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1878 - accuracy: 0.9943\n","Epoch 113: val_loss improved from 0.18890 to 0.18793, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1878 - accuracy: 0.9943 - val_loss: 0.1879 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 114/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1863 - accuracy: 0.9943\n","Epoch 114: val_loss improved from 0.18793 to 0.18560, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1863 - accuracy: 0.9943 - val_loss: 0.1856 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 115/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 0.9945\n","Epoch 115: val_loss improved from 0.18560 to 0.18521, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1859 - accuracy: 0.9945 - val_loss: 0.1852 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 116/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1861 - accuracy: 0.9944\n","Epoch 116: val_loss improved from 0.18521 to 0.18496, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1861 - accuracy: 0.9944 - val_loss: 0.1850 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 117/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1882 - accuracy: 0.9938\n","Epoch 117: val_loss improved from 0.18496 to 0.18285, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1882 - accuracy: 0.9938 - val_loss: 0.1828 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 118/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1850 - accuracy: 0.9942\n","Epoch 118: val_loss improved from 0.18285 to 0.18141, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1850 - accuracy: 0.9942 - val_loss: 0.1814 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 119/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1826 - accuracy: 0.9942\n","Epoch 119: val_loss improved from 0.18141 to 0.18012, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1826 - accuracy: 0.9942 - val_loss: 0.1801 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 120/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1806 - accuracy: 0.9945\n","Epoch 120: val_loss did not improve from 0.18012\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1806 - accuracy: 0.9945 - val_loss: 0.1802 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 121/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9945\n","Epoch 121: val_loss improved from 0.18012 to 0.17954, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1799 - accuracy: 0.9945 - val_loss: 0.1795 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 122/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9945\n","Epoch 122: val_loss improved from 0.17954 to 0.17719, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1815 - accuracy: 0.9945 - val_loss: 0.1772 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 123/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9945\n","Epoch 123: val_loss improved from 0.17719 to 0.17697, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1774 - accuracy: 0.9945 - val_loss: 0.1770 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 124/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1781 - accuracy: 0.9947\n","Epoch 124: val_loss improved from 0.17697 to 0.17540, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1781 - accuracy: 0.9947 - val_loss: 0.1754 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 125/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1742 - accuracy: 0.9949\n","Epoch 125: val_loss improved from 0.17540 to 0.17464, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1742 - accuracy: 0.9949 - val_loss: 0.1746 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 126/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9950\n","Epoch 126: val_loss improved from 0.17464 to 0.17383, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1739 - accuracy: 0.9950 - val_loss: 0.1738 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 127/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1758 - accuracy: 0.9947\n","Epoch 127: val_loss did not improve from 0.17383\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1758 - accuracy: 0.9947 - val_loss: 0.1757 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 128/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1738 - accuracy: 0.9945\n","Epoch 128: val_loss improved from 0.17383 to 0.17169, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1738 - accuracy: 0.9945 - val_loss: 0.1717 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 129/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1787 - accuracy: 0.9914\n","Epoch 129: val_loss did not improve from 0.17169\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1787 - accuracy: 0.9914 - val_loss: 0.1726 - val_accuracy: 0.9866 - lr: 1.0000e-04\n","Epoch 130/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9912\n","Epoch 130: val_loss improved from 0.17169 to 0.16762, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1770 - accuracy: 0.9912 - val_loss: 0.1676 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 131/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.9939\n","Epoch 131: val_loss did not improve from 0.16762\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1706 - accuracy: 0.9939 - val_loss: 0.1692 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 132/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1710 - accuracy: 0.9923\n","Epoch 132: val_loss did not improve from 0.16762\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1710 - accuracy: 0.9923 - val_loss: 0.1822 - val_accuracy: 0.9854 - lr: 1.0000e-04\n","Epoch 133/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1745 - accuracy: 0.9911\n","Epoch 133: val_loss did not improve from 0.16762\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1745 - accuracy: 0.9911 - val_loss: 0.1716 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 134/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9926\n","Epoch 134: val_loss improved from 0.16762 to 0.16574, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1691 - accuracy: 0.9926 - val_loss: 0.1657 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 135/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9910\n","Epoch 135: val_loss did not improve from 0.16574\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1726 - accuracy: 0.9910 - val_loss: 0.1662 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 136/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1692 - accuracy: 0.9932\n","Epoch 136: val_loss did not improve from 0.16574\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1692 - accuracy: 0.9932 - val_loss: 0.1697 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 137/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9942\n","Epoch 137: val_loss improved from 0.16574 to 0.16441, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1667 - accuracy: 0.9942 - val_loss: 0.1644 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 138/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9945\n","Epoch 138: val_loss did not improve from 0.16441\n","56/56 [==============================] - 15s 273ms/step - loss: 0.1644 - accuracy: 0.9945 - val_loss: 0.1652 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 139/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1625 - accuracy: 0.9947\n","Epoch 139: val_loss did not improve from 0.16441\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1625 - accuracy: 0.9947 - val_loss: 0.1646 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 140/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1637 - accuracy: 0.9941\n","Epoch 140: val_loss did not improve from 0.16441\n","56/56 [==============================] - 15s 273ms/step - loss: 0.1637 - accuracy: 0.9941 - val_loss: 0.1684 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 141/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9934\n","Epoch 141: val_loss improved from 0.16441 to 0.16407, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1633 - accuracy: 0.9934 - val_loss: 0.1641 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 142/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1580 - accuracy: 0.9947\n","Epoch 142: val_loss improved from 0.16407 to 0.16116, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1580 - accuracy: 0.9947 - val_loss: 0.1612 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 143/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9946\n","Epoch 143: val_loss improved from 0.16116 to 0.15935, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1610 - accuracy: 0.9946 - val_loss: 0.1593 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 144/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1607 - accuracy: 0.9948\n","Epoch 144: val_loss improved from 0.15935 to 0.15843, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1607 - accuracy: 0.9948 - val_loss: 0.1584 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 145/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1592 - accuracy: 0.9951\n","Epoch 145: val_loss improved from 0.15843 to 0.15790, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1592 - accuracy: 0.9951 - val_loss: 0.1579 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 146/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1568 - accuracy: 0.9950\n","Epoch 146: val_loss improved from 0.15790 to 0.15643, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1568 - accuracy: 0.9950 - val_loss: 0.1564 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 147/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.9951\n","Epoch 147: val_loss improved from 0.15643 to 0.15534, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1565 - accuracy: 0.9951 - val_loss: 0.1553 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 148/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1556 - accuracy: 0.9953\n","Epoch 148: val_loss improved from 0.15534 to 0.15499, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1556 - accuracy: 0.9953 - val_loss: 0.1550 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 149/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1533 - accuracy: 0.9953\n","Epoch 149: val_loss improved from 0.15499 to 0.15406, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1533 - accuracy: 0.9953 - val_loss: 0.1541 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 150/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9954\n","Epoch 150: val_loss improved from 0.15406 to 0.15113, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 281ms/step - loss: 0.1527 - accuracy: 0.9954 - val_loss: 0.1511 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 151/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1507 - accuracy: 0.9954\n","Epoch 151: val_loss improved from 0.15113 to 0.15099, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1507 - accuracy: 0.9954 - val_loss: 0.1510 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 152/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1511 - accuracy: 0.9954\n","Epoch 152: val_loss improved from 0.15099 to 0.14964, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1511 - accuracy: 0.9954 - val_loss: 0.1496 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 153/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.9953\n","Epoch 153: val_loss improved from 0.14964 to 0.14704, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1513 - accuracy: 0.9953 - val_loss: 0.1470 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 154/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.9956\n","Epoch 154: val_loss did not improve from 0.14704\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1479 - accuracy: 0.9956 - val_loss: 0.1484 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 155/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1485 - accuracy: 0.9956\n","Epoch 155: val_loss improved from 0.14704 to 0.14692, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1485 - accuracy: 0.9956 - val_loss: 0.1469 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 156/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9955\n","Epoch 156: val_loss improved from 0.14692 to 0.14563, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1482 - accuracy: 0.9955 - val_loss: 0.1456 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 157/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 0.9954\n","Epoch 157: val_loss improved from 0.14563 to 0.14359, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1470 - accuracy: 0.9954 - val_loss: 0.1436 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 158/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1471 - accuracy: 0.9955\n","Epoch 158: val_loss did not improve from 0.14359\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1471 - accuracy: 0.9955 - val_loss: 0.1439 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 159/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.9955\n","Epoch 159: val_loss did not improve from 0.14359\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1468 - accuracy: 0.9955 - val_loss: 0.1440 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 160/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9957\n","Epoch 160: val_loss did not improve from 0.14359\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1446 - accuracy: 0.9957 - val_loss: 0.1437 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 161/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.9957\n","Epoch 161: val_loss improved from 0.14359 to 0.14288, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1462 - accuracy: 0.9957 - val_loss: 0.1429 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 162/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9958\n","Epoch 162: val_loss did not improve from 0.14288\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1417 - accuracy: 0.9958 - val_loss: 0.1433 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 163/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1415 - accuracy: 0.9957\n","Epoch 163: val_loss improved from 0.14288 to 0.14053, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1415 - accuracy: 0.9957 - val_loss: 0.1405 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 164/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1409 - accuracy: 0.9958\n","Epoch 164: val_loss improved from 0.14053 to 0.13926, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1409 - accuracy: 0.9958 - val_loss: 0.1393 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 165/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9956\n","Epoch 165: val_loss improved from 0.13926 to 0.13820, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1423 - accuracy: 0.9956 - val_loss: 0.1382 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 166/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 0.9957\n","Epoch 166: val_loss improved from 0.13820 to 0.13808, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1416 - accuracy: 0.9957 - val_loss: 0.1381 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 167/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9958\n","Epoch 167: val_loss improved from 0.13808 to 0.13746, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1397 - accuracy: 0.9958 - val_loss: 0.1375 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 168/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9958\n","Epoch 168: val_loss improved from 0.13746 to 0.13531, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1412 - accuracy: 0.9958 - val_loss: 0.1353 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 169/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1373 - accuracy: 0.9958\n","Epoch 169: val_loss did not improve from 0.13531\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1373 - accuracy: 0.9958 - val_loss: 0.1363 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 170/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1386 - accuracy: 0.9958\n","Epoch 170: val_loss improved from 0.13531 to 0.13474, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1386 - accuracy: 0.9958 - val_loss: 0.1347 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 171/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1455 - accuracy: 0.9913\n","Epoch 171: val_loss did not improve from 0.13474\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1455 - accuracy: 0.9913 - val_loss: 0.1424 - val_accuracy: 0.9861 - lr: 1.0000e-04\n","Epoch 172/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9910\n","Epoch 172: val_loss did not improve from 0.13474\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1433 - accuracy: 0.9910 - val_loss: 0.1360 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 173/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1382 - accuracy: 0.9939\n","Epoch 173: val_loss did not improve from 0.13474\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1382 - accuracy: 0.9939 - val_loss: 0.1349 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 174/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1358 - accuracy: 0.9946\n","Epoch 174: val_loss did not improve from 0.13474\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1358 - accuracy: 0.9946 - val_loss: 0.1358 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 175/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9952\n","Epoch 175: val_loss did not improve from 0.13474\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1361 - accuracy: 0.9952 - val_loss: 0.1352 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 176/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9956\n","Epoch 176: val_loss improved from 0.13474 to 0.13336, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1336 - accuracy: 0.9956 - val_loss: 0.1334 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 177/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9958\n","Epoch 177: val_loss improved from 0.13336 to 0.13282, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1319 - accuracy: 0.9958 - val_loss: 0.1328 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 178/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9956\n","Epoch 178: val_loss improved from 0.13282 to 0.12999, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.1324 - accuracy: 0.9956 - val_loss: 0.1300 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 179/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9956\n","Epoch 179: val_loss improved from 0.12999 to 0.12928, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1324 - accuracy: 0.9956 - val_loss: 0.1293 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 180/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1340 - accuracy: 0.9919\n","Epoch 180: val_loss did not improve from 0.12928\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1340 - accuracy: 0.9919 - val_loss: 0.1337 - val_accuracy: 0.9877 - lr: 1.0000e-04\n","Epoch 181/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9948\n","Epoch 181: val_loss improved from 0.12928 to 0.12745, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1301 - accuracy: 0.9948 - val_loss: 0.1275 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 182/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9956\n","Epoch 182: val_loss improved from 0.12745 to 0.12688, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1289 - accuracy: 0.9956 - val_loss: 0.1269 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 183/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1283 - accuracy: 0.9958\n","Epoch 183: val_loss did not improve from 0.12688\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1283 - accuracy: 0.9958 - val_loss: 0.1270 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 184/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9956\n","Epoch 184: val_loss improved from 0.12688 to 0.12537, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1277 - accuracy: 0.9956 - val_loss: 0.1254 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 185/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9943\n","Epoch 185: val_loss did not improve from 0.12537\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1281 - accuracy: 0.9943 - val_loss: 0.1324 - val_accuracy: 0.9880 - lr: 1.0000e-04\n","Epoch 186/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.9907\n","Epoch 186: val_loss did not improve from 0.12537\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1326 - accuracy: 0.9907 - val_loss: 0.1483 - val_accuracy: 0.9843 - lr: 1.0000e-04\n","Epoch 187/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.9920\n","Epoch 187: val_loss did not improve from 0.12537\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1321 - accuracy: 0.9920 - val_loss: 0.1269 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 188/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9945\n","Epoch 188: val_loss did not improve from 0.12537\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1251 - accuracy: 0.9945 - val_loss: 0.1280 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 189/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1290 - accuracy: 0.9950\n","Epoch 189: val_loss did not improve from 0.12537\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1290 - accuracy: 0.9950 - val_loss: 0.1267 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 190/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9952\n","Epoch 190: val_loss did not improve from 0.12537\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1241 - accuracy: 0.9952 - val_loss: 0.1262 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 191/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9956\n","Epoch 191: val_loss improved from 0.12537 to 0.12528, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1250 - accuracy: 0.9956 - val_loss: 0.1253 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 192/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9957\n","Epoch 192: val_loss improved from 0.12528 to 0.12266, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1225 - accuracy: 0.9957 - val_loss: 0.1227 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 193/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1221 - accuracy: 0.9959\n","Epoch 193: val_loss improved from 0.12266 to 0.12156, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1221 - accuracy: 0.9959 - val_loss: 0.1216 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 194/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9962\n","Epoch 194: val_loss improved from 0.12156 to 0.12035, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1205 - accuracy: 0.9962 - val_loss: 0.1203 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 195/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9960\n","Epoch 195: val_loss did not improve from 0.12035\n","56/56 [==============================] - 17s 299ms/step - loss: 0.1235 - accuracy: 0.9960 - val_loss: 0.1209 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 196/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9961\n","Epoch 196: val_loss improved from 0.12035 to 0.11925, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1204 - accuracy: 0.9961 - val_loss: 0.1192 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 197/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9959\n","Epoch 197: val_loss improved from 0.11925 to 0.11813, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1194 - accuracy: 0.9959 - val_loss: 0.1181 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 198/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1222 - accuracy: 0.9940\n","Epoch 198: val_loss did not improve from 0.11813\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1222 - accuracy: 0.9940 - val_loss: 0.1343 - val_accuracy: 0.9837 - lr: 1.0000e-04\n","Epoch 199/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9944\n","Epoch 199: val_loss improved from 0.11813 to 0.11442, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1216 - accuracy: 0.9944 - val_loss: 0.1144 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 200/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.9955\n","Epoch 200: val_loss did not improve from 0.11442\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1196 - accuracy: 0.9955 - val_loss: 0.1163 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 201/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 0.9959\n","Epoch 201: val_loss did not improve from 0.11442\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1182 - accuracy: 0.9959 - val_loss: 0.1164 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 202/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.9960\n","Epoch 202: val_loss did not improve from 0.11442\n","56/56 [==============================] - 15s 273ms/step - loss: 0.1196 - accuracy: 0.9960 - val_loss: 0.1161 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 203/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9960\n","Epoch 203: val_loss did not improve from 0.11442\n","56/56 [==============================] - 17s 299ms/step - loss: 0.1160 - accuracy: 0.9960 - val_loss: 0.1157 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 204/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9961\n","Epoch 204: val_loss improved from 0.11442 to 0.11314, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1145 - accuracy: 0.9961 - val_loss: 0.1131 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 205/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9962\n","Epoch 205: val_loss improved from 0.11314 to 0.11286, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1153 - accuracy: 0.9962 - val_loss: 0.1129 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 206/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9962\n","Epoch 206: val_loss improved from 0.11286 to 0.11172, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1130 - accuracy: 0.9962 - val_loss: 0.1117 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 207/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1137 - accuracy: 0.9963\n","Epoch 207: val_loss improved from 0.11172 to 0.11021, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1137 - accuracy: 0.9963 - val_loss: 0.1102 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 208/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1113 - accuracy: 0.9962\n","Epoch 208: val_loss did not improve from 0.11021\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1113 - accuracy: 0.9962 - val_loss: 0.1105 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 209/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9962\n","Epoch 209: val_loss did not improve from 0.11021\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1112 - accuracy: 0.9962 - val_loss: 0.1107 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 210/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9964\n","Epoch 210: val_loss did not improve from 0.11021\n","56/56 [==============================] - 15s 273ms/step - loss: 0.1087 - accuracy: 0.9964 - val_loss: 0.1110 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 211/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.9963\n","Epoch 211: val_loss improved from 0.11021 to 0.10967, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1094 - accuracy: 0.9963 - val_loss: 0.1097 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 212/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9964\n","Epoch 212: val_loss improved from 0.10967 to 0.10649, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1140 - accuracy: 0.9964 - val_loss: 0.1065 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 213/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9964\n","Epoch 213: val_loss did not improve from 0.10649\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1096 - accuracy: 0.9964 - val_loss: 0.1082 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 214/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9963\n","Epoch 214: val_loss did not improve from 0.10649\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1099 - accuracy: 0.9963 - val_loss: 0.1074 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 215/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1106 - accuracy: 0.9964\n","Epoch 215: val_loss did not improve from 0.10649\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1106 - accuracy: 0.9964 - val_loss: 0.1066 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 216/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9964\n","Epoch 216: val_loss did not improve from 0.10649\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1087 - accuracy: 0.9964 - val_loss: 0.1073 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 217/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9964\n","Epoch 217: val_loss did not improve from 0.10649\n","56/56 [==============================] - 17s 299ms/step - loss: 0.1071 - accuracy: 0.9964 - val_loss: 0.1070 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 218/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 0.9965\n","Epoch 218: val_loss improved from 0.10649 to 0.10592, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1064 - accuracy: 0.9965 - val_loss: 0.1059 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 219/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1053 - accuracy: 0.9965\n","Epoch 219: val_loss improved from 0.10592 to 0.10524, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1053 - accuracy: 0.9965 - val_loss: 0.1052 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 220/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.9967\n","Epoch 220: val_loss improved from 0.10524 to 0.10447, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1034 - accuracy: 0.9967 - val_loss: 0.1045 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 221/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9966\n","Epoch 221: val_loss did not improve from 0.10447\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1054 - accuracy: 0.9966 - val_loss: 0.1045 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 222/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1040 - accuracy: 0.9967\n","Epoch 222: val_loss improved from 0.10447 to 0.10293, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1040 - accuracy: 0.9967 - val_loss: 0.1029 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 223/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9967\n","Epoch 223: val_loss improved from 0.10293 to 0.10203, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 17s 309ms/step - loss: 0.1022 - accuracy: 0.9967 - val_loss: 0.1020 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 224/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1053 - accuracy: 0.9957\n","Epoch 224: val_loss did not improve from 0.10203\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1053 - accuracy: 0.9957 - val_loss: 0.1026 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 225/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9950\n","Epoch 225: val_loss improved from 0.10203 to 0.10200, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1043 - accuracy: 0.9950 - val_loss: 0.1020 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 226/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9959\n","Epoch 226: val_loss improved from 0.10200 to 0.10002, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1019 - accuracy: 0.9959 - val_loss: 0.1000 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 227/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9964\n","Epoch 227: val_loss did not improve from 0.10002\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1027 - accuracy: 0.9964 - val_loss: 0.1004 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 228/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9964\n","Epoch 228: val_loss improved from 0.10002 to 0.09966, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1043 - accuracy: 0.9964 - val_loss: 0.0997 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 229/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.9965\n","Epoch 229: val_loss did not improve from 0.09966\n","56/56 [==============================] - 15s 276ms/step - loss: 0.1028 - accuracy: 0.9965 - val_loss: 0.0998 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 230/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9966\n","Epoch 230: val_loss improved from 0.09966 to 0.09905, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0999 - accuracy: 0.9966 - val_loss: 0.0990 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 231/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9966\n","Epoch 231: val_loss improved from 0.09905 to 0.09852, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0993 - accuracy: 0.9966 - val_loss: 0.0985 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 232/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9966\n","Epoch 232: val_loss improved from 0.09852 to 0.09800, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0980 - accuracy: 0.9966 - val_loss: 0.0980 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 233/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0996 - accuracy: 0.9965\n","Epoch 233: val_loss improved from 0.09800 to 0.09638, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 17s 309ms/step - loss: 0.0996 - accuracy: 0.9965 - val_loss: 0.0964 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 234/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1000 - accuracy: 0.9965\n","Epoch 234: val_loss did not improve from 0.09638\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1000 - accuracy: 0.9965 - val_loss: 0.0968 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 235/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9967\n","Epoch 235: val_loss improved from 0.09638 to 0.09419, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1001 - accuracy: 0.9967 - val_loss: 0.0942 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 236/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9963\n","Epoch 236: val_loss did not improve from 0.09419\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0971 - accuracy: 0.9963 - val_loss: 0.0943 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 237/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9967\n","Epoch 237: val_loss did not improve from 0.09419\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0985 - accuracy: 0.9967 - val_loss: 0.0947 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 238/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9968\n","Epoch 238: val_loss did not improve from 0.09419\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0960 - accuracy: 0.9968 - val_loss: 0.0944 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 239/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9969\n","Epoch 239: val_loss did not improve from 0.09419\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0944 - accuracy: 0.9969 - val_loss: 0.0945 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 240/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0945 - accuracy: 0.9968\n","Epoch 240: val_loss improved from 0.09419 to 0.09314, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0945 - accuracy: 0.9968 - val_loss: 0.0931 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 241/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9969\n","Epoch 241: val_loss improved from 0.09314 to 0.09262, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0943 - accuracy: 0.9969 - val_loss: 0.0926 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 242/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9969\n","Epoch 242: val_loss did not improve from 0.09262\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0947 - accuracy: 0.9969 - val_loss: 0.0927 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 243/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0924 - accuracy: 0.9968\n","Epoch 243: val_loss improved from 0.09262 to 0.09122, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0924 - accuracy: 0.9968 - val_loss: 0.0912 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 244/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9969\n","Epoch 244: val_loss improved from 0.09122 to 0.09031, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0940 - accuracy: 0.9969 - val_loss: 0.0903 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 245/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9969\n","Epoch 245: val_loss did not improve from 0.09031\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0923 - accuracy: 0.9969 - val_loss: 0.0910 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 246/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9967\n","Epoch 246: val_loss improved from 0.09031 to 0.09016, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0943 - accuracy: 0.9967 - val_loss: 0.0902 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 247/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9968\n","Epoch 247: val_loss did not improve from 0.09016\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0947 - accuracy: 0.9968 - val_loss: 0.0904 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 248/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9969\n","Epoch 248: val_loss did not improve from 0.09016\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0923 - accuracy: 0.9969 - val_loss: 0.0904 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 249/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9969\n","Epoch 249: val_loss improved from 0.09016 to 0.08973, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0915 - accuracy: 0.9969 - val_loss: 0.0897 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 250/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9970\n","Epoch 250: val_loss improved from 0.08973 to 0.08936, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0889 - accuracy: 0.9970 - val_loss: 0.0894 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 251/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9970\n","Epoch 251: val_loss improved from 0.08936 to 0.08864, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0891 - accuracy: 0.9970 - val_loss: 0.0886 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 252/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9971\n","Epoch 252: val_loss improved from 0.08864 to 0.08682, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0906 - accuracy: 0.9971 - val_loss: 0.0868 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 253/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9971\n","Epoch 253: val_loss did not improve from 0.08682\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0898 - accuracy: 0.9971 - val_loss: 0.0873 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 254/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0894 - accuracy: 0.9972\n","Epoch 254: val_loss improved from 0.08682 to 0.08667, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0894 - accuracy: 0.9972 - val_loss: 0.0867 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 255/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9962\n","Epoch 255: val_loss improved from 0.08667 to 0.08664, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0888 - accuracy: 0.9962 - val_loss: 0.0866 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 256/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9967\n","Epoch 256: val_loss did not improve from 0.08664\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0881 - accuracy: 0.9967 - val_loss: 0.0867 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 257/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9969\n","Epoch 257: val_loss did not improve from 0.08664\n","56/56 [==============================] - 16s 278ms/step - loss: 0.0874 - accuracy: 0.9969 - val_loss: 0.0866 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 258/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9971\n","Epoch 258: val_loss improved from 0.08664 to 0.08607, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0862 - accuracy: 0.9971 - val_loss: 0.0861 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 259/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9970\n","Epoch 259: val_loss improved from 0.08607 to 0.08517, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0851 - accuracy: 0.9970 - val_loss: 0.0852 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 260/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9972\n","Epoch 260: val_loss did not improve from 0.08517\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0858 - accuracy: 0.9972 - val_loss: 0.0853 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 261/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9973\n","Epoch 261: val_loss improved from 0.08517 to 0.08468, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0850 - accuracy: 0.9973 - val_loss: 0.0847 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 262/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9972\n","Epoch 262: val_loss improved from 0.08468 to 0.08406, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.0876 - accuracy: 0.9972 - val_loss: 0.0841 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 263/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9971\n","Epoch 263: val_loss did not improve from 0.08406\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0833 - accuracy: 0.9971 - val_loss: 0.0847 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 264/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9973\n","Epoch 264: val_loss improved from 0.08406 to 0.08358, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0845 - accuracy: 0.9973 - val_loss: 0.0836 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 265/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9973\n","Epoch 265: val_loss improved from 0.08358 to 0.08211, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0837 - accuracy: 0.9973 - val_loss: 0.0821 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 266/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9972\n","Epoch 266: val_loss did not improve from 0.08211\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0835 - accuracy: 0.9972 - val_loss: 0.0825 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 267/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9973\n","Epoch 267: val_loss did not improve from 0.08211\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0833 - accuracy: 0.9973 - val_loss: 0.0835 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 268/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9965\n","Epoch 268: val_loss improved from 0.08211 to 0.08193, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0890 - accuracy: 0.9965 - val_loss: 0.0819 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 269/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9923\n","Epoch 269: val_loss did not improve from 0.08193\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0929 - accuracy: 0.9923 - val_loss: 0.1624 - val_accuracy: 0.9536 - lr: 1.0000e-04\n","Epoch 270/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9889\n","Epoch 270: val_loss did not improve from 0.08193\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0962 - accuracy: 0.9889 - val_loss: 0.0999 - val_accuracy: 0.9800 - lr: 1.0000e-04\n","Epoch 271/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9916\n","Epoch 271: val_loss improved from 0.08193 to 0.08044, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0917 - accuracy: 0.9916 - val_loss: 0.0804 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 272/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9949\n","Epoch 272: val_loss did not improve from 0.08044\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0879 - accuracy: 0.9949 - val_loss: 0.0816 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 273/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9957\n","Epoch 273: val_loss did not improve from 0.08044\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0842 - accuracy: 0.9957 - val_loss: 0.0840 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 274/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9959\n","Epoch 274: val_loss did not improve from 0.08044\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0866 - accuracy: 0.9959 - val_loss: 0.0825 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 275/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9966\n","Epoch 275: val_loss did not improve from 0.08044\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0821 - accuracy: 0.9966 - val_loss: 0.0823 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 276/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9967\n","Epoch 276: val_loss did not improve from 0.08044\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0826 - accuracy: 0.9967 - val_loss: 0.0815 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 277/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9967\n","Epoch 277: val_loss did not improve from 0.08044\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0807 - accuracy: 0.9967 - val_loss: 0.0805 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 278/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9969\n","Epoch 278: val_loss improved from 0.08044 to 0.07856, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0799 - accuracy: 0.9969 - val_loss: 0.0786 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 279/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9970\n","Epoch 279: val_loss improved from 0.07856 to 0.07852, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 17s 309ms/step - loss: 0.0795 - accuracy: 0.9970 - val_loss: 0.0785 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 280/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9971\n","Epoch 280: val_loss improved from 0.07852 to 0.07738, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0797 - accuracy: 0.9971 - val_loss: 0.0774 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 281/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9971\n","Epoch 281: val_loss improved from 0.07738 to 0.07667, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0797 - accuracy: 0.9971 - val_loss: 0.0767 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 282/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9972\n","Epoch 282: val_loss improved from 0.07667 to 0.07655, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0764 - accuracy: 0.9972 - val_loss: 0.0765 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 283/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9972\n","Epoch 283: val_loss improved from 0.07655 to 0.07554, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0764 - accuracy: 0.9972 - val_loss: 0.0755 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 284/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9973\n","Epoch 284: val_loss did not improve from 0.07554\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0771 - accuracy: 0.9973 - val_loss: 0.0755 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 285/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9971\n","Epoch 285: val_loss improved from 0.07554 to 0.07422, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0816 - accuracy: 0.9971 - val_loss: 0.0742 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 286/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9973\n","Epoch 286: val_loss did not improve from 0.07422\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0756 - accuracy: 0.9973 - val_loss: 0.0750 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 287/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9973\n","Epoch 287: val_loss did not improve from 0.07422\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0760 - accuracy: 0.9973 - val_loss: 0.0743 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 288/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9974\n","Epoch 288: val_loss improved from 0.07422 to 0.07306, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0771 - accuracy: 0.9974 - val_loss: 0.0731 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 289/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9973\n","Epoch 289: val_loss improved from 0.07306 to 0.07299, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0790 - accuracy: 0.9973 - val_loss: 0.0730 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 290/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9973\n","Epoch 290: val_loss improved from 0.07299 to 0.07277, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0764 - accuracy: 0.9973 - val_loss: 0.0728 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 291/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9973\n","Epoch 291: val_loss did not improve from 0.07277\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0762 - accuracy: 0.9973 - val_loss: 0.0728 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 292/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9975\n","Epoch 292: val_loss improved from 0.07277 to 0.07263, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0746 - accuracy: 0.9975 - val_loss: 0.0726 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 293/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9974\n","Epoch 293: val_loss improved from 0.07263 to 0.07143, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0758 - accuracy: 0.9974 - val_loss: 0.0714 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 294/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9975\n","Epoch 294: val_loss did not improve from 0.07143\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0711 - accuracy: 0.9975 - val_loss: 0.0719 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 295/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9975\n","Epoch 295: val_loss improved from 0.07143 to 0.07110, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0712 - accuracy: 0.9975 - val_loss: 0.0711 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 296/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9975\n","Epoch 296: val_loss improved from 0.07110 to 0.07024, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0764 - accuracy: 0.9975 - val_loss: 0.0702 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 297/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9975\n","Epoch 297: val_loss improved from 0.07024 to 0.07019, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0732 - accuracy: 0.9975 - val_loss: 0.0702 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 298/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9976\n","Epoch 298: val_loss did not improve from 0.07019\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0698 - accuracy: 0.9976 - val_loss: 0.0704 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 299/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9976\n","Epoch 299: val_loss improved from 0.07019 to 0.06970, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0708 - accuracy: 0.9976 - val_loss: 0.0697 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 300/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9976\n","Epoch 300: val_loss improved from 0.06970 to 0.06950, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0709 - accuracy: 0.9976 - val_loss: 0.0695 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 301/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9976\n","Epoch 301: val_loss improved from 0.06950 to 0.06892, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0685 - accuracy: 0.9976 - val_loss: 0.0689 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 302/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9975\n","Epoch 302: val_loss improved from 0.06892 to 0.06860, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0692 - accuracy: 0.9975 - val_loss: 0.0686 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 303/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9977\n","Epoch 303: val_loss improved from 0.06860 to 0.06820, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0704 - accuracy: 0.9977 - val_loss: 0.0682 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 304/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9976\n","Epoch 304: val_loss improved from 0.06820 to 0.06753, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0705 - accuracy: 0.9976 - val_loss: 0.0675 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 305/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9976\n","Epoch 305: val_loss did not improve from 0.06753\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0694 - accuracy: 0.9976 - val_loss: 0.0681 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 306/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9977\n","Epoch 306: val_loss improved from 0.06753 to 0.06728, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0691 - accuracy: 0.9977 - val_loss: 0.0673 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 307/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9976\n","Epoch 307: val_loss did not improve from 0.06728\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0689 - accuracy: 0.9976 - val_loss: 0.0680 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 308/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9977\n","Epoch 308: val_loss improved from 0.06728 to 0.06623, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0684 - accuracy: 0.9977 - val_loss: 0.0662 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 309/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9976\n","Epoch 309: val_loss did not improve from 0.06623\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0687 - accuracy: 0.9976 - val_loss: 0.0667 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 310/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9975\n","Epoch 310: val_loss did not improve from 0.06623\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0693 - accuracy: 0.9975 - val_loss: 0.0666 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 311/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9977\n","Epoch 311: val_loss improved from 0.06623 to 0.06553, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0690 - accuracy: 0.9977 - val_loss: 0.0655 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 312/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9977\n","Epoch 312: val_loss improved from 0.06553 to 0.06400, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0679 - accuracy: 0.9977 - val_loss: 0.0640 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 313/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9976\n","Epoch 313: val_loss did not improve from 0.06400\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0657 - accuracy: 0.9976 - val_loss: 0.0653 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 314/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9978\n","Epoch 314: val_loss did not improve from 0.06400\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0659 - accuracy: 0.9978 - val_loss: 0.0646 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 315/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.9978\n","Epoch 315: val_loss did not improve from 0.06400\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0641 - accuracy: 0.9978 - val_loss: 0.0640 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 316/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9978\n","Epoch 316: val_loss did not improve from 0.06400\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0683 - accuracy: 0.9978 - val_loss: 0.0640 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 317/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9977\n","Epoch 317: val_loss improved from 0.06400 to 0.06368, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0651 - accuracy: 0.9977 - val_loss: 0.0637 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 318/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9978\n","Epoch 318: val_loss did not improve from 0.06368\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0657 - accuracy: 0.9978 - val_loss: 0.0640 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 319/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9978\n","Epoch 319: val_loss did not improve from 0.06368\n","56/56 [==============================] - 17s 300ms/step - loss: 0.0655 - accuracy: 0.9978 - val_loss: 0.0637 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 320/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9979\n","Epoch 320: val_loss improved from 0.06368 to 0.06296, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0617 - accuracy: 0.9979 - val_loss: 0.0630 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 321/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9979\n","Epoch 321: val_loss improved from 0.06296 to 0.06271, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0626 - accuracy: 0.9979 - val_loss: 0.0627 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 322/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9978\n","Epoch 322: val_loss improved from 0.06271 to 0.06183, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0626 - accuracy: 0.9978 - val_loss: 0.0618 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 323/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9979\n","Epoch 323: val_loss improved from 0.06183 to 0.06175, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0645 - accuracy: 0.9979 - val_loss: 0.0618 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 324/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9979\n","Epoch 324: val_loss improved from 0.06175 to 0.06143, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0626 - accuracy: 0.9979 - val_loss: 0.0614 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 325/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9978\n","Epoch 325: val_loss improved from 0.06143 to 0.06083, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0632 - accuracy: 0.9978 - val_loss: 0.0608 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 326/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.9978\n","Epoch 326: val_loss did not improve from 0.06083\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0628 - accuracy: 0.9978 - val_loss: 0.0609 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 327/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9978\n","Epoch 327: val_loss improved from 0.06083 to 0.06008, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0631 - accuracy: 0.9978 - val_loss: 0.0601 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 328/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9979\n","Epoch 328: val_loss did not improve from 0.06008\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0624 - accuracy: 0.9979 - val_loss: 0.0606 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 329/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9979\n","Epoch 329: val_loss did not improve from 0.06008\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0623 - accuracy: 0.9979 - val_loss: 0.0610 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 330/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9980\n","Epoch 330: val_loss improved from 0.06008 to 0.05955, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0599 - accuracy: 0.9980 - val_loss: 0.0595 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 331/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9980\n","Epoch 331: val_loss did not improve from 0.05955\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0604 - accuracy: 0.9980 - val_loss: 0.0602 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 332/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9980\n","Epoch 332: val_loss did not improve from 0.05955\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0594 - accuracy: 0.9980 - val_loss: 0.0598 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 333/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9980\n","Epoch 333: val_loss improved from 0.05955 to 0.05942, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0593 - accuracy: 0.9980 - val_loss: 0.0594 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 334/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9980\n","Epoch 334: val_loss improved from 0.05942 to 0.05889, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0608 - accuracy: 0.9980 - val_loss: 0.0589 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 335/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9980\n","Epoch 335: val_loss did not improve from 0.05889\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0594 - accuracy: 0.9980 - val_loss: 0.0598 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 336/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9980\n","Epoch 336: val_loss improved from 0.05889 to 0.05806, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0595 - accuracy: 0.9980 - val_loss: 0.0581 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 337/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9980\n","Epoch 337: val_loss did not improve from 0.05806\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0597 - accuracy: 0.9980 - val_loss: 0.0581 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 338/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9978\n","Epoch 338: val_loss improved from 0.05806 to 0.05777, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0595 - accuracy: 0.9978 - val_loss: 0.0578 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 339/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9980\n","Epoch 339: val_loss did not improve from 0.05777\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0570 - accuracy: 0.9980 - val_loss: 0.0578 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 340/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9979\n","Epoch 340: val_loss improved from 0.05777 to 0.05647, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0604 - accuracy: 0.9979 - val_loss: 0.0565 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 341/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9981\n","Epoch 341: val_loss did not improve from 0.05647\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0562 - accuracy: 0.9981 - val_loss: 0.0584 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 342/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9982\n","Epoch 342: val_loss did not improve from 0.05647\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0570 - accuracy: 0.9982 - val_loss: 0.0571 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 343/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9982\n","Epoch 343: val_loss did not improve from 0.05647\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0577 - accuracy: 0.9982 - val_loss: 0.0565 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 344/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9981\n","Epoch 344: val_loss improved from 0.05647 to 0.05587, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0576 - accuracy: 0.9981 - val_loss: 0.0559 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 345/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9981\n","Epoch 345: val_loss did not improve from 0.05587\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0575 - accuracy: 0.9981 - val_loss: 0.0562 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 346/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9982\n","Epoch 346: val_loss did not improve from 0.05587\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0555 - accuracy: 0.9982 - val_loss: 0.0566 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 347/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9981\n","Epoch 347: val_loss improved from 0.05587 to 0.05524, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0581 - accuracy: 0.9981 - val_loss: 0.0552 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 348/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9980\n","Epoch 348: val_loss improved from 0.05524 to 0.05457, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0572 - accuracy: 0.9980 - val_loss: 0.0546 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 349/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9982\n","Epoch 349: val_loss did not improve from 0.05457\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0548 - accuracy: 0.9982 - val_loss: 0.0558 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 350/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9982\n","Epoch 350: val_loss did not improve from 0.05457\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0563 - accuracy: 0.9982 - val_loss: 0.0552 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 351/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9983\n","Epoch 351: val_loss did not improve from 0.05457\n","56/56 [==============================] - 17s 299ms/step - loss: 0.0547 - accuracy: 0.9983 - val_loss: 0.0549 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 352/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9983\n","Epoch 352: val_loss improved from 0.05457 to 0.05353, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0544 - accuracy: 0.9983 - val_loss: 0.0535 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 353/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9983\n","Epoch 353: val_loss did not improve from 0.05353\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0563 - accuracy: 0.9983 - val_loss: 0.0541 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 354/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9982\n","Epoch 354: val_loss improved from 0.05353 to 0.05228, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0538 - accuracy: 0.9982 - val_loss: 0.0523 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 355/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9983\n","Epoch 355: val_loss did not improve from 0.05228\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0539 - accuracy: 0.9983 - val_loss: 0.0524 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 356/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9983\n","Epoch 356: val_loss improved from 0.05228 to 0.05215, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0531 - accuracy: 0.9983 - val_loss: 0.0522 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 357/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9983\n","Epoch 357: val_loss did not improve from 0.05215\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0532 - accuracy: 0.9983 - val_loss: 0.0543 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 358/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9980\n","Epoch 358: val_loss did not improve from 0.05215\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0544 - accuracy: 0.9980 - val_loss: 0.0533 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 359/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9983\n","Epoch 359: val_loss did not improve from 0.05215\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0528 - accuracy: 0.9983 - val_loss: 0.0530 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 360/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9984\n","Epoch 360: val_loss did not improve from 0.05215\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0519 - accuracy: 0.9984 - val_loss: 0.0524 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 361/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9984\n","Epoch 361: val_loss improved from 0.05215 to 0.05134, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 17s 307ms/step - loss: 0.0523 - accuracy: 0.9984 - val_loss: 0.0513 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 362/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.9984\n","Epoch 362: val_loss did not improve from 0.05134\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0540 - accuracy: 0.9984 - val_loss: 0.0515 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 363/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9984\n","Epoch 363: val_loss improved from 0.05134 to 0.05128, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 17s 309ms/step - loss: 0.0503 - accuracy: 0.9984 - val_loss: 0.0513 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 364/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9920\n","Epoch 364: val_loss did not improve from 0.05128\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0670 - accuracy: 0.9920 - val_loss: 0.1163 - val_accuracy: 0.9621 - lr: 1.0000e-04\n","Epoch 365/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9871\n","Epoch 365: val_loss did not improve from 0.05128\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0753 - accuracy: 0.9871 - val_loss: 0.0655 - val_accuracy: 0.9843 - lr: 1.0000e-04\n","Epoch 366/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9933\n","Epoch 366: val_loss did not improve from 0.05128\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0629 - accuracy: 0.9933 - val_loss: 0.0646 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 367/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9957\n","Epoch 367: val_loss did not improve from 0.05128\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0555 - accuracy: 0.9957 - val_loss: 0.0599 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 368/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9966\n","Epoch 368: val_loss did not improve from 0.05128\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0534 - accuracy: 0.9966 - val_loss: 0.0591 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 369/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9971\n","Epoch 369: val_loss did not improve from 0.05128\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0541 - accuracy: 0.9971 - val_loss: 0.0571 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 370/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9974\n","Epoch 370: val_loss did not improve from 0.05128\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0519 - accuracy: 0.9974 - val_loss: 0.0558 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 371/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9976\n","Epoch 371: val_loss did not improve from 0.05128\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0512 - accuracy: 0.9976 - val_loss: 0.0541 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 372/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9977\n","Epoch 372: val_loss did not improve from 0.05128\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0497 - accuracy: 0.9977 - val_loss: 0.0527 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 373/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9978\n","Epoch 373: val_loss improved from 0.05128 to 0.05127, saving model to daunet_bce.hdf5\n","\n","Epoch 373: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0505 - accuracy: 0.9978 - val_loss: 0.0513 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 374/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9980\n","Epoch 374: val_loss improved from 0.05127 to 0.05049, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0495 - accuracy: 0.9980 - val_loss: 0.0505 - val_accuracy: 0.9960 - lr: 1.0000e-05\n","Epoch 375/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9980\n","Epoch 375: val_loss improved from 0.05049 to 0.05011, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 17s 308ms/step - loss: 0.0495 - accuracy: 0.9980 - val_loss: 0.0501 - val_accuracy: 0.9961 - lr: 1.0000e-05\n","Epoch 376/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9978\n","Epoch 376: val_loss did not improve from 0.05011\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0532 - accuracy: 0.9978 - val_loss: 0.0501 - val_accuracy: 0.9961 - lr: 1.0000e-05\n","Epoch 377/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9980\n","Epoch 377: val_loss improved from 0.05011 to 0.04995, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0510 - accuracy: 0.9980 - val_loss: 0.0499 - val_accuracy: 0.9961 - lr: 1.0000e-05\n","Epoch 378/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9980\n","Epoch 378: val_loss improved from 0.04995 to 0.04980, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0488 - accuracy: 0.9980 - val_loss: 0.0498 - val_accuracy: 0.9961 - lr: 1.0000e-05\n","Epoch 379/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9980\n","Epoch 379: val_loss did not improve from 0.04980\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0514 - accuracy: 0.9980 - val_loss: 0.0498 - val_accuracy: 0.9961 - lr: 1.0000e-05\n","Epoch 380/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9981\n","Epoch 380: val_loss improved from 0.04980 to 0.04970, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0506 - accuracy: 0.9981 - val_loss: 0.0497 - val_accuracy: 0.9961 - lr: 1.0000e-05\n","Epoch 381/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.9981\n","Epoch 381: val_loss improved from 0.04970 to 0.04964, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0499 - accuracy: 0.9981 - val_loss: 0.0496 - val_accuracy: 0.9961 - lr: 1.0000e-05\n","Epoch 382/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9981\n","Epoch 382: val_loss improved from 0.04964 to 0.04964, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0502 - accuracy: 0.9981 - val_loss: 0.0496 - val_accuracy: 0.9961 - lr: 1.0000e-05\n","Epoch 383/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9981\n","Epoch 383: val_loss improved from 0.04964 to 0.04957, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0511 - accuracy: 0.9981 - val_loss: 0.0496 - val_accuracy: 0.9961 - lr: 1.0000e-05\n","Epoch 384/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9981\n","Epoch 384: val_loss improved from 0.04957 to 0.04947, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0488 - accuracy: 0.9981 - val_loss: 0.0495 - val_accuracy: 0.9962 - lr: 1.0000e-05\n","Epoch 385/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9980\n","Epoch 385: val_loss did not improve from 0.04947\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0528 - accuracy: 0.9980 - val_loss: 0.0495 - val_accuracy: 0.9961 - lr: 1.0000e-05\n","Epoch 386/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9981\n","Epoch 386: val_loss did not improve from 0.04947\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0520 - accuracy: 0.9981 - val_loss: 0.0496 - val_accuracy: 0.9962 - lr: 1.0000e-05\n","Epoch 387/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9980\n","Epoch 387: val_loss did not improve from 0.04947\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0535 - accuracy: 0.9980 - val_loss: 0.0497 - val_accuracy: 0.9961 - lr: 1.0000e-05\n","Epoch 388/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9980\n","Epoch 388: val_loss did not improve from 0.04947\n","56/56 [==============================] - 15s 273ms/step - loss: 0.0518 - accuracy: 0.9980 - val_loss: 0.0496 - val_accuracy: 0.9962 - lr: 1.0000e-05\n","Epoch 389/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9982\n","Epoch 389: val_loss improved from 0.04947 to 0.04945, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0505 - accuracy: 0.9982 - val_loss: 0.0494 - val_accuracy: 0.9962 - lr: 1.0000e-05\n","Epoch 390/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9981\n","Epoch 390: val_loss did not improve from 0.04945\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0509 - accuracy: 0.9981 - val_loss: 0.0495 - val_accuracy: 0.9962 - lr: 1.0000e-05\n","Epoch 391/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9982\n","Epoch 391: val_loss improved from 0.04945 to 0.04936, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0475 - accuracy: 0.9982 - val_loss: 0.0494 - val_accuracy: 0.9962 - lr: 1.0000e-05\n","Epoch 392/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9981\n","Epoch 392: val_loss improved from 0.04936 to 0.04931, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0513 - accuracy: 0.9981 - val_loss: 0.0493 - val_accuracy: 0.9962 - lr: 1.0000e-05\n","Epoch 393/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9982\n","Epoch 393: val_loss improved from 0.04931 to 0.04930, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0496 - accuracy: 0.9982 - val_loss: 0.0493 - val_accuracy: 0.9962 - lr: 1.0000e-05\n","Epoch 394/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9982\n","Epoch 394: val_loss improved from 0.04930 to 0.04925, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0493 - accuracy: 0.9982 - val_loss: 0.0492 - val_accuracy: 0.9962 - lr: 1.0000e-05\n","Epoch 395/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9982\n","Epoch 395: val_loss improved from 0.04925 to 0.04898, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0474 - accuracy: 0.9982 - val_loss: 0.0490 - val_accuracy: 0.9962 - lr: 1.0000e-05\n","Epoch 396/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9982\n","Epoch 396: val_loss improved from 0.04898 to 0.04894, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0495 - accuracy: 0.9982 - val_loss: 0.0489 - val_accuracy: 0.9962 - lr: 1.0000e-05\n","Epoch 397/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9982\n","Epoch 397: val_loss improved from 0.04894 to 0.04885, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0490 - accuracy: 0.9982 - val_loss: 0.0489 - val_accuracy: 0.9962 - lr: 1.0000e-05\n","Epoch 398/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9983\n","Epoch 398: val_loss improved from 0.04885 to 0.04880, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0486 - accuracy: 0.9983 - val_loss: 0.0488 - val_accuracy: 0.9963 - lr: 1.0000e-05\n","Epoch 399/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9983\n","Epoch 399: val_loss improved from 0.04880 to 0.04878, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0477 - accuracy: 0.9983 - val_loss: 0.0488 - val_accuracy: 0.9963 - lr: 1.0000e-05\n","Epoch 400/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9982\n","Epoch 400: val_loss improved from 0.04878 to 0.04876, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0494 - accuracy: 0.9982 - val_loss: 0.0488 - val_accuracy: 0.9963 - lr: 1.0000e-05\n","Epoch 401/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9982\n","Epoch 401: val_loss did not improve from 0.04876\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0515 - accuracy: 0.9982 - val_loss: 0.0488 - val_accuracy: 0.9963 - lr: 1.0000e-05\n","Epoch 402/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9982\n","Epoch 402: val_loss improved from 0.04876 to 0.04869, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0504 - accuracy: 0.9982 - val_loss: 0.0487 - val_accuracy: 0.9963 - lr: 1.0000e-05\n","Epoch 403/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9983\n","Epoch 403: val_loss did not improve from 0.04869\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0510 - accuracy: 0.9983 - val_loss: 0.0488 - val_accuracy: 0.9963 - lr: 1.0000e-05\n","Epoch 404/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9983\n","Epoch 404: val_loss improved from 0.04869 to 0.04864, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0498 - accuracy: 0.9983 - val_loss: 0.0486 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 405/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9983\n","Epoch 405: val_loss improved from 0.04864 to 0.04849, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0478 - accuracy: 0.9983 - val_loss: 0.0485 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 406/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9983\n","Epoch 406: val_loss did not improve from 0.04849\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0502 - accuracy: 0.9983 - val_loss: 0.0487 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 407/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9983\n","Epoch 407: val_loss did not improve from 0.04849\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0484 - accuracy: 0.9983 - val_loss: 0.0487 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 408/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9983\n","Epoch 408: val_loss did not improve from 0.04849\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0516 - accuracy: 0.9983 - val_loss: 0.0487 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 409/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9983\n","Epoch 409: val_loss did not improve from 0.04849\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0489 - accuracy: 0.9983 - val_loss: 0.0485 - val_accuracy: 0.9963 - lr: 1.0000e-05\n","Epoch 410/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9983\n","Epoch 410: val_loss did not improve from 0.04849\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0496 - accuracy: 0.9983 - val_loss: 0.0488 - val_accuracy: 0.9963 - lr: 1.0000e-05\n","Epoch 411/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9983\n","Epoch 411: val_loss did not improve from 0.04849\n","56/56 [==============================] - 17s 298ms/step - loss: 0.0486 - accuracy: 0.9983 - val_loss: 0.0486 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 412/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9983\n","Epoch 412: val_loss improved from 0.04849 to 0.04849, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0507 - accuracy: 0.9983 - val_loss: 0.0485 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 413/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9984\n","Epoch 413: val_loss improved from 0.04849 to 0.04844, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 17s 308ms/step - loss: 0.0467 - accuracy: 0.9984 - val_loss: 0.0484 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 414/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9983\n","Epoch 414: val_loss did not improve from 0.04844\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0494 - accuracy: 0.9983 - val_loss: 0.0485 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 415/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9984\n","Epoch 415: val_loss did not improve from 0.04844\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0508 - accuracy: 0.9984 - val_loss: 0.0484 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 416/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9984\n","Epoch 416: val_loss improved from 0.04844 to 0.04837, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0495 - accuracy: 0.9984 - val_loss: 0.0484 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 417/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9984\n","Epoch 417: val_loss did not improve from 0.04837\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0508 - accuracy: 0.9984 - val_loss: 0.0485 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 418/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9984\n","Epoch 418: val_loss did not improve from 0.04837\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0485 - accuracy: 0.9984 - val_loss: 0.0485 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 419/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9984\n","Epoch 419: val_loss improved from 0.04837 to 0.04831, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 17s 308ms/step - loss: 0.0488 - accuracy: 0.9984 - val_loss: 0.0483 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 420/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9984\n","Epoch 420: val_loss did not improve from 0.04831\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0486 - accuracy: 0.9984 - val_loss: 0.0484 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 421/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9984\n","Epoch 421: val_loss improved from 0.04831 to 0.04823, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0474 - accuracy: 0.9984 - val_loss: 0.0482 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 422/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9984\n","Epoch 422: val_loss improved from 0.04823 to 0.04812, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0494 - accuracy: 0.9984 - val_loss: 0.0481 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 423/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9985\n","Epoch 423: val_loss improved from 0.04812 to 0.04804, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0482 - accuracy: 0.9985 - val_loss: 0.0480 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 424/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9984\n","Epoch 424: val_loss did not improve from 0.04804\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0485 - accuracy: 0.9984 - val_loss: 0.0481 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 425/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9984\n","Epoch 425: val_loss improved from 0.04804 to 0.04796, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0484 - accuracy: 0.9984 - val_loss: 0.0480 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 426/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9985\n","Epoch 426: val_loss did not improve from 0.04796\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0477 - accuracy: 0.9985 - val_loss: 0.0480 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 427/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9984\n","Epoch 427: val_loss improved from 0.04796 to 0.04749, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0504 - accuracy: 0.9984 - val_loss: 0.0475 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 428/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9985\n","Epoch 428: val_loss did not improve from 0.04749\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0468 - accuracy: 0.9985 - val_loss: 0.0477 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 429/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9985\n","Epoch 429: val_loss did not improve from 0.04749\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0482 - accuracy: 0.9985 - val_loss: 0.0479 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 430/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9984\n","Epoch 430: val_loss did not improve from 0.04749\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0475 - accuracy: 0.9984 - val_loss: 0.0478 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 431/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9985\n","Epoch 431: val_loss did not improve from 0.04749\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0472 - accuracy: 0.9985 - val_loss: 0.0476 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 432/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9985\n","Epoch 432: val_loss did not improve from 0.04749\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0486 - accuracy: 0.9985 - val_loss: 0.0478 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 433/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9985\n","Epoch 433: val_loss did not improve from 0.04749\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0486 - accuracy: 0.9985 - val_loss: 0.0476 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 434/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9985\n","Epoch 434: val_loss did not improve from 0.04749\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0472 - accuracy: 0.9985 - val_loss: 0.0475 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 435/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9985\n","Epoch 435: val_loss did not improve from 0.04749\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0475 - accuracy: 0.9985 - val_loss: 0.0476 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 436/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9985\n","Epoch 436: val_loss did not improve from 0.04749\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0479 - accuracy: 0.9985 - val_loss: 0.0475 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 437/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9985\n","Epoch 437: val_loss improved from 0.04749 to 0.04748, saving model to daunet_bce.hdf5\n","\n","Epoch 437: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0503 - accuracy: 0.9985 - val_loss: 0.0475 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 438/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9986\n","Epoch 438: val_loss improved from 0.04748 to 0.04740, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0466 - accuracy: 0.9986 - val_loss: 0.0474 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 439/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9985\n","Epoch 439: val_loss did not improve from 0.04740\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0500 - accuracy: 0.9985 - val_loss: 0.0476 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 440/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9986\n","Epoch 440: val_loss did not improve from 0.04740\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0482 - accuracy: 0.9986 - val_loss: 0.0476 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 441/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9986\n","Epoch 441: val_loss did not improve from 0.04740\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0473 - accuracy: 0.9986 - val_loss: 0.0475 - val_accuracy: 0.9965 - lr: 1.0000e-06\n","Epoch 442/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9986\n","Epoch 442: val_loss did not improve from 0.04740\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0476 - accuracy: 0.9986 - val_loss: 0.0475 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 443/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9985\n","Epoch 443: val_loss did not improve from 0.04740\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0474 - accuracy: 0.9985 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 444/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9985\n","Epoch 444: val_loss improved from 0.04740 to 0.04739, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0473 - accuracy: 0.9985 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 445/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9985\n","Epoch 445: val_loss improved from 0.04739 to 0.04738, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0477 - accuracy: 0.9985 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 446/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9985\n","Epoch 446: val_loss did not improve from 0.04738\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0483 - accuracy: 0.9985 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 447/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9985\n","Epoch 447: val_loss improved from 0.04738 to 0.04735, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.0468 - accuracy: 0.9985 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 448/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9986\n","Epoch 448: val_loss did not improve from 0.04735\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0483 - accuracy: 0.9986 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 449/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9986\n","Epoch 449: val_loss did not improve from 0.04735\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0474 - accuracy: 0.9986 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 450/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9986\n","Epoch 450: val_loss did not improve from 0.04735\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0474 - accuracy: 0.9986 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 451/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9985\n","Epoch 451: val_loss did not improve from 0.04735\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0477 - accuracy: 0.9985 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 452/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9986\n","Epoch 452: val_loss improved from 0.04735 to 0.04731, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0470 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 453/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9986\n","Epoch 453: val_loss did not improve from 0.04731\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0479 - accuracy: 0.9986 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 454/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9986\n","Epoch 454: val_loss did not improve from 0.04731\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0478 - accuracy: 0.9986 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 455/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9985\n","Epoch 455: val_loss did not improve from 0.04731\n","56/56 [==============================] - 17s 300ms/step - loss: 0.0475 - accuracy: 0.9985 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 456/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9986\n","Epoch 456: val_loss did not improve from 0.04731\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0474 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 457/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9986\n","Epoch 457: val_loss did not improve from 0.04731\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0472 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 458/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9986\n","Epoch 458: val_loss improved from 0.04731 to 0.04729, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0469 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 459/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9986\n","Epoch 459: val_loss improved from 0.04729 to 0.04720, saving model to daunet_bce.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0461 - accuracy: 0.9986 - val_loss: 0.0472 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 460/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9986\n","Epoch 460: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0468 - accuracy: 0.9986 - val_loss: 0.0472 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 461/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9986\n","Epoch 461: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0476 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 462/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9986\n","Epoch 462: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0465 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 463/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9985\n","Epoch 463: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 273ms/step - loss: 0.0490 - accuracy: 0.9985 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 464/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9986\n","Epoch 464: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0464 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 465/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9986\n","Epoch 465: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0475 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 466/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9986\n","Epoch 466: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0469 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 467/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9986\n","Epoch 467: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0485 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 468/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9986\n","Epoch 468: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0483 - accuracy: 0.9986 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 469/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9986\n","Epoch 469: val_loss did not improve from 0.04720\n","\n","Epoch 469: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0493 - accuracy: 0.9986 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-06\n","Epoch 470/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9986\n","Epoch 470: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 276ms/step - loss: 0.0477 - accuracy: 0.9986 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-07\n","Epoch 471/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9986\n","Epoch 471: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0464 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-07\n","Epoch 472/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9986\n","Epoch 472: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0468 - accuracy: 0.9986 - val_loss: 0.0472 - val_accuracy: 0.9966 - lr: 1.0000e-07\n","Epoch 473/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9985\n","Epoch 473: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0501 - accuracy: 0.9985 - val_loss: 0.0474 - val_accuracy: 0.9966 - lr: 1.0000e-07\n","Epoch 474/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9986\n","Epoch 474: val_loss did not improve from 0.04720\n","56/56 [==============================] - 15s 275ms/step - loss: 0.0472 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9966 - lr: 1.0000e-07\n","Epoch 474: early stopping\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x75cd01416f90>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Train model\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr=ReduceLROnPlateau(monitor='val_loss',\n","                         factor=0.1,\n","                         patience=10,\n","                         verbose=1,\n","                         mode='auto',\n","                         min_delta=0.00003,\n","                         cooldown=0,\n","                         min_lr=0)\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15,verbose=1,mode='min')\n","save_model= ModelCheckpoint('daunet_bce.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","unet2.fit(images, masks, validation_data=(val_images,val_masks), batch_size=16, epochs=1000,verbose=1,shuffle=True,callbacks=[save_model,reduce_lr,early_stop])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T12:40:53.595658Z","iopub.status.busy":"2023-04-05T12:40:53.594569Z","iopub.status.idle":"2023-04-05T12:40:53.603136Z","shell.execute_reply":"2023-04-05T12:40:53.602060Z","shell.execute_reply.started":"2023-04-05T12:40:53.595617Z"},"trusted":true},"outputs":[],"source":["np.save('daunet_bce-history.npy',unet2.history.history)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T12:40:59.511406Z","iopub.status.busy":"2023-04-05T12:40:59.510701Z","iopub.status.idle":"2023-04-05T12:40:59.518256Z","shell.execute_reply":"2023-04-05T12:40:59.516597Z","shell.execute_reply.started":"2023-04-05T12:40:59.511368Z"},"trusted":true},"outputs":[],"source":["model_history = np.load('daunet_bce-history.npy', allow_pickle='TRUE').item()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T12:41:01.953082Z","iopub.status.busy":"2023-04-05T12:41:01.952164Z","iopub.status.idle":"2023-04-05T12:41:02.475120Z","shell.execute_reply":"2023-04-05T12:41:02.474176Z","shell.execute_reply.started":"2023-04-05T12:41:01.953043Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<matplotlib.legend.Legend at 0x75ccf4173510>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#@title Plot accuracy and loss \n","from matplotlib import pyplot as plt\n","## Accuracy\n","plt.plot(model_history['accuracy'])\n","plt.plot(model_history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')\n","plt.show()\n","\n","## Loss\n","plt.plot(model_history['loss'])\n","plt.plot(model_history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T12:41:53.602611Z","iopub.status.busy":"2023-04-05T12:41:53.602150Z","iopub.status.idle":"2023-04-05T12:42:17.168081Z","shell.execute_reply":"2023-04-05T12:42:17.167059Z","shell.execute_reply.started":"2023-04-05T12:41:53.602571Z"},"trusted":true},"outputs":[],"source":["## Test images\n","test_dir=r'/content/gdrive/MyDrive/mudtest/'\n","test_images_list = os.listdir(r\"{}/images/\".format(test_dir))\n","test_masks_list = []\n","test_images = []\n","for n in test_images_list:\n","  test_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}/images/{}\".format(test_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  test_images.append(a)\n","\n","## Test masks\n","test_masks = []\n","for n in test_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}/labels/{}\".format(test_dir,n))))\n","  test_masks.append(a)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T12:42:22.358330Z","iopub.status.busy":"2023-04-05T12:42:22.357349Z","iopub.status.idle":"2023-04-05T12:42:22.509838Z","shell.execute_reply":"2023-04-05T12:42:22.508760Z","shell.execute_reply.started":"2023-04-05T12:42:22.358292Z"},"trusted":true},"outputs":[],"source":["for i in range(len(test_images)):\n","  test_images[i] = test_images[i].astype('float32')\n","  test_images[i] = test_images[i].T\n","\n","for i in range(len(test_masks)):\n","  test_masks[i] = test_masks[i].reshape(1,256,256,1)\n","  test_masks[i] = test_masks[i].T\n","for i in range(len(test_images)):\n","  test_images[i] = test_images[i].reshape(-1,256,256,10)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T12:42:25.200848Z","iopub.status.busy":"2023-04-05T12:42:25.200349Z","iopub.status.idle":"2023-04-05T12:42:25.212942Z","shell.execute_reply":"2023-04-05T12:42:25.211519Z","shell.execute_reply.started":"2023-04-05T12:42:25.200805Z"},"trusted":true},"outputs":[],"source":["#@title Returns an image or array plot of mask prediction\n","\n","def reconstruct_image(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","  # Standardise between 0-1\n","  reconstruction = reconstruction/np.max(reconstruction)\n","\n","  # Round to 0-1, binary pixel-by-pixel classification \n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  # Plot reconstructed mask (prediction)\n","  plt.imshow(reconstruction) \n","'''\n","  Returns array of mask prediction, given model and image\n","'''\n","def reconstruct_array(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","\n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  return reconstruction # Returns array"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T12:42:28.613260Z","iopub.status.busy":"2023-04-05T12:42:28.612734Z","iopub.status.idle":"2023-04-05T12:42:28.631281Z","shell.execute_reply":"2023-04-05T12:42:28.629973Z","shell.execute_reply.started":"2023-04-05T12:42:28.613224Z"},"trusted":true},"outputs":[],"source":["#@title Metric functions for evaluation\n","def accuracy_eval(model, image, mask): # Gives score of mask vs prediction\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","    return accuracy_score(mask.flatten(), reconstruction)\n","\n","  else: # If a list of images input, find accuracy for each\n","    accuracy = []\n","    for i in range(len(image)):\n","      reconstruction = model.predict(image[i].reshape(1, 256, 256, 10))\n","      reconstruction = np.round(reconstruction).flatten()\n","      accuracy.append(accuracy_score(mask[i].flatten(), reconstruction))\n","    return accuracy\n","\n","def recall_eval(model, image, mask): # Find recall score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return recall_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    recall = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        recall.append(recall_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return recall\n","\n","def precision_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return precision_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    precision = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        precision.append(precision_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return precision\n","\n","def iou_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return jaccard_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    iou = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        iou.append(jaccard_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return iou\n","\n","def f1_score_eval_basic(precision, recall):\n","    prec = np.mean(precision)\n","    rec = np.mean(recall)\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def produce_mask(image): # Outputs rounded image (binary)\n","  return np.round(image)\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T12:42:32.045736Z","iopub.status.busy":"2023-04-05T12:42:32.045006Z","iopub.status.idle":"2023-04-05T12:44:06.837595Z","shell.execute_reply":"2023-04-05T12:44:06.836549Z","shell.execute_reply.started":"2023-04-05T12:42:32.045677Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 52ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 52ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 49ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n"]}],"source":["accuracy = (accuracy_eval(unet2, test_images, test_masks))\n","precision = (precision_eval(unet2, test_images, test_masks))\n","recall = (recall_eval(unet2, test_images, test_masks))\n","iou = (iou_eval(unet2, test_images, test_masks))"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T12:44:19.402445Z","iopub.status.busy":"2023-04-05T12:44:19.401832Z","iopub.status.idle":"2023-04-05T12:44:19.407480Z","shell.execute_reply":"2023-04-05T12:44:19.406311Z","shell.execute_reply.started":"2023-04-05T12:44:19.402408Z"},"trusted":true},"outputs":[],"source":["f1_score = (f1_score_eval_basic(precision, recall))"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T12:44:22.011952Z","iopub.status.busy":"2023-04-05T12:44:22.011337Z","iopub.status.idle":"2023-04-05T12:44:22.019441Z","shell.execute_reply":"2023-04-05T12:44:22.018332Z","shell.execute_reply.started":"2023-04-05T12:44:22.011912Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model accuracy:  0.9865743880881402 0.011784140507306744\n","model precision:  0.9663777095164706 0.0451159087245963\n","model recall:  0.9766326827660597 0.03985395068361591\n","model F1-score:  0.9714781338885959\n","model iou:  0.9447177008406598\n"]}],"source":["\n","# Print score eval results for each model\n","print('model accuracy: ', np.mean(accuracy), np.std(accuracy))\n","# Print precision eval results for each model\n","print('model precision: ', np.mean(precision), np.std(precision))\n","# Print recall eval results for each model\n","print('model recall: ', np.mean(recall), np.std(recall))\n","# Print f1-score eval results for each model\n","print('model F1-score: ', np.mean(f1_score))\n","print('model iou: ', np.mean(iou))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
