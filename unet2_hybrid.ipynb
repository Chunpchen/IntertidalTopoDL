{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-03T15:02:45.667067Z","iopub.status.busy":"2023-04-03T15:02:45.666101Z","iopub.status.idle":"2023-04-03T15:03:24.305818Z","shell.execute_reply":"2023-04-03T15:03:24.304647Z","shell.execute_reply.started":"2023-04-03T15:02:45.667015Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/davej23/image-segmentation-keras.git\n","  Cloning https://github.com/davej23/image-segmentation-keras.git to /tmp/pip-req-build-myu4fthw\n","  Running command git clone --filter=blob:none --quiet https://github.com/davej23/image-segmentation-keras.git /tmp/pip-req-build-myu4fthw\n","  Resolved https://github.com/davej23/image-segmentation-keras.git to commit e01b0a8d5859854cd9d259a618829889166439f5\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting rarfile\n","  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n","Collecting segmentation-models\n","  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n","Collecting rioxarray\n","  Downloading rioxarray-0.9.1.tar.gz (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting keras-applications<=1.0.8,>=1.0.7\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting efficientnet==1.0.0\n","  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n","Collecting image-classifiers==1.0.0\n","  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n","Collecting h5py<=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: Keras>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (2.11.0)\n","Collecting imageio==2.5.0\n","  Downloading imageio-2.5.0-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (0.4.0)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.5.4.60)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.64.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (1.21.6)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (9.4.0)\n","Requirement already satisfied: pyproj>=2.2 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (3.1.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from rioxarray) (23.0)\n","Requirement already satisfied: rasterio in /opt/conda/lib/python3.7/site-packages (from rioxarray) (1.2.10)\n","Requirement already satisfied: xarray>=0.17 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (0.20.2)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py<=2.10.0->keras-segmentation==0.3.0) (1.16.0)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (3.5.3)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.7.3)\n","Requirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.8.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from pyproj>=2.2->rioxarray) (2022.12.7)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.11.4)\n","Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.4.0)\n","Requirement already satisfied: pandas>=1.1 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (1.3.5)\n","Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (8.1.3)\n","Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (0.7.2)\n","Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (22.2.0)\n","Requirement already satisfied: click-plugins in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.1.1)\n","Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.4.7)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (59.8.0)\n","Requirement already satisfied: affine in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (2.4.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2023.2)\n","Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.11.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.0)\n","Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n","Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio->rioxarray) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->xarray>=0.17->rioxarray) (3.11.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (1.4.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (4.38.0)\n","Building wheels for collected packages: keras-segmentation, rioxarray\n","  Building wheel for keras-segmentation (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for keras-segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34377 sha256=b8184fdeb6222b14b66461458d7835fdf1a2fa12b68ea5b6ab08bf46c8ccbc8d\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3w0i8q0e/wheels/f4/fb/07/8f81ceb3d9fe936f5e4dcd1a64cbc489e42e6e7f9c2f166785\n","  Building wheel for rioxarray (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rioxarray: filename=rioxarray-0.9.1-py3-none-any.whl size=54590 sha256=cf504bfa7ef2184b946710e46fd66bea2c9706b9e007157bf9ecbb4fe4a63f44\n","  Stored in directory: /root/.cache/pip/wheels/03/b2/26/2e2cc1797ac99cc070d2cae87c340bd3429bbb583c90b1c780\n","Successfully built keras-segmentation rioxarray\n","Installing collected packages: rarfile, imageio, h5py, keras-applications, image-classifiers, efficientnet, segmentation-models, keras-segmentation, rioxarray\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.25.0\n","    Uninstalling imageio-2.25.0:\n","      Successfully uninstalled imageio-2.25.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n","tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed efficientnet-1.0.0 h5py-2.10.0 image-classifiers-1.0.0 imageio-2.5.0 keras-applications-1.0.8 keras-segmentation-0.3.0 rarfile-4.0 rioxarray-0.9.1 segmentation-models-1.0.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#@title import packages\n","import keras\n","import numpy as np\n","import os\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.losses import *\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint\n","import sys\n","\n","!pip install rarfile segmentation-models git+https://github.com/davej23/image-segmentation-keras.git rioxarray\n","from rarfile import RarFile\n","from sklearn.metrics import *\n","import rioxarray as rxr"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:03:24.308385Z","iopub.status.busy":"2023-04-03T15:03:24.308018Z","iopub.status.idle":"2023-04-03T15:05:06.750301Z","shell.execute_reply":"2023-04-03T15:05:06.749225Z","shell.execute_reply.started":"2023-04-03T15:03:24.308347Z"},"trusted":true},"outputs":[],"source":["base_dir = r\"/content/gdrive/MyDrive/mudtrain/\"\n","#@title Read training images and normalise\n","training_images_list = os.listdir(r\"{}train/images/\".format(base_dir))\n","training_masks_list = []\n","training_images = []\n","for n in training_images_list:\n","  training_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}train/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  training_images.append(a)\n","\n","## Training masks\n","training_masks = []\n","for n in training_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}train/labels/{}\".format(base_dir,n))))\n","  training_masks.append(a)\n","\n","\n","## Validation images\n","validation_images_list = os.listdir(r\"{}val/images/\".format(base_dir))\n","validation_masks_list = []\n","validation_images = []\n","for n in validation_images_list:\n","  validation_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}val/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  validation_images.append(a)\n","\n","## Validation masks\n","validation_masks = []\n","for n in validation_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}val/labels/{}\".format(base_dir,n))))\n","  validation_masks.append(a)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:05:16.161085Z","iopub.status.busy":"2023-04-03T15:05:16.160578Z","iopub.status.idle":"2023-04-03T15:05:16.765036Z","shell.execute_reply":"2023-04-03T15:05:16.763926Z","shell.execute_reply.started":"2023-04-03T15:05:16.161047Z"},"trusted":true},"outputs":[],"source":["#@title Pre-process data, reshaping and transposing\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].astype('float32')\n","  training_images[i] = training_images[i].T\n","\n","for i in range(len(training_masks)):\n","  training_masks[i] = training_masks[i].reshape(1,256,256)\n","  training_masks[i] = training_masks[i].T\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].astype('float32')\n","  validation_images[i] = validation_images[i].T\n","\n","for i in range(len(validation_masks)):\n","  validation_masks[i] = validation_masks[i].reshape(1,256,256)\n","  validation_masks[i] = validation_masks[i].T\n","\n","\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].reshape(256,256,10)\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].reshape(256,256,10)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:05:19.976515Z","iopub.status.busy":"2023-04-03T15:05:19.975802Z","iopub.status.idle":"2023-04-03T15:05:24.841148Z","shell.execute_reply":"2023-04-03T15:05:24.839909Z","shell.execute_reply.started":"2023-04-03T15:05:19.976476Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 10)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["images=np.vstack([training_images])\n","val_images=np.vstack([validation_images])\n","images.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:05:27.470223Z","iopub.status.busy":"2023-04-03T15:05:27.469531Z","iopub.status.idle":"2023-04-03T15:05:27.630006Z","shell.execute_reply":"2023-04-03T15:05:27.628906Z","shell.execute_reply.started":"2023-04-03T15:05:27.470182Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 1)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["masks=np.vstack([training_masks])\n","val_masks=np.vstack([validation_masks])\n","val_masks.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:05:29.872296Z","iopub.status.busy":"2023-04-03T15:05:29.871566Z","iopub.status.idle":"2023-04-03T15:05:30.103371Z","shell.execute_reply":"2023-04-03T15:05:30.102412Z","shell.execute_reply.started":"2023-04-03T15:05:29.872256Z"},"trusted":true},"outputs":[{"data":{"text/plain":["904"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del training_images,validation_images,training_masks,validation_masks,training_images_list,validation_images_list,\n","training_masks_list,validation_masks_list\n","gc.collect()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:06:24.109650Z","iopub.status.busy":"2023-04-03T18:06:24.108850Z","iopub.status.idle":"2023-04-03T18:06:24.124512Z","shell.execute_reply":"2023-04-03T18:06:24.123236Z","shell.execute_reply.started":"2023-04-03T18:06:24.109607Z"},"trusted":true},"outputs":[],"source":["del images,masks,val_images,val_masks"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:05:33.160617Z","iopub.status.busy":"2023-04-03T15:05:33.159909Z","iopub.status.idle":"2023-04-03T15:05:33.172078Z","shell.execute_reply":"2023-04-03T15:05:33.170984Z","shell.execute_reply.started":"2023-04-03T15:05:33.160577Z"},"trusted":true},"outputs":[],"source":["#@title boundary loss\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import models\n","\n","#Shape of semantic segmentation mask\n","OUTPUT_SHAPE = (256, 256, 1)\n","def boundary_loss(y_true, y_pred):\n","\n","    \"\"\"\n","    Paper Implemented : https://arxiv.org/abs/1905.07852\n","    Using Binary Segmentation mask, generates boundary mask on fly and claculates boundary loss.\n","    :param y_true:\n","    :param y_pred:\n","    :return:\n","    \"\"\"\n","    y_true=tf.cast(y_true,tf.float32)\n","    y_pred=tf.cast(y_pred,tf.float32)\n","    \n","    y_pred_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd = y_pred_bd - (1 - y_pred)\n","    y_true_bd = y_true_bd - (1 - y_true)\n","\n","    y_pred_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd_ext = y_pred_bd_ext - (1 - y_pred)\n","    y_true_bd_ext = y_true_bd_ext - (1 - y_true)\n","\n","    P = K.sum(y_pred_bd * y_true_bd_ext) / K.sum(y_pred_bd) + 1e-7\n","    R = K.sum(y_true_bd * y_pred_bd_ext) / K.sum(y_true_bd) + 1e-7\n","    F1_Score = 2 * P * R / (P + R + 1e-7)\n","    loss = K.mean(1 - F1_Score)\n","    \n","    return loss"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:05:36.594846Z","iopub.status.busy":"2023-04-03T15:05:36.594426Z","iopub.status.idle":"2023-04-03T15:05:39.005124Z","shell.execute_reply":"2023-04-03T15:05:39.004051Z","shell.execute_reply.started":"2023-04-03T15:05:36.594812Z"},"trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, Callback\n","class AlphaScheduler(Callback):\n","  def init(self, alpha, update_fn):\n","    self.alpha = alpha\n","    self.update_fn = update_fn\n","  def on_epoch_end(self, epoch, logs=None):\n","    updated_alpha = self.update_fn(K.get_value(self.alpha))\n","\n","alpha = K.variable(1, dtype='float32')\n","\n","def update_alpha(value):\n","  return np.clip(value - 0.005, 0.005, 1)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:05:40.503351Z","iopub.status.busy":"2023-04-03T15:05:40.502651Z","iopub.status.idle":"2023-04-03T15:05:40.509616Z","shell.execute_reply":"2023-04-03T15:05:40.508337Z","shell.execute_reply.started":"2023-04-03T15:05:40.503312Z"},"trusted":true},"outputs":[],"source":["def gl_sl_wrapper(alpha):\n","    def gl_sl(y_true, y_pred):\n","        return alpha*keras.losses.binary_crossentropy(y_true, y_pred) +  (1-alpha)* boundary_loss(y_true, y_pred)\n","    return gl_sl"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:05:43.901302Z","iopub.status.busy":"2023-04-03T15:05:43.900872Z","iopub.status.idle":"2023-04-03T15:05:44.224185Z","shell.execute_reply":"2023-04-03T15:05:44.223143Z","shell.execute_reply.started":"2023-04-03T15:05:43.901268Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import models, layers, regularizers\n","from tensorflow.keras import backend as K\n","from keras.layers import concatenate, Conv2DTranspose, Activation\n","from keras.layers import BatchNormalization\n","from keras.layers import Conv2D, Input, AvgPool2D\n","\n","#convolutional block\n","def conv_block(x, kernelsize, filters, dropout, batchnorm=True): \n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    if dropout > 0:\n","        conv = layers.Dropout(dropout)(conv)\n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(conv)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    return conv\n","\"\"\"\n","Standard UNet++ [Zhou et.al, 2018]\n","Total params: 9,041,601\n","\"\"\"\n","def unet2(input_shape,dropout=0, batchnorm=True):\n","    \n","    filters = [32,64, 128, 256,512]\n","    kernelsize = 3\n","    inputs = Input(input_shape, name='main_input')\n","    \n","    conv1_1 = conv_block(inputs, kernelsize, filters[0],dropout,batchnorm)\n","    pool1 = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)\n","\n","    conv2_1=conv_block(pool1, kernelsize, filters[1],dropout,batchnorm)\n","    pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n","\n","    up1_2 = Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)\n","    conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=3)\n","    conv1_2=conv_block(conv1_2, kernelsize, filters[0],dropout,batchnorm)\n","\n","\n","    conv3_1=conv_block(pool2, kernelsize, filters[2],dropout,batchnorm)\n","    pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n","\n","    up2_2 = Conv2DTranspose(filters[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)\n","    conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=3)\n","    conv2_2=conv_block(conv2_2, kernelsize, filters[1],dropout,batchnorm)\n","\n","    up1_3 = Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)\n","    conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=3)\n","    conv1_3=conv_block(conv1_3, kernelsize, filters[0],dropout,batchnorm)\n","\n","    conv4_1=conv_block(pool3, kernelsize, filters[3],dropout,batchnorm)\n","    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n","\n","    up3_2 = Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)\n","    conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=3)\n","    conv3_2=conv_block(conv3_2, kernelsize, filters[2],dropout,batchnorm)\n","\n","    up2_3 = Conv2DTranspose(filters[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)\n","    conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=3)\n","    conv2_3=conv_block(conv2_3, kernelsize, filters[1],dropout,batchnorm)\n","\n","    up1_4 = Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)\n","    conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=3)\n","    conv1_4=conv_block(conv1_4, kernelsize, filters[0],dropout,batchnorm)\n","\n","    conv5_1=conv_block(pool4, kernelsize, filters[4],dropout,batchnorm)\n","    up4_2 = Conv2DTranspose(filters[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)\n","    conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=3)\n","    conv4_2=conv_block(conv4_2, kernelsize, filters[3],dropout,batchnorm)\n","\n","    up3_3 = Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)\n","    conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=3)\n","    conv3_3=conv_block(conv3_3, kernelsize, filters[2],dropout,batchnorm)\n","\n","    up2_4 = Conv2DTranspose(filters[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)\n","    conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=3)\n","    conv2_4=conv_block(conv2_4, kernelsize, filters[1],dropout,batchnorm)\n","\n","    up1_5 = Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)\n","    conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=3)\n","    conv1_5=conv_block(conv1_5, kernelsize, filters[0],dropout,batchnorm)\n","\n","    conv_final = layers.Conv2D(1, kernel_size=(1,1))(conv1_5)\n","    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n","    outputs = layers.Activation('sigmoid')(conv_final)  \n","    \n","    model = models.Model(inputs=[inputs], outputs=[outputs]) \n","\n","    return model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:05:48.529722Z","iopub.status.busy":"2023-04-03T15:05:48.529063Z","iopub.status.idle":"2023-04-03T15:05:49.965300Z","shell.execute_reply":"2023-04-03T15:05:49.964433Z","shell.execute_reply.started":"2023-04-03T15:05:48.529667Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," main_input (InputLayer)        [(None, 256, 256, 1  0           []                               \n","                                0)]                                                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 256, 256, 32  2912        ['main_input[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 256, 256, 32  128        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," activation (Activation)        (None, 256, 256, 32  0           ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 256, 256, 32  9248        ['activation[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 256, 256, 32  128        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_1 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," pool1 (MaxPooling2D)           (None, 128, 128, 32  0           ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 128, 128, 64  18496       ['pool1[0][0]']                  \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 128, 128, 64  256        ['conv2d_2[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_2 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_2[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 128, 128, 64  36928       ['activation_2[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 128, 128, 64  256        ['conv2d_3[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_3 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_3[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," pool2 (MaxPooling2D)           (None, 64, 64, 64)   0           ['activation_3[0][0]']           \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 64, 64, 128)  73856       ['pool2[0][0]']                  \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_6[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_6 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 64, 64, 128)  147584      ['activation_6[0][0]']           \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_7[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_7 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," pool3 (MaxPooling2D)           (None, 32, 32, 128)  0           ['activation_7[0][0]']           \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 32, 32, 256)  295168      ['pool3[0][0]']                  \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_12[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_12 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_12[0][0]']          \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_13[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_13 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," pool4 (MaxPooling2D)           (None, 16, 16, 256)  0           ['activation_13[0][0]']          \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 16, 16, 512)  1180160     ['pool4[0][0]']                  \n","                                                                                                  \n"," batch_normalization_20 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_20[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_20 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_20[0][0]'] \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 16, 16, 512)  2359808     ['activation_20[0][0]']          \n","                                                                                                  \n"," batch_normalization_21 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_21[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_21 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_21[0][0]'] \n","                                                                                                  \n"," up42 (Conv2DTranspose)         (None, 32, 32, 256)  524544      ['activation_21[0][0]']          \n","                                                                                                  \n"," merge42 (Concatenate)          (None, 32, 32, 512)  0           ['up42[0][0]',                   \n","                                                                  'activation_13[0][0]']          \n","                                                                                                  \n"," up32 (Conv2DTranspose)         (None, 64, 64, 128)  131200      ['activation_13[0][0]']          \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 32, 32, 256)  1179904     ['merge42[0][0]']                \n","                                                                                                  \n"," merge32 (Concatenate)          (None, 64, 64, 256)  0           ['up32[0][0]',                   \n","                                                                  'activation_7[0][0]']           \n","                                                                                                  \n"," up22 (Conv2DTranspose)         (None, 128, 128, 64  32832       ['activation_7[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_22 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_22[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 64, 64, 128)  295040      ['merge32[0][0]']                \n","                                                                                                  \n"," merge22 (Concatenate)          (None, 128, 128, 12  0           ['up22[0][0]',                   \n","                                8)                                'activation_3[0][0]']           \n","                                                                                                  \n"," up12 (Conv2DTranspose)         (None, 256, 256, 32  8224        ['activation_3[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," activation_22 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_22[0][0]'] \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 64, 64, 128)  512        ['conv2d_14[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 128, 128, 64  73792       ['merge22[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," merge12 (Concatenate)          (None, 256, 256, 64  0           ['up12[0][0]',                   \n","                                )                                 'activation_1[0][0]']           \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_22[0][0]']          \n","                                                                                                  \n"," activation_14 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_14[0][0]'] \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 128, 128, 64  256        ['conv2d_8[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 256, 256, 32  18464       ['merge12[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_23 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_23[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_14[0][0]']          \n","                                                                                                  \n"," activation_8 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_8[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 256, 256, 32  128        ['conv2d_4[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_23 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_23[0][0]'] \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 64, 64, 128)  512        ['conv2d_15[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 128, 128, 64  36928       ['activation_8[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," activation_4 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_4[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," up33 (Conv2DTranspose)         (None, 64, 64, 128)  131200      ['activation_23[0][0]']          \n","                                                                                                  \n"," activation_15 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_15[0][0]'] \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 128, 128, 64  256        ['conv2d_9[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 256, 256, 32  9248        ['activation_4[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," merge33 (Concatenate)          (None, 64, 64, 384)  0           ['up33[0][0]',                   \n","                                                                  'activation_7[0][0]',           \n","                                                                  'activation_15[0][0]']          \n","                                                                                                  \n"," activation_9 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_9[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," up23 (Conv2DTranspose)         (None, 128, 128, 64  32832       ['activation_15[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 256, 256, 32  128        ['conv2d_5[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," conv2d_24 (Conv2D)             (None, 64, 64, 128)  442496      ['merge33[0][0]']                \n","                                                                                                  \n"," merge23 (Concatenate)          (None, 128, 128, 19  0           ['up23[0][0]',                   \n","                                2)                                'activation_3[0][0]',           \n","                                                                  'activation_9[0][0]']           \n","                                                                                                  \n"," activation_5 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_5[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," up13 (Conv2DTranspose)         (None, 256, 256, 32  8224        ['activation_9[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_24 (BatchN  (None, 64, 64, 128)  512        ['conv2d_24[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 128, 128, 64  110656      ['merge23[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," merge13 (Concatenate)          (None, 256, 256, 96  0           ['up13[0][0]',                   \n","                                )                                 'activation_1[0][0]',           \n","                                                                  'activation_5[0][0]']           \n","                                                                                                  \n"," activation_24 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_24[0][0]'] \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 128, 128, 64  256        ['conv2d_16[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 256, 256, 32  27680       ['merge13[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_25 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_24[0][0]']          \n","                                                                                                  \n"," activation_16 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_16[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 256, 256, 32  128        ['conv2d_10[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_25 (BatchN  (None, 64, 64, 128)  512        ['conv2d_25[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 128, 128, 64  36928       ['activation_16[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_10 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_10[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_25 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_25[0][0]'] \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 128, 128, 64  256        ['conv2d_17[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 256, 256, 32  9248        ['activation_10[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," up24 (Conv2DTranspose)         (None, 128, 128, 64  32832       ['activation_25[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_17 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_17[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 256, 256, 32  128        ['conv2d_11[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," merge24 (Concatenate)          (None, 128, 128, 25  0           ['up24[0][0]',                   \n","                                6)                                'activation_3[0][0]',           \n","                                                                  'activation_9[0][0]',           \n","                                                                  'activation_17[0][0]']          \n","                                                                                                  \n"," activation_11 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_11[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," up14 (Conv2DTranspose)         (None, 256, 256, 32  8224        ['activation_17[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_26 (Conv2D)             (None, 128, 128, 64  147520      ['merge24[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," merge14 (Concatenate)          (None, 256, 256, 12  0           ['up14[0][0]',                   \n","                                8)                                'activation_1[0][0]',           \n","                                                                  'activation_5[0][0]',           \n","                                                                  'activation_11[0][0]']          \n","                                                                                                  \n"," batch_normalization_26 (BatchN  (None, 128, 128, 64  256        ['conv2d_26[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 256, 256, 32  36896       ['merge14[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," activation_26 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_26[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 256, 256, 32  128        ['conv2d_18[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_27 (Conv2D)             (None, 128, 128, 64  36928       ['activation_26[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_18 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_18[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_27 (BatchN  (None, 128, 128, 64  256        ['conv2d_27[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 256, 256, 32  9248        ['activation_18[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_27 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_27[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_19 (BatchN  (None, 256, 256, 32  128        ['conv2d_19[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," up15 (Conv2DTranspose)         (None, 256, 256, 32  8224        ['activation_27[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_19 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_19[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," merge15 (Concatenate)          (None, 256, 256, 16  0           ['up15[0][0]',                   \n","                                0)                                'activation_1[0][0]',           \n","                                                                  'activation_5[0][0]',           \n","                                                                  'activation_11[0][0]',          \n","                                                                  'activation_19[0][0]']          \n","                                                                                                  \n"," conv2d_28 (Conv2D)             (None, 256, 256, 32  46112       ['merge15[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_28 (BatchN  (None, 256, 256, 32  128        ['conv2d_28[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_28 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_28[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_29 (Conv2D)             (None, 256, 256, 32  9248        ['activation_28[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_29 (BatchN  (None, 256, 256, 32  128        ['conv2d_29[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_29 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_29[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_30 (Conv2D)             (None, 256, 256, 1)  33          ['activation_29[0][0]']          \n","                                                                                                  \n"," batch_normalization_30 (BatchN  (None, 256, 256, 1)  4          ['conv2d_30[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_30 (Activation)     (None, 256, 256, 1)  0           ['batch_normalization_30[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 9,058,789\n","Trainable params: 9,051,491\n","Non-trainable params: 7,298\n","__________________________________________________________________________________________________\n"]}],"source":["from keras import metrics\n","unet2= unet2(input_shape=(256,256,10))#binary_crossentropy\n","unet2.compile(optimizer = adam_v2.Adam(learning_rate = 1e-4), loss =gl_sl_wrapper(alpha), metrics = ['accuracy',metrics.MeanIoU(2)])\n","unet2.summary()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:06:05.375282Z","iopub.status.busy":"2023-04-03T15:06:05.374552Z","iopub.status.idle":"2023-04-03T17:50:17.927323Z","shell.execute_reply":"2023-04-03T17:50:17.926115Z","shell.execute_reply.started":"2023-04-03T15:06:05.375242Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.4523 - accuracy: 0.8761 - mean_io_u: 0.3501\n","Epoch 1: val_loss improved from inf to 0.59366, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 47s 598ms/step - loss: 0.4523 - accuracy: 0.8761 - mean_io_u: 0.3501 - val_loss: 0.5937 - val_accuracy: 0.7989 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 2/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3933 - accuracy: 0.9489 - mean_io_u: 0.3501\n","Epoch 2: val_loss improved from 0.59366 to 0.57930, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.3933 - accuracy: 0.9489 - mean_io_u: 0.3501 - val_loss: 0.5793 - val_accuracy: 0.7204 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 3/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3817 - accuracy: 0.9613 - mean_io_u: 0.3501\n","Epoch 3: val_loss improved from 0.57930 to 0.51051, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.3817 - accuracy: 0.9613 - mean_io_u: 0.3501 - val_loss: 0.5105 - val_accuracy: 0.8130 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 4/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3745 - accuracy: 0.9655 - mean_io_u: 0.3501\n","Epoch 4: val_loss improved from 0.51051 to 0.43416, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.3745 - accuracy: 0.9655 - mean_io_u: 0.3501 - val_loss: 0.4342 - val_accuracy: 0.9416 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 5/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3755 - accuracy: 0.9656 - mean_io_u: 0.3501\n","Epoch 5: val_loss improved from 0.43416 to 0.42414, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.3755 - accuracy: 0.9656 - mean_io_u: 0.3501 - val_loss: 0.4241 - val_accuracy: 0.9472 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 6/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3660 - accuracy: 0.9715 - mean_io_u: 0.3501\n","Epoch 6: val_loss improved from 0.42414 to 0.39033, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.3660 - accuracy: 0.9715 - mean_io_u: 0.3501 - val_loss: 0.3903 - val_accuracy: 0.9659 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 7/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.9712 - mean_io_u: 0.3501\n","Epoch 7: val_loss improved from 0.39033 to 0.37994, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.3667 - accuracy: 0.9712 - mean_io_u: 0.3501 - val_loss: 0.3799 - val_accuracy: 0.9614 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 8/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3571 - accuracy: 0.9784 - mean_io_u: 0.3501\n","Epoch 8: val_loss did not improve from 0.37994\n","56/56 [==============================] - 27s 487ms/step - loss: 0.3571 - accuracy: 0.9784 - mean_io_u: 0.3501 - val_loss: 0.3808 - val_accuracy: 0.9688 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 9/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.9766 - mean_io_u: 0.3501\n","Epoch 9: val_loss improved from 0.37994 to 0.35629, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.3559 - accuracy: 0.9766 - mean_io_u: 0.3501 - val_loss: 0.3563 - val_accuracy: 0.9809 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 10/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3512 - accuracy: 0.9803 - mean_io_u: 0.3501\n","Epoch 10: val_loss improved from 0.35629 to 0.35092, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.3512 - accuracy: 0.9803 - mean_io_u: 0.3501 - val_loss: 0.3509 - val_accuracy: 0.9822 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 11/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3490 - accuracy: 0.9800 - mean_io_u: 0.3501\n","Epoch 11: val_loss improved from 0.35092 to 0.34403, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.3490 - accuracy: 0.9800 - mean_io_u: 0.3501 - val_loss: 0.3440 - val_accuracy: 0.9811 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 12/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3528 - accuracy: 0.9771 - mean_io_u: 0.3501\n","Epoch 12: val_loss did not improve from 0.34403\n","56/56 [==============================] - 27s 477ms/step - loss: 0.3528 - accuracy: 0.9771 - mean_io_u: 0.3501 - val_loss: 0.3466 - val_accuracy: 0.9801 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 13/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3435 - accuracy: 0.9831 - mean_io_u: 0.3501\n","Epoch 13: val_loss improved from 0.34403 to 0.34298, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.3435 - accuracy: 0.9831 - mean_io_u: 0.3501 - val_loss: 0.3430 - val_accuracy: 0.9829 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 14/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3460 - accuracy: 0.9809 - mean_io_u: 0.3501\n","Epoch 14: val_loss improved from 0.34298 to 0.34129, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.3460 - accuracy: 0.9809 - mean_io_u: 0.3501 - val_loss: 0.3413 - val_accuracy: 0.9854 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 15/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3382 - accuracy: 0.9843 - mean_io_u: 0.3501\n","Epoch 15: val_loss improved from 0.34129 to 0.33764, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.3382 - accuracy: 0.9843 - mean_io_u: 0.3501 - val_loss: 0.3376 - val_accuracy: 0.9853 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 16/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3364 - accuracy: 0.9835 - mean_io_u: 0.3501\n","Epoch 16: val_loss improved from 0.33764 to 0.33614, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.3364 - accuracy: 0.9835 - mean_io_u: 0.3501 - val_loss: 0.3361 - val_accuracy: 0.9824 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 17/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3368 - accuracy: 0.9802 - mean_io_u: 0.3501\n","Epoch 17: val_loss improved from 0.33614 to 0.33323, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.3368 - accuracy: 0.9802 - mean_io_u: 0.3501 - val_loss: 0.3332 - val_accuracy: 0.9853 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 18/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.9835 - mean_io_u: 0.3501\n","Epoch 18: val_loss did not improve from 0.33323\n","56/56 [==============================] - 27s 488ms/step - loss: 0.3340 - accuracy: 0.9835 - mean_io_u: 0.3501 - val_loss: 0.3354 - val_accuracy: 0.9790 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 19/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3349 - accuracy: 0.9789 - mean_io_u: 0.3501\n","Epoch 19: val_loss improved from 0.33323 to 0.32576, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 490ms/step - loss: 0.3349 - accuracy: 0.9789 - mean_io_u: 0.3501 - val_loss: 0.3258 - val_accuracy: 0.9832 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 20/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.9845 - mean_io_u: 0.3501\n","Epoch 20: val_loss did not improve from 0.32576\n","56/56 [==============================] - 27s 488ms/step - loss: 0.3278 - accuracy: 0.9845 - mean_io_u: 0.3501 - val_loss: 0.3306 - val_accuracy: 0.9866 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 21/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.9870 - mean_io_u: 0.3501\n","Epoch 21: val_loss did not improve from 0.32576\n","56/56 [==============================] - 27s 476ms/step - loss: 0.3237 - accuracy: 0.9870 - mean_io_u: 0.3501 - val_loss: 0.3374 - val_accuracy: 0.9857 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 22/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3213 - accuracy: 0.9876 - mean_io_u: 0.3501\n","Epoch 22: val_loss improved from 0.32576 to 0.32196, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.3213 - accuracy: 0.9876 - mean_io_u: 0.3501 - val_loss: 0.3220 - val_accuracy: 0.9872 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 23/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.9867 - mean_io_u: 0.3501\n","Epoch 23: val_loss improved from 0.32196 to 0.32155, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.3195 - accuracy: 0.9867 - mean_io_u: 0.3501 - val_loss: 0.3215 - val_accuracy: 0.9858 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 24/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3216 - accuracy: 0.9843 - mean_io_u: 0.3501\n","Epoch 24: val_loss improved from 0.32155 to 0.31922, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.3216 - accuracy: 0.9843 - mean_io_u: 0.3501 - val_loss: 0.3192 - val_accuracy: 0.9860 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 25/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3172 - accuracy: 0.9855 - mean_io_u: 0.3501\n","Epoch 25: val_loss did not improve from 0.31922\n","56/56 [==============================] - 27s 477ms/step - loss: 0.3172 - accuracy: 0.9855 - mean_io_u: 0.3501 - val_loss: 0.3217 - val_accuracy: 0.9812 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 26/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3173 - accuracy: 0.9834 - mean_io_u: 0.3501\n","Epoch 26: val_loss improved from 0.31922 to 0.31303, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 490ms/step - loss: 0.3173 - accuracy: 0.9834 - mean_io_u: 0.3501 - val_loss: 0.3130 - val_accuracy: 0.9859 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 27/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3182 - accuracy: 0.9824 - mean_io_u: 0.3501\n","Epoch 27: val_loss did not improve from 0.31303\n","56/56 [==============================] - 27s 487ms/step - loss: 0.3182 - accuracy: 0.9824 - mean_io_u: 0.3501 - val_loss: 0.3165 - val_accuracy: 0.9847 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 28/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.9863 - mean_io_u: 0.3501\n","Epoch 28: val_loss improved from 0.31303 to 0.31039, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.3132 - accuracy: 0.9863 - mean_io_u: 0.3501 - val_loss: 0.3104 - val_accuracy: 0.9867 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 29/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3096 - accuracy: 0.9872 - mean_io_u: 0.3501\n","Epoch 29: val_loss did not improve from 0.31039\n","56/56 [==============================] - 27s 487ms/step - loss: 0.3096 - accuracy: 0.9872 - mean_io_u: 0.3501 - val_loss: 0.3119 - val_accuracy: 0.9870 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 30/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.9884 - mean_io_u: 0.3501\n","Epoch 30: val_loss did not improve from 0.31039\n","56/56 [==============================] - 27s 477ms/step - loss: 0.3052 - accuracy: 0.9884 - mean_io_u: 0.3501 - val_loss: 0.3178 - val_accuracy: 0.9871 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 31/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3030 - accuracy: 0.9889 - mean_io_u: 0.3501\n","Epoch 31: val_loss improved from 0.31039 to 0.30813, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 490ms/step - loss: 0.3030 - accuracy: 0.9889 - mean_io_u: 0.3501 - val_loss: 0.3081 - val_accuracy: 0.9864 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 32/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3025 - accuracy: 0.9884 - mean_io_u: 0.3501\n","Epoch 32: val_loss improved from 0.30813 to 0.30542, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.3025 - accuracy: 0.9884 - mean_io_u: 0.3501 - val_loss: 0.3054 - val_accuracy: 0.9878 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 33/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2995 - accuracy: 0.9898 - mean_io_u: 0.3501\n","Epoch 33: val_loss improved from 0.30542 to 0.30175, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.2995 - accuracy: 0.9898 - mean_io_u: 0.3501 - val_loss: 0.3018 - val_accuracy: 0.9891 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 34/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2978 - accuracy: 0.9895 - mean_io_u: 0.3501\n","Epoch 34: val_loss improved from 0.30175 to 0.29925, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.2978 - accuracy: 0.9895 - mean_io_u: 0.3501 - val_loss: 0.2992 - val_accuracy: 0.9889 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 35/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.9903 - mean_io_u: 0.3501\n","Epoch 35: val_loss did not improve from 0.29925\n","56/56 [==============================] - 27s 487ms/step - loss: 0.2942 - accuracy: 0.9903 - mean_io_u: 0.3501 - val_loss: 0.3061 - val_accuracy: 0.9897 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 36/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.9890 - mean_io_u: 0.3501\n","Epoch 36: val_loss improved from 0.29925 to 0.29337, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.2969 - accuracy: 0.9890 - mean_io_u: 0.3501 - val_loss: 0.2934 - val_accuracy: 0.9877 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 37/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2967 - accuracy: 0.9871 - mean_io_u: 0.3501\n","Epoch 37: val_loss improved from 0.29337 to 0.29318, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2967 - accuracy: 0.9871 - mean_io_u: 0.3501 - val_loss: 0.2932 - val_accuracy: 0.9894 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 38/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.9900 - mean_io_u: 0.3501\n","Epoch 38: val_loss improved from 0.29318 to 0.29140, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 500ms/step - loss: 0.2912 - accuracy: 0.9900 - mean_io_u: 0.3501 - val_loss: 0.2914 - val_accuracy: 0.9903 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 39/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2867 - accuracy: 0.9910 - mean_io_u: 0.3501\n","Epoch 39: val_loss improved from 0.29140 to 0.28881, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.2867 - accuracy: 0.9910 - mean_io_u: 0.3501 - val_loss: 0.2888 - val_accuracy: 0.9900 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 40/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9898 - mean_io_u: 0.3501\n","Epoch 40: val_loss improved from 0.28881 to 0.28465, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2881 - accuracy: 0.9898 - mean_io_u: 0.3501 - val_loss: 0.2846 - val_accuracy: 0.9898 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 41/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9900 - mean_io_u: 0.3501\n","Epoch 41: val_loss did not improve from 0.28465\n","56/56 [==============================] - 27s 487ms/step - loss: 0.2858 - accuracy: 0.9900 - mean_io_u: 0.3501 - val_loss: 0.2870 - val_accuracy: 0.9887 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 42/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.9886 - mean_io_u: 0.3501\n","Epoch 42: val_loss did not improve from 0.28465\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2855 - accuracy: 0.9886 - mean_io_u: 0.3501 - val_loss: 0.2905 - val_accuracy: 0.9851 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 43/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.9883 - mean_io_u: 0.3501\n","Epoch 43: val_loss did not improve from 0.28465\n","56/56 [==============================] - 27s 487ms/step - loss: 0.2854 - accuracy: 0.9883 - mean_io_u: 0.3501 - val_loss: 0.2903 - val_accuracy: 0.9877 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 44/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2806 - accuracy: 0.9900 - mean_io_u: 0.3501\n","Epoch 44: val_loss improved from 0.28465 to 0.27997, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 487ms/step - loss: 0.2806 - accuracy: 0.9900 - mean_io_u: 0.3501 - val_loss: 0.2800 - val_accuracy: 0.9896 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 45/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2789 - accuracy: 0.9904 - mean_io_u: 0.3501\n","Epoch 45: val_loss did not improve from 0.27997\n","56/56 [==============================] - 27s 486ms/step - loss: 0.2789 - accuracy: 0.9904 - mean_io_u: 0.3501 - val_loss: 0.2810 - val_accuracy: 0.9898 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 46/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.9892 - mean_io_u: 0.3501\n","Epoch 46: val_loss improved from 0.27997 to 0.27673, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2819 - accuracy: 0.9892 - mean_io_u: 0.3501 - val_loss: 0.2767 - val_accuracy: 0.9879 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 47/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.9899 - mean_io_u: 0.3501\n","Epoch 47: val_loss did not improve from 0.27673\n","56/56 [==============================] - 27s 487ms/step - loss: 0.2779 - accuracy: 0.9899 - mean_io_u: 0.3501 - val_loss: 0.2791 - val_accuracy: 0.9896 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 48/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.9909 - mean_io_u: 0.3501\n","Epoch 48: val_loss improved from 0.27673 to 0.27497, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 500ms/step - loss: 0.2775 - accuracy: 0.9909 - mean_io_u: 0.3501 - val_loss: 0.2750 - val_accuracy: 0.9903 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 49/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.9910 - mean_io_u: 0.3501\n","Epoch 49: val_loss did not improve from 0.27497\n","56/56 [==============================] - 27s 476ms/step - loss: 0.2733 - accuracy: 0.9910 - mean_io_u: 0.3501 - val_loss: 0.2751 - val_accuracy: 0.9899 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 50/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9917 - mean_io_u: 0.3501\n","Epoch 50: val_loss improved from 0.27497 to 0.27178, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.2709 - accuracy: 0.9917 - mean_io_u: 0.3501 - val_loss: 0.2718 - val_accuracy: 0.9913 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 51/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.9919 - mean_io_u: 0.3501\n","Epoch 51: val_loss improved from 0.27178 to 0.26830, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 487ms/step - loss: 0.2712 - accuracy: 0.9919 - mean_io_u: 0.3501 - val_loss: 0.2683 - val_accuracy: 0.9910 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 52/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2678 - accuracy: 0.9921 - mean_io_u: 0.3501\n","Epoch 52: val_loss improved from 0.26830 to 0.26757, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.2678 - accuracy: 0.9921 - mean_io_u: 0.3501 - val_loss: 0.2676 - val_accuracy: 0.9911 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 53/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9921 - mean_io_u: 0.3501\n","Epoch 53: val_loss improved from 0.26757 to 0.26655, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 506ms/step - loss: 0.2640 - accuracy: 0.9921 - mean_io_u: 0.3501 - val_loss: 0.2666 - val_accuracy: 0.9911 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 54/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.9921 - mean_io_u: 0.3501\n","Epoch 54: val_loss improved from 0.26655 to 0.26426, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2645 - accuracy: 0.9921 - mean_io_u: 0.3501 - val_loss: 0.2643 - val_accuracy: 0.9910 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 55/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9918 - mean_io_u: 0.3501\n","Epoch 55: val_loss did not improve from 0.26426\n","56/56 [==============================] - 27s 478ms/step - loss: 0.2620 - accuracy: 0.9918 - mean_io_u: 0.3501 - val_loss: 0.2667 - val_accuracy: 0.9901 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 56/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2631 - accuracy: 0.9909 - mean_io_u: 0.3501\n","Epoch 56: val_loss improved from 0.26426 to 0.26156, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.2631 - accuracy: 0.9909 - mean_io_u: 0.3501 - val_loss: 0.2616 - val_accuracy: 0.9902 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 57/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9915 - mean_io_u: 0.3501\n","Epoch 57: val_loss did not improve from 0.26156\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2615 - accuracy: 0.9915 - mean_io_u: 0.3501 - val_loss: 0.2661 - val_accuracy: 0.9901 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 58/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2599 - accuracy: 0.9915 - mean_io_u: 0.3501\n","Epoch 58: val_loss improved from 0.26156 to 0.25809, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.2599 - accuracy: 0.9915 - mean_io_u: 0.3501 - val_loss: 0.2581 - val_accuracy: 0.9909 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 59/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.9904 - mean_io_u: 0.3501\n","Epoch 59: val_loss did not improve from 0.25809\n","56/56 [==============================] - 27s 487ms/step - loss: 0.2590 - accuracy: 0.9904 - mean_io_u: 0.3501 - val_loss: 0.2585 - val_accuracy: 0.9895 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 60/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2563 - accuracy: 0.9912 - mean_io_u: 0.3501\n","Epoch 60: val_loss improved from 0.25809 to 0.25643, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2563 - accuracy: 0.9912 - mean_io_u: 0.3501 - val_loss: 0.2564 - val_accuracy: 0.9910 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 61/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.9925 - mean_io_u: 0.3501\n","Epoch 61: val_loss improved from 0.25643 to 0.25367, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.2516 - accuracy: 0.9925 - mean_io_u: 0.3501 - val_loss: 0.2537 - val_accuracy: 0.9912 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 62/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.9892 - mean_io_u: 0.3501\n","Epoch 62: val_loss did not improve from 0.25367\n","56/56 [==============================] - 27s 476ms/step - loss: 0.2564 - accuracy: 0.9892 - mean_io_u: 0.3501 - val_loss: 0.2539 - val_accuracy: 0.9888 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 63/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.9911 - mean_io_u: 0.3501\n","Epoch 63: val_loss improved from 0.25367 to 0.24803, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.2521 - accuracy: 0.9911 - mean_io_u: 0.3501 - val_loss: 0.2480 - val_accuracy: 0.9913 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 64/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.9919 - mean_io_u: 0.3501\n","Epoch 64: val_loss improved from 0.24803 to 0.24696, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2481 - accuracy: 0.9919 - mean_io_u: 0.3501 - val_loss: 0.2470 - val_accuracy: 0.9916 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 65/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.9922 - mean_io_u: 0.3501\n","Epoch 65: val_loss did not improve from 0.24696\n","56/56 [==============================] - 27s 476ms/step - loss: 0.2497 - accuracy: 0.9922 - mean_io_u: 0.3501 - val_loss: 0.2579 - val_accuracy: 0.9919 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 66/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.9924 - mean_io_u: 0.3501\n","Epoch 66: val_loss improved from 0.24696 to 0.24673, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.2472 - accuracy: 0.9924 - mean_io_u: 0.3501 - val_loss: 0.2467 - val_accuracy: 0.9916 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 67/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.9920 - mean_io_u: 0.3501\n","Epoch 67: val_loss did not improve from 0.24673\n","56/56 [==============================] - 27s 487ms/step - loss: 0.2459 - accuracy: 0.9920 - mean_io_u: 0.3501 - val_loss: 0.2496 - val_accuracy: 0.9869 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 68/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.9923 - mean_io_u: 0.3501\n","Epoch 68: val_loss improved from 0.24673 to 0.24178, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 490ms/step - loss: 0.2435 - accuracy: 0.9923 - mean_io_u: 0.3501 - val_loss: 0.2418 - val_accuracy: 0.9914 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 69/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2427 - accuracy: 0.9929 - mean_io_u: 0.3501\n","Epoch 69: val_loss improved from 0.24178 to 0.24097, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.2427 - accuracy: 0.9929 - mean_io_u: 0.3501 - val_loss: 0.2410 - val_accuracy: 0.9919 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 70/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2427 - accuracy: 0.9903 - mean_io_u: 0.3501\n","Epoch 70: val_loss improved from 0.24097 to 0.23580, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2427 - accuracy: 0.9903 - mean_io_u: 0.3501 - val_loss: 0.2358 - val_accuracy: 0.9921 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 71/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9929 - mean_io_u: 0.3501\n","Epoch 71: val_loss did not improve from 0.23580\n","56/56 [==============================] - 27s 478ms/step - loss: 0.2411 - accuracy: 0.9929 - mean_io_u: 0.3501 - val_loss: 0.2392 - val_accuracy: 0.9921 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 72/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.9921 - mean_io_u: 0.3501\n","Epoch 72: val_loss did not improve from 0.23580\n","56/56 [==============================] - 27s 487ms/step - loss: 0.2389 - accuracy: 0.9921 - mean_io_u: 0.3501 - val_loss: 0.2370 - val_accuracy: 0.9914 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 73/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2355 - accuracy: 0.9932 - mean_io_u: 0.3501\n","Epoch 73: val_loss improved from 0.23580 to 0.23418, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.2355 - accuracy: 0.9932 - mean_io_u: 0.3501 - val_loss: 0.2342 - val_accuracy: 0.9924 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 74/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.9920 - mean_io_u: 0.3501\n","Epoch 74: val_loss did not improve from 0.23418\n","56/56 [==============================] - 27s 478ms/step - loss: 0.2392 - accuracy: 0.9920 - mean_io_u: 0.3501 - val_loss: 0.2357 - val_accuracy: 0.9923 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 75/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.9925 - mean_io_u: 0.3501\n","Epoch 75: val_loss did not improve from 0.23418\n","56/56 [==============================] - 27s 477ms/step - loss: 0.2351 - accuracy: 0.9925 - mean_io_u: 0.3501 - val_loss: 0.2369 - val_accuracy: 0.9926 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 76/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2324 - accuracy: 0.9934 - mean_io_u: 0.3501\n","Epoch 76: val_loss improved from 0.23418 to 0.23194, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.2324 - accuracy: 0.9934 - mean_io_u: 0.3501 - val_loss: 0.2319 - val_accuracy: 0.9925 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 77/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.9936 - mean_io_u: 0.3501\n","Epoch 77: val_loss improved from 0.23194 to 0.23130, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.2310 - accuracy: 0.9936 - mean_io_u: 0.3501 - val_loss: 0.2313 - val_accuracy: 0.9926 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 78/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.9933 - mean_io_u: 0.3501\n","Epoch 78: val_loss improved from 0.23130 to 0.22672, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.2310 - accuracy: 0.9933 - mean_io_u: 0.3501 - val_loss: 0.2267 - val_accuracy: 0.9919 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 79/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.9939 - mean_io_u: 0.3501\n","Epoch 79: val_loss did not improve from 0.22672\n","56/56 [==============================] - 27s 489ms/step - loss: 0.2282 - accuracy: 0.9939 - mean_io_u: 0.3501 - val_loss: 0.2277 - val_accuracy: 0.9925 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 80/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2281 - accuracy: 0.9937 - mean_io_u: 0.3501\n","Epoch 80: val_loss improved from 0.22672 to 0.22412, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2281 - accuracy: 0.9937 - mean_io_u: 0.3501 - val_loss: 0.2241 - val_accuracy: 0.9929 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 81/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2270 - accuracy: 0.9938 - mean_io_u: 0.3501\n","Epoch 81: val_loss did not improve from 0.22412\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2270 - accuracy: 0.9938 - mean_io_u: 0.3501 - val_loss: 0.2242 - val_accuracy: 0.9933 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 82/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9941 - mean_io_u: 0.3501\n","Epoch 82: val_loss improved from 0.22412 to 0.22297, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.2220 - accuracy: 0.9941 - mean_io_u: 0.3501 - val_loss: 0.2230 - val_accuracy: 0.9933 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 83/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9941 - mean_io_u: 0.3501\n","Epoch 83: val_loss improved from 0.22297 to 0.22003, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 487ms/step - loss: 0.2220 - accuracy: 0.9941 - mean_io_u: 0.3501 - val_loss: 0.2200 - val_accuracy: 0.9929 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 84/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9943 - mean_io_u: 0.3501\n","Epoch 84: val_loss did not improve from 0.22003\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2204 - accuracy: 0.9943 - mean_io_u: 0.3501 - val_loss: 0.2205 - val_accuracy: 0.9924 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 85/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2159 - accuracy: 0.9944 - mean_io_u: 0.3501\n","Epoch 85: val_loss improved from 0.22003 to 0.21778, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2159 - accuracy: 0.9944 - mean_io_u: 0.3501 - val_loss: 0.2178 - val_accuracy: 0.9931 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 86/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2179 - accuracy: 0.9943 - mean_io_u: 0.3501\n","Epoch 86: val_loss improved from 0.21778 to 0.21624, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.2179 - accuracy: 0.9943 - mean_io_u: 0.3501 - val_loss: 0.2162 - val_accuracy: 0.9934 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 87/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2171 - accuracy: 0.9943 - mean_io_u: 0.3501\n","Epoch 87: val_loss improved from 0.21624 to 0.21561, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.2171 - accuracy: 0.9943 - mean_io_u: 0.3501 - val_loss: 0.2156 - val_accuracy: 0.9936 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 88/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.9850 - mean_io_u: 0.3501\n","Epoch 88: val_loss did not improve from 0.21561\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2279 - accuracy: 0.9850 - mean_io_u: 0.3501 - val_loss: 0.2910 - val_accuracy: 0.9262 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 89/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2257 - accuracy: 0.9862 - mean_io_u: 0.3501\n","Epoch 89: val_loss did not improve from 0.21561\n","56/56 [==============================] - 27s 489ms/step - loss: 0.2257 - accuracy: 0.9862 - mean_io_u: 0.3501 - val_loss: 0.2180 - val_accuracy: 0.9867 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 90/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2157 - accuracy: 0.9901 - mean_io_u: 0.3501\n","Epoch 90: val_loss did not improve from 0.21561\n","56/56 [==============================] - 27s 487ms/step - loss: 0.2157 - accuracy: 0.9901 - mean_io_u: 0.3501 - val_loss: 0.2370 - val_accuracy: 0.9798 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 91/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2179 - accuracy: 0.9882 - mean_io_u: 0.3501\n","Epoch 91: val_loss did not improve from 0.21561\n","56/56 [==============================] - 27s 476ms/step - loss: 0.2179 - accuracy: 0.9882 - mean_io_u: 0.3501 - val_loss: 0.3549 - val_accuracy: 0.9112 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 92/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2146 - accuracy: 0.9907 - mean_io_u: 0.3501\n","Epoch 92: val_loss did not improve from 0.21561\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2146 - accuracy: 0.9907 - mean_io_u: 0.3501 - val_loss: 0.2733 - val_accuracy: 0.9236 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 93/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2130 - accuracy: 0.9915 - mean_io_u: 0.3501\n","Epoch 93: val_loss improved from 0.21561 to 0.20998, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.2130 - accuracy: 0.9915 - mean_io_u: 0.3501 - val_loss: 0.2100 - val_accuracy: 0.9904 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 94/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2105 - accuracy: 0.9928 - mean_io_u: 0.3501\n","Epoch 94: val_loss did not improve from 0.20998\n","56/56 [==============================] - 27s 478ms/step - loss: 0.2105 - accuracy: 0.9928 - mean_io_u: 0.3501 - val_loss: 0.2119 - val_accuracy: 0.9911 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 95/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.9934 - mean_io_u: 0.3501\n","Epoch 95: val_loss did not improve from 0.20998\n","56/56 [==============================] - 27s 487ms/step - loss: 0.2076 - accuracy: 0.9934 - mean_io_u: 0.3501 - val_loss: 0.2106 - val_accuracy: 0.9918 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 96/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2075 - accuracy: 0.9934 - mean_io_u: 0.3501\n","Epoch 96: val_loss improved from 0.20998 to 0.20907, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2075 - accuracy: 0.9934 - mean_io_u: 0.3501 - val_loss: 0.2091 - val_accuracy: 0.9924 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 97/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2078 - accuracy: 0.9933 - mean_io_u: 0.3501\n","Epoch 97: val_loss improved from 0.20907 to 0.20337, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 500ms/step - loss: 0.2078 - accuracy: 0.9933 - mean_io_u: 0.3501 - val_loss: 0.2034 - val_accuracy: 0.9928 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 98/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9936 - mean_io_u: 0.3501\n","Epoch 98: val_loss improved from 0.20337 to 0.20067, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 503ms/step - loss: 0.2053 - accuracy: 0.9936 - mean_io_u: 0.3501 - val_loss: 0.2007 - val_accuracy: 0.9932 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 99/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2025 - accuracy: 0.9941 - mean_io_u: 0.3501\n","Epoch 99: val_loss did not improve from 0.20067\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2025 - accuracy: 0.9941 - mean_io_u: 0.3501 - val_loss: 0.2013 - val_accuracy: 0.9933 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 100/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.9940 - mean_io_u: 0.3501\n","Epoch 100: val_loss improved from 0.20067 to 0.20053, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2028 - accuracy: 0.9940 - mean_io_u: 0.3501 - val_loss: 0.2005 - val_accuracy: 0.9927 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 101/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1992 - accuracy: 0.9942 - mean_io_u: 0.3501\n","Epoch 101: val_loss improved from 0.20053 to 0.19854, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.1992 - accuracy: 0.9942 - mean_io_u: 0.3501 - val_loss: 0.1985 - val_accuracy: 0.9934 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 102/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2012 - accuracy: 0.9944 - mean_io_u: 0.3501\n","Epoch 102: val_loss improved from 0.19854 to 0.19742, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.2012 - accuracy: 0.9944 - mean_io_u: 0.3501 - val_loss: 0.1974 - val_accuracy: 0.9936 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 103/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1988 - accuracy: 0.9946 - mean_io_u: 0.3501\n","Epoch 103: val_loss improved from 0.19742 to 0.19669, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1988 - accuracy: 0.9946 - mean_io_u: 0.3501 - val_loss: 0.1967 - val_accuracy: 0.9936 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 104/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9942 - mean_io_u: 0.3501\n","Epoch 104: val_loss improved from 0.19669 to 0.19524, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.2003 - accuracy: 0.9942 - mean_io_u: 0.3501 - val_loss: 0.1952 - val_accuracy: 0.9935 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 105/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.9947 - mean_io_u: 0.3501\n","Epoch 105: val_loss improved from 0.19524 to 0.19432, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1955 - accuracy: 0.9947 - mean_io_u: 0.3501 - val_loss: 0.1943 - val_accuracy: 0.9937 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 106/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1951 - accuracy: 0.9947 - mean_io_u: 0.3501\n","Epoch 106: val_loss improved from 0.19432 to 0.19385, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.1951 - accuracy: 0.9947 - mean_io_u: 0.3501 - val_loss: 0.1939 - val_accuracy: 0.9937 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 107/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1945 - accuracy: 0.9947 - mean_io_u: 0.3501\n","Epoch 107: val_loss did not improve from 0.19385\n","56/56 [==============================] - 27s 477ms/step - loss: 0.1945 - accuracy: 0.9947 - mean_io_u: 0.3501 - val_loss: 0.1962 - val_accuracy: 0.9935 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 108/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9943 - mean_io_u: 0.3501\n","Epoch 108: val_loss did not improve from 0.19385\n","56/56 [==============================] - 27s 476ms/step - loss: 0.1932 - accuracy: 0.9943 - mean_io_u: 0.3501 - val_loss: 0.1999 - val_accuracy: 0.9933 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 109/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9943 - mean_io_u: 0.3501\n","Epoch 109: val_loss improved from 0.19385 to 0.19071, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1939 - accuracy: 0.9943 - mean_io_u: 0.3501 - val_loss: 0.1907 - val_accuracy: 0.9927 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 110/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9924 - mean_io_u: 0.3501\n","Epoch 110: val_loss improved from 0.19071 to 0.18883, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.1961 - accuracy: 0.9924 - mean_io_u: 0.3501 - val_loss: 0.1888 - val_accuracy: 0.9923 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 111/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.9936 - mean_io_u: 0.3501\n","Epoch 111: val_loss improved from 0.18883 to 0.18607, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.1936 - accuracy: 0.9936 - mean_io_u: 0.3501 - val_loss: 0.1861 - val_accuracy: 0.9935 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 112/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1908 - accuracy: 0.9947 - mean_io_u: 0.3501\n","Epoch 112: val_loss did not improve from 0.18607\n","56/56 [==============================] - 27s 477ms/step - loss: 0.1908 - accuracy: 0.9947 - mean_io_u: 0.3501 - val_loss: 0.1879 - val_accuracy: 0.9936 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 113/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1900 - accuracy: 0.9949 - mean_io_u: 0.3501\n","Epoch 113: val_loss improved from 0.18607 to 0.18592, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.1900 - accuracy: 0.9949 - mean_io_u: 0.3501 - val_loss: 0.1859 - val_accuracy: 0.9940 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 114/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1848 - accuracy: 0.9950 - mean_io_u: 0.3501\n","Epoch 114: val_loss did not improve from 0.18592\n","56/56 [==============================] - 27s 476ms/step - loss: 0.1848 - accuracy: 0.9950 - mean_io_u: 0.3501 - val_loss: 0.1870 - val_accuracy: 0.9937 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 115/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9951 - mean_io_u: 0.3501\n","Epoch 115: val_loss improved from 0.18592 to 0.18407, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1840 - accuracy: 0.9951 - mean_io_u: 0.3501 - val_loss: 0.1841 - val_accuracy: 0.9939 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 116/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9949 - mean_io_u: 0.3501\n","Epoch 116: val_loss did not improve from 0.18407\n","56/56 [==============================] - 27s 487ms/step - loss: 0.1898 - accuracy: 0.9949 - mean_io_u: 0.3501 - val_loss: 0.1860 - val_accuracy: 0.9922 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 117/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1830 - accuracy: 0.9951 - mean_io_u: 0.3501\n","Epoch 117: val_loss improved from 0.18407 to 0.18094, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1830 - accuracy: 0.9951 - mean_io_u: 0.3501 - val_loss: 0.1809 - val_accuracy: 0.9941 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 118/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9953 - mean_io_u: 0.3501\n","Epoch 118: val_loss improved from 0.18094 to 0.18028, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.1810 - accuracy: 0.9953 - mean_io_u: 0.3501 - val_loss: 0.1803 - val_accuracy: 0.9943 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 119/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9953 - mean_io_u: 0.3501\n","Epoch 119: val_loss improved from 0.18028 to 0.17873, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 487ms/step - loss: 0.1800 - accuracy: 0.9953 - mean_io_u: 0.3501 - val_loss: 0.1787 - val_accuracy: 0.9942 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 120/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1831 - accuracy: 0.9938 - mean_io_u: 0.3501\n","Epoch 120: val_loss did not improve from 0.17873\n","56/56 [==============================] - 27s 487ms/step - loss: 0.1831 - accuracy: 0.9938 - mean_io_u: 0.3501 - val_loss: 0.1794 - val_accuracy: 0.9882 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 121/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1811 - accuracy: 0.9935 - mean_io_u: 0.3501\n","Epoch 121: val_loss did not improve from 0.17873\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1811 - accuracy: 0.9935 - mean_io_u: 0.3501 - val_loss: 0.1873 - val_accuracy: 0.9934 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 122/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1768 - accuracy: 0.9945 - mean_io_u: 0.3501\n","Epoch 122: val_loss did not improve from 0.17873\n","56/56 [==============================] - 27s 487ms/step - loss: 0.1768 - accuracy: 0.9945 - mean_io_u: 0.3501 - val_loss: 0.2116 - val_accuracy: 0.9598 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 123/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1802 - accuracy: 0.9940 - mean_io_u: 0.3501\n","Epoch 123: val_loss did not improve from 0.17873\n","56/56 [==============================] - 27s 487ms/step - loss: 0.1802 - accuracy: 0.9940 - mean_io_u: 0.3501 - val_loss: 0.2131 - val_accuracy: 0.9646 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 124/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9945 - mean_io_u: 0.3501\n","Epoch 124: val_loss improved from 0.17873 to 0.17317, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1773 - accuracy: 0.9945 - mean_io_u: 0.3501 - val_loss: 0.1732 - val_accuracy: 0.9934 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 125/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1814 - accuracy: 0.9903 - mean_io_u: 0.3501\n","Epoch 125: val_loss did not improve from 0.17317\n","56/56 [==============================] - 27s 476ms/step - loss: 0.1814 - accuracy: 0.9903 - mean_io_u: 0.3501 - val_loss: 0.1913 - val_accuracy: 0.9767 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 126/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1778 - accuracy: 0.9920 - mean_io_u: 0.3501\n","Epoch 126: val_loss did not improve from 0.17317\n","56/56 [==============================] - 27s 477ms/step - loss: 0.1778 - accuracy: 0.9920 - mean_io_u: 0.3501 - val_loss: 0.1738 - val_accuracy: 0.9907 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 127/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9940 - mean_io_u: 0.3501\n","Epoch 127: val_loss improved from 0.17317 to 0.17298, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.1739 - accuracy: 0.9940 - mean_io_u: 0.3501 - val_loss: 0.1730 - val_accuracy: 0.9930 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 128/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9941 - mean_io_u: 0.3501\n","Epoch 128: val_loss improved from 0.17298 to 0.17130, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1760 - accuracy: 0.9941 - mean_io_u: 0.3501 - val_loss: 0.1713 - val_accuracy: 0.9935 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 129/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1730 - accuracy: 0.9949 - mean_io_u: 0.3501\n","Epoch 129: val_loss improved from 0.17130 to 0.16961, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 500ms/step - loss: 0.1730 - accuracy: 0.9949 - mean_io_u: 0.3501 - val_loss: 0.1696 - val_accuracy: 0.9937 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 130/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.9953 - mean_io_u: 0.3501\n","Epoch 130: val_loss improved from 0.16961 to 0.16943, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.1706 - accuracy: 0.9953 - mean_io_u: 0.3501 - val_loss: 0.1694 - val_accuracy: 0.9941 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 131/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9955 - mean_io_u: 0.3501\n","Epoch 131: val_loss improved from 0.16943 to 0.16927, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.1704 - accuracy: 0.9955 - mean_io_u: 0.3501 - val_loss: 0.1693 - val_accuracy: 0.9937 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 132/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.9953 - mean_io_u: 0.3501\n","Epoch 132: val_loss improved from 0.16927 to 0.16744, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.1708 - accuracy: 0.9953 - mean_io_u: 0.3501 - val_loss: 0.1674 - val_accuracy: 0.9939 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 133/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1715 - accuracy: 0.9954 - mean_io_u: 0.3501\n","Epoch 133: val_loss improved from 0.16744 to 0.16698, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.1715 - accuracy: 0.9954 - mean_io_u: 0.3501 - val_loss: 0.1670 - val_accuracy: 0.9941 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 134/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.9955 - mean_io_u: 0.3501\n","Epoch 134: val_loss improved from 0.16698 to 0.16615, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1675 - accuracy: 0.9955 - mean_io_u: 0.3501 - val_loss: 0.1662 - val_accuracy: 0.9940 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 135/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.9958 - mean_io_u: 0.3501\n","Epoch 135: val_loss improved from 0.16615 to 0.16425, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1650 - accuracy: 0.9958 - mean_io_u: 0.3501 - val_loss: 0.1642 - val_accuracy: 0.9944 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 136/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9958 - mean_io_u: 0.3501\n","Epoch 136: val_loss improved from 0.16425 to 0.16419, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1669 - accuracy: 0.9958 - mean_io_u: 0.3501 - val_loss: 0.1642 - val_accuracy: 0.9943 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 137/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1626 - accuracy: 0.9958 - mean_io_u: 0.3501\n","Epoch 137: val_loss improved from 0.16419 to 0.16238, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 490ms/step - loss: 0.1626 - accuracy: 0.9958 - mean_io_u: 0.3501 - val_loss: 0.1624 - val_accuracy: 0.9946 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 138/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1626 - accuracy: 0.9960 - mean_io_u: 0.3501\n","Epoch 138: val_loss improved from 0.16238 to 0.15985, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.1626 - accuracy: 0.9960 - mean_io_u: 0.3501 - val_loss: 0.1598 - val_accuracy: 0.9947 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 139/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1600 - accuracy: 0.9960 - mean_io_u: 0.3501\n","Epoch 139: val_loss improved from 0.15985 to 0.15904, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.1600 - accuracy: 0.9960 - mean_io_u: 0.3501 - val_loss: 0.1590 - val_accuracy: 0.9948 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 140/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.9960 - mean_io_u: 0.3501\n","Epoch 140: val_loss did not improve from 0.15904\n","56/56 [==============================] - 27s 487ms/step - loss: 0.1638 - accuracy: 0.9960 - mean_io_u: 0.3501 - val_loss: 0.1592 - val_accuracy: 0.9948 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 141/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1592 - accuracy: 0.9957 - mean_io_u: 0.3501\n","Epoch 141: val_loss improved from 0.15904 to 0.15805, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1592 - accuracy: 0.9957 - mean_io_u: 0.3501 - val_loss: 0.1580 - val_accuracy: 0.9946 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 142/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9960 - mean_io_u: 0.3501\n","Epoch 142: val_loss improved from 0.15805 to 0.15634, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.1590 - accuracy: 0.9960 - mean_io_u: 0.3501 - val_loss: 0.1563 - val_accuracy: 0.9949 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 143/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9961 - mean_io_u: 0.3501\n","Epoch 143: val_loss did not improve from 0.15634\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1586 - accuracy: 0.9961 - mean_io_u: 0.3501 - val_loss: 0.1570 - val_accuracy: 0.9945 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 144/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9959 - mean_io_u: 0.3501\n","Epoch 144: val_loss improved from 0.15634 to 0.15406, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1613 - accuracy: 0.9959 - mean_io_u: 0.3501 - val_loss: 0.1541 - val_accuracy: 0.9949 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 145/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9953 - mean_io_u: 0.3501\n","Epoch 145: val_loss did not improve from 0.15406\n","56/56 [==============================] - 27s 478ms/step - loss: 0.1599 - accuracy: 0.9953 - mean_io_u: 0.3501 - val_loss: 0.1633 - val_accuracy: 0.9882 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 146/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.9956 - mean_io_u: 0.3501\n","Epoch 146: val_loss improved from 0.15406 to 0.15275, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 490ms/step - loss: 0.1565 - accuracy: 0.9956 - mean_io_u: 0.3501 - val_loss: 0.1528 - val_accuracy: 0.9947 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 147/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1524 - accuracy: 0.9961 - mean_io_u: 0.3501\n","Epoch 147: val_loss improved from 0.15275 to 0.15222, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1524 - accuracy: 0.9961 - mean_io_u: 0.3501 - val_loss: 0.1522 - val_accuracy: 0.9950 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 148/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.9962 - mean_io_u: 0.3501\n","Epoch 148: val_loss improved from 0.15222 to 0.15143, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.1522 - accuracy: 0.9962 - mean_io_u: 0.3501 - val_loss: 0.1514 - val_accuracy: 0.9951 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 149/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.9962 - mean_io_u: 0.3501\n","Epoch 149: val_loss improved from 0.15143 to 0.15075, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1525 - accuracy: 0.9962 - mean_io_u: 0.3501 - val_loss: 0.1508 - val_accuracy: 0.9950 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 150/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1544 - accuracy: 0.9963 - mean_io_u: 0.3501\n","Epoch 150: val_loss improved from 0.15075 to 0.15067, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 498ms/step - loss: 0.1544 - accuracy: 0.9963 - mean_io_u: 0.3501 - val_loss: 0.1507 - val_accuracy: 0.9948 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 151/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 0.9963 - mean_io_u: 0.3501\n","Epoch 151: val_loss improved from 0.15067 to 0.14886, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1519 - accuracy: 0.9963 - mean_io_u: 0.3501 - val_loss: 0.1489 - val_accuracy: 0.9948 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 152/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.9962 - mean_io_u: 0.3501\n","Epoch 152: val_loss improved from 0.14886 to 0.14453, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.1538 - accuracy: 0.9962 - mean_io_u: 0.3501 - val_loss: 0.1445 - val_accuracy: 0.9943 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 153/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9962 - mean_io_u: 0.3501\n","Epoch 153: val_loss did not improve from 0.14453\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1491 - accuracy: 0.9962 - mean_io_u: 0.3501 - val_loss: 0.1448 - val_accuracy: 0.9949 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 154/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.9965 - mean_io_u: 0.3501\n","Epoch 154: val_loss did not improve from 0.14453\n","56/56 [==============================] - 27s 477ms/step - loss: 0.1468 - accuracy: 0.9965 - mean_io_u: 0.3501 - val_loss: 0.1462 - val_accuracy: 0.9950 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 155/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 0.9965 - mean_io_u: 0.3501\n","Epoch 155: val_loss did not improve from 0.14453\n","56/56 [==============================] - 27s 476ms/step - loss: 0.1470 - accuracy: 0.9965 - mean_io_u: 0.3501 - val_loss: 0.1453 - val_accuracy: 0.9953 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 156/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1485 - accuracy: 0.9964 - mean_io_u: 0.3501\n","Epoch 156: val_loss improved from 0.14453 to 0.14392, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 500ms/step - loss: 0.1485 - accuracy: 0.9964 - mean_io_u: 0.3501 - val_loss: 0.1439 - val_accuracy: 0.9947 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 157/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1466 - accuracy: 0.9964 - mean_io_u: 0.3501\n","Epoch 157: val_loss improved from 0.14392 to 0.14384, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.1466 - accuracy: 0.9964 - mean_io_u: 0.3501 - val_loss: 0.1438 - val_accuracy: 0.9951 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 158/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1459 - accuracy: 0.9964 - mean_io_u: 0.3501\n","Epoch 158: val_loss improved from 0.14384 to 0.14381, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.1459 - accuracy: 0.9964 - mean_io_u: 0.3501 - val_loss: 0.1438 - val_accuracy: 0.9951 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 159/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1438 - accuracy: 0.9966 - mean_io_u: 0.3501\n","Epoch 159: val_loss improved from 0.14381 to 0.14284, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 500ms/step - loss: 0.1438 - accuracy: 0.9966 - mean_io_u: 0.3501 - val_loss: 0.1428 - val_accuracy: 0.9952 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 160/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9965 - mean_io_u: 0.3501\n","Epoch 160: val_loss improved from 0.14284 to 0.14167, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 500ms/step - loss: 0.1452 - accuracy: 0.9965 - mean_io_u: 0.3501 - val_loss: 0.1417 - val_accuracy: 0.9954 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 161/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1445 - accuracy: 0.9966 - mean_io_u: 0.3501\n","Epoch 161: val_loss improved from 0.14167 to 0.14091, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 490ms/step - loss: 0.1445 - accuracy: 0.9966 - mean_io_u: 0.3501 - val_loss: 0.1409 - val_accuracy: 0.9955 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 162/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.9967 - mean_io_u: 0.3501\n","Epoch 162: val_loss improved from 0.14091 to 0.14042, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.1410 - accuracy: 0.9967 - mean_io_u: 0.3501 - val_loss: 0.1404 - val_accuracy: 0.9955 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 163/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1408 - accuracy: 0.9968 - mean_io_u: 0.3501\n","Epoch 163: val_loss improved from 0.14042 to 0.13890, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1408 - accuracy: 0.9968 - mean_io_u: 0.3501 - val_loss: 0.1389 - val_accuracy: 0.9956 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 164/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.9967 - mean_io_u: 0.3501\n","Epoch 164: val_loss improved from 0.13890 to 0.13806, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1401 - accuracy: 0.9967 - mean_io_u: 0.3501 - val_loss: 0.1381 - val_accuracy: 0.9954 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 165/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1418 - accuracy: 0.9966 - mean_io_u: 0.3501\n","Epoch 165: val_loss did not improve from 0.13806\n","56/56 [==============================] - 27s 478ms/step - loss: 0.1418 - accuracy: 0.9966 - mean_io_u: 0.3501 - val_loss: 0.1381 - val_accuracy: 0.9942 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 166/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9966 - mean_io_u: 0.3501\n","Epoch 166: val_loss improved from 0.13806 to 0.13615, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1388 - accuracy: 0.9966 - mean_io_u: 0.3501 - val_loss: 0.1362 - val_accuracy: 0.9951 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 167/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 0.9966 - mean_io_u: 0.3501\n","Epoch 167: val_loss improved from 0.13615 to 0.13574, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.1416 - accuracy: 0.9966 - mean_io_u: 0.3501 - val_loss: 0.1357 - val_accuracy: 0.9953 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 168/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9968 - mean_io_u: 0.3501\n","Epoch 168: val_loss improved from 0.13574 to 0.13382, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1400 - accuracy: 0.9968 - mean_io_u: 0.3501 - val_loss: 0.1338 - val_accuracy: 0.9954 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 169/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9967 - mean_io_u: 0.3501\n","Epoch 169: val_loss did not improve from 0.13382\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1379 - accuracy: 0.9967 - mean_io_u: 0.3501 - val_loss: 0.1338 - val_accuracy: 0.9951 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 170/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1393 - accuracy: 0.9967 - mean_io_u: 0.3501\n","Epoch 170: val_loss improved from 0.13382 to 0.13316, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 499ms/step - loss: 0.1393 - accuracy: 0.9967 - mean_io_u: 0.3501 - val_loss: 0.1332 - val_accuracy: 0.9952 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 171/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1368 - accuracy: 0.9969 - mean_io_u: 0.3501\n","Epoch 171: val_loss did not improve from 0.13316\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1368 - accuracy: 0.9969 - mean_io_u: 0.3501 - val_loss: 0.1334 - val_accuracy: 0.9954 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 172/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.9971 - mean_io_u: 0.3501\n","Epoch 172: val_loss improved from 0.13316 to 0.13243, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 490ms/step - loss: 0.1356 - accuracy: 0.9971 - mean_io_u: 0.3501 - val_loss: 0.1324 - val_accuracy: 0.9957 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 173/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9971 - mean_io_u: 0.3501\n","Epoch 173: val_loss improved from 0.13243 to 0.13189, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.1319 - accuracy: 0.9971 - mean_io_u: 0.3501 - val_loss: 0.1319 - val_accuracy: 0.9956 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 174/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9970 - mean_io_u: 0.3501\n","Epoch 174: val_loss improved from 0.13189 to 0.12989, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.1317 - accuracy: 0.9970 - mean_io_u: 0.3501 - val_loss: 0.1299 - val_accuracy: 0.9957 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 175/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9971 - mean_io_u: 0.3501\n","Epoch 175: val_loss improved from 0.12989 to 0.12868, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 495ms/step - loss: 0.1334 - accuracy: 0.9971 - mean_io_u: 0.3501 - val_loss: 0.1287 - val_accuracy: 0.9957 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 176/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9971 - mean_io_u: 0.3501\n","Epoch 176: val_loss did not improve from 0.12868\n","56/56 [==============================] - 27s 479ms/step - loss: 0.1299 - accuracy: 0.9971 - mean_io_u: 0.3501 - val_loss: 0.1287 - val_accuracy: 0.9956 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 177/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.9971 - mean_io_u: 0.3501\n","Epoch 177: val_loss did not improve from 0.12868\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1308 - accuracy: 0.9971 - mean_io_u: 0.3501 - val_loss: 0.1288 - val_accuracy: 0.9956 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 178/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1286 - accuracy: 0.9971 - mean_io_u: 0.3501\n","Epoch 178: val_loss improved from 0.12868 to 0.12745, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.1286 - accuracy: 0.9971 - mean_io_u: 0.3501 - val_loss: 0.1275 - val_accuracy: 0.9957 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 179/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9972 - mean_io_u: 0.3501\n","Epoch 179: val_loss improved from 0.12745 to 0.12678, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.1281 - accuracy: 0.9972 - mean_io_u: 0.3501 - val_loss: 0.1268 - val_accuracy: 0.9958 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 180/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9972 - mean_io_u: 0.3501\n","Epoch 180: val_loss improved from 0.12678 to 0.12585, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 490ms/step - loss: 0.1301 - accuracy: 0.9972 - mean_io_u: 0.3501 - val_loss: 0.1258 - val_accuracy: 0.9956 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 181/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1269 - accuracy: 0.9973 - mean_io_u: 0.3501\n","Epoch 181: val_loss improved from 0.12585 to 0.12507, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 502ms/step - loss: 0.1269 - accuracy: 0.9973 - mean_io_u: 0.3501 - val_loss: 0.1251 - val_accuracy: 0.9959 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 182/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.9971 - mean_io_u: 0.3501\n","Epoch 182: val_loss did not improve from 0.12507\n","56/56 [==============================] - 27s 479ms/step - loss: 0.1305 - accuracy: 0.9971 - mean_io_u: 0.3501 - val_loss: 0.1256 - val_accuracy: 0.9956 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 183/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9973 - mean_io_u: 0.3501\n","Epoch 183: val_loss did not improve from 0.12507\n","56/56 [==============================] - 27s 479ms/step - loss: 0.1278 - accuracy: 0.9973 - mean_io_u: 0.3501 - val_loss: 0.1261 - val_accuracy: 0.9957 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 184/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9972 - mean_io_u: 0.3501\n","Epoch 184: val_loss improved from 0.12507 to 0.12331, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.1263 - accuracy: 0.9972 - mean_io_u: 0.3501 - val_loss: 0.1233 - val_accuracy: 0.9958 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 185/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9974 - mean_io_u: 0.3501\n","Epoch 185: val_loss improved from 0.12331 to 0.12280, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.1256 - accuracy: 0.9974 - mean_io_u: 0.3501 - val_loss: 0.1228 - val_accuracy: 0.9959 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 186/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9974 - mean_io_u: 0.3501\n","Epoch 186: val_loss improved from 0.12280 to 0.12208, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.1235 - accuracy: 0.9974 - mean_io_u: 0.3501 - val_loss: 0.1221 - val_accuracy: 0.9957 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 187/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9974 - mean_io_u: 0.3501\n","Epoch 187: val_loss improved from 0.12208 to 0.12084, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.1241 - accuracy: 0.9974 - mean_io_u: 0.3501 - val_loss: 0.1208 - val_accuracy: 0.9959 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 188/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 0.9974 - mean_io_u: 0.3501\n","Epoch 188: val_loss improved from 0.12084 to 0.11974, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 490ms/step - loss: 0.1229 - accuracy: 0.9974 - mean_io_u: 0.3501 - val_loss: 0.1197 - val_accuracy: 0.9961 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 189/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9973 - mean_io_u: 0.3501\n","Epoch 189: val_loss improved from 0.11974 to 0.11966, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 502ms/step - loss: 0.1231 - accuracy: 0.9973 - mean_io_u: 0.3501 - val_loss: 0.1197 - val_accuracy: 0.9958 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 190/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9974 - mean_io_u: 0.3501\n","Epoch 190: val_loss improved from 0.11966 to 0.11891, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 500ms/step - loss: 0.1200 - accuracy: 0.9974 - mean_io_u: 0.3501 - val_loss: 0.1189 - val_accuracy: 0.9960 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 191/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9976 - mean_io_u: 0.3501\n","Epoch 191: val_loss improved from 0.11891 to 0.11774, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 492ms/step - loss: 0.1200 - accuracy: 0.9976 - mean_io_u: 0.3501 - val_loss: 0.1177 - val_accuracy: 0.9959 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 192/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1188 - accuracy: 0.9975 - mean_io_u: 0.3501\n","Epoch 192: val_loss improved from 0.11774 to 0.11722, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.1188 - accuracy: 0.9975 - mean_io_u: 0.3501 - val_loss: 0.1172 - val_accuracy: 0.9960 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 193/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1199 - accuracy: 0.9976 - mean_io_u: 0.3501\n","Epoch 193: val_loss did not improve from 0.11722\n","56/56 [==============================] - 27s 479ms/step - loss: 0.1199 - accuracy: 0.9976 - mean_io_u: 0.3501 - val_loss: 0.1173 - val_accuracy: 0.9961 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 194/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9975 - mean_io_u: 0.3501\n","Epoch 194: val_loss improved from 0.11722 to 0.11607, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 490ms/step - loss: 0.1183 - accuracy: 0.9975 - mean_io_u: 0.3501 - val_loss: 0.1161 - val_accuracy: 0.9960 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 195/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9974 - mean_io_u: 0.3501\n","Epoch 195: val_loss did not improve from 0.11607\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1173 - accuracy: 0.9974 - mean_io_u: 0.3501 - val_loss: 0.1164 - val_accuracy: 0.9960 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 196/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9975 - mean_io_u: 0.3501\n","Epoch 196: val_loss improved from 0.11607 to 0.11425, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 500ms/step - loss: 0.1190 - accuracy: 0.9975 - mean_io_u: 0.3501 - val_loss: 0.1142 - val_accuracy: 0.9960 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 197/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.9938 - mean_io_u: 0.3501\n","Epoch 197: val_loss did not improve from 0.11425\n","56/56 [==============================] - 27s 480ms/step - loss: 0.1254 - accuracy: 0.9938 - mean_io_u: 0.3501 - val_loss: 0.1278 - val_accuracy: 0.9860 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 198/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9940 - mean_io_u: 0.3501\n","Epoch 198: val_loss improved from 0.11425 to 0.11097, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 492ms/step - loss: 0.1231 - accuracy: 0.9940 - mean_io_u: 0.3501 - val_loss: 0.1110 - val_accuracy: 0.9938 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 199/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1171 - accuracy: 0.9963 - mean_io_u: 0.3501\n","Epoch 199: val_loss did not improve from 0.11097\n","56/56 [==============================] - 27s 480ms/step - loss: 0.1171 - accuracy: 0.9963 - mean_io_u: 0.3501 - val_loss: 0.1155 - val_accuracy: 0.9953 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 200/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9970 - mean_io_u: 0.3501\n","Epoch 200: val_loss did not improve from 0.11097\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1146 - accuracy: 0.9970 - mean_io_u: 0.3501 - val_loss: 0.1146 - val_accuracy: 0.9959 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 201/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9973 - mean_io_u: 0.3501\n","Epoch 201: val_loss did not improve from 0.11097\n","56/56 [==============================] - 27s 480ms/step - loss: 0.1140 - accuracy: 0.9973 - mean_io_u: 0.3501 - val_loss: 0.1143 - val_accuracy: 0.9959 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 202/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9975 - mean_io_u: 0.3501\n","Epoch 202: val_loss did not improve from 0.11097\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1140 - accuracy: 0.9975 - mean_io_u: 0.3501 - val_loss: 0.1131 - val_accuracy: 0.9960 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 203/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9975 - mean_io_u: 0.3501\n","Epoch 203: val_loss did not improve from 0.11097\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1140 - accuracy: 0.9975 - mean_io_u: 0.3501 - val_loss: 0.1115 - val_accuracy: 0.9960 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 204/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.9974 - mean_io_u: 0.3501\n","Epoch 204: val_loss did not improve from 0.11097\n","56/56 [==============================] - 27s 478ms/step - loss: 0.1118 - accuracy: 0.9974 - mean_io_u: 0.3501 - val_loss: 0.1114 - val_accuracy: 0.9960 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 205/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9975 - mean_io_u: 0.3501\n","Epoch 205: val_loss did not improve from 0.11097\n","56/56 [==============================] - 27s 479ms/step - loss: 0.1147 - accuracy: 0.9975 - mean_io_u: 0.3501 - val_loss: 0.1111 - val_accuracy: 0.9962 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 206/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1124 - accuracy: 0.9976 - mean_io_u: 0.3501\n","Epoch 206: val_loss improved from 0.11097 to 0.11088, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 492ms/step - loss: 0.1124 - accuracy: 0.9976 - mean_io_u: 0.3501 - val_loss: 0.1109 - val_accuracy: 0.9959 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 207/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9977 - mean_io_u: 0.3501\n","Epoch 207: val_loss improved from 0.11088 to 0.10944, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.1110 - accuracy: 0.9977 - mean_io_u: 0.3501 - val_loss: 0.1094 - val_accuracy: 0.9963 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 208/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9978 - mean_io_u: 0.3501\n","Epoch 208: val_loss improved from 0.10944 to 0.10829, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 492ms/step - loss: 0.1102 - accuracy: 0.9978 - mean_io_u: 0.3501 - val_loss: 0.1083 - val_accuracy: 0.9960 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 209/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9976 - mean_io_u: 0.3501\n","Epoch 209: val_loss improved from 0.10829 to 0.10685, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.1101 - accuracy: 0.9976 - mean_io_u: 0.3501 - val_loss: 0.1069 - val_accuracy: 0.9963 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 210/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9977 - mean_io_u: 0.3501\n","Epoch 210: val_loss did not improve from 0.10685\n","56/56 [==============================] - 27s 480ms/step - loss: 0.1098 - accuracy: 0.9977 - mean_io_u: 0.3501 - val_loss: 0.1076 - val_accuracy: 0.9963 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 211/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9979 - mean_io_u: 0.3501\n","Epoch 211: val_loss improved from 0.10685 to 0.10569, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1074 - accuracy: 0.9979 - mean_io_u: 0.3501 - val_loss: 0.1057 - val_accuracy: 0.9963 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 212/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9976 - mean_io_u: 0.3501\n","Epoch 212: val_loss improved from 0.10569 to 0.10539, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 493ms/step - loss: 0.1114 - accuracy: 0.9976 - mean_io_u: 0.3501 - val_loss: 0.1054 - val_accuracy: 0.9963 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 213/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9978 - mean_io_u: 0.3501\n","Epoch 213: val_loss did not improve from 0.10539\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1080 - accuracy: 0.9978 - mean_io_u: 0.3501 - val_loss: 0.1056 - val_accuracy: 0.9963 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 214/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9980 - mean_io_u: 0.3501\n","Epoch 214: val_loss improved from 0.10539 to 0.10437, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 492ms/step - loss: 0.1051 - accuracy: 0.9980 - mean_io_u: 0.3501 - val_loss: 0.1044 - val_accuracy: 0.9963 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 215/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9978 - mean_io_u: 0.3501\n","Epoch 215: val_loss did not improve from 0.10437\n","56/56 [==============================] - 27s 478ms/step - loss: 0.1087 - accuracy: 0.9978 - mean_io_u: 0.3501 - val_loss: 0.1048 - val_accuracy: 0.9962 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 216/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9977 - mean_io_u: 0.3501\n","Epoch 216: val_loss improved from 0.10437 to 0.10239, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.1080 - accuracy: 0.9977 - mean_io_u: 0.3501 - val_loss: 0.1024 - val_accuracy: 0.9960 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 217/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9979 - mean_io_u: 0.3501\n","Epoch 217: val_loss did not improve from 0.10239\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1044 - accuracy: 0.9979 - mean_io_u: 0.3501 - val_loss: 0.1025 - val_accuracy: 0.9963 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 218/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9981 - mean_io_u: 0.3501\n","Epoch 218: val_loss did not improve from 0.10239\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1037 - accuracy: 0.9981 - mean_io_u: 0.3501 - val_loss: 0.1030 - val_accuracy: 0.9963 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 219/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9979 - mean_io_u: 0.3501\n","Epoch 219: val_loss did not improve from 0.10239\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1044 - accuracy: 0.9979 - mean_io_u: 0.3501 - val_loss: 0.1025 - val_accuracy: 0.9963 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 220/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9976 - mean_io_u: 0.3501\n","Epoch 220: val_loss improved from 0.10239 to 0.10068, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 502ms/step - loss: 0.1068 - accuracy: 0.9976 - mean_io_u: 0.3501 - val_loss: 0.1007 - val_accuracy: 0.9963 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 221/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9980 - mean_io_u: 0.3501\n","Epoch 221: val_loss did not improve from 0.10068\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1023 - accuracy: 0.9980 - mean_io_u: 0.3501 - val_loss: 0.1010 - val_accuracy: 0.9965 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 222/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9980 - mean_io_u: 0.3501\n","Epoch 222: val_loss did not improve from 0.10068\n","56/56 [==============================] - 27s 489ms/step - loss: 0.1043 - accuracy: 0.9980 - mean_io_u: 0.3501 - val_loss: 0.1008 - val_accuracy: 0.9967 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 223/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9981 - mean_io_u: 0.3501\n","Epoch 223: val_loss improved from 0.10068 to 0.10007, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.1017 - accuracy: 0.9981 - mean_io_u: 0.3501 - val_loss: 0.1001 - val_accuracy: 0.9966 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 224/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9982 - mean_io_u: 0.3501\n","Epoch 224: val_loss improved from 0.10007 to 0.09959, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 492ms/step - loss: 0.1016 - accuracy: 0.9982 - mean_io_u: 0.3501 - val_loss: 0.0996 - val_accuracy: 0.9967 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 225/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9982 - mean_io_u: 0.3501\n","Epoch 225: val_loss improved from 0.09959 to 0.09856, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0988 - accuracy: 0.9982 - mean_io_u: 0.3501 - val_loss: 0.0986 - val_accuracy: 0.9967 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 226/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9982 - mean_io_u: 0.3501\n","Epoch 226: val_loss improved from 0.09856 to 0.09752, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 502ms/step - loss: 0.1026 - accuracy: 0.9982 - mean_io_u: 0.3501 - val_loss: 0.0975 - val_accuracy: 0.9967 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 227/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9983 - mean_io_u: 0.3501\n","Epoch 227: val_loss improved from 0.09752 to 0.09650, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 500ms/step - loss: 0.0989 - accuracy: 0.9983 - mean_io_u: 0.3501 - val_loss: 0.0965 - val_accuracy: 0.9967 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 228/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9982 - mean_io_u: 0.3501\n","Epoch 228: val_loss improved from 0.09650 to 0.09543, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 502ms/step - loss: 0.1011 - accuracy: 0.9982 - mean_io_u: 0.3501 - val_loss: 0.0954 - val_accuracy: 0.9966 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 229/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9982 - mean_io_u: 0.3501\n","Epoch 229: val_loss did not improve from 0.09543\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0973 - accuracy: 0.9982 - mean_io_u: 0.3501 - val_loss: 0.0956 - val_accuracy: 0.9966 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 230/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9982 - mean_io_u: 0.3501\n","Epoch 230: val_loss did not improve from 0.09543\n","56/56 [==============================] - 27s 480ms/step - loss: 0.0988 - accuracy: 0.9982 - mean_io_u: 0.3501 - val_loss: 0.0959 - val_accuracy: 0.9967 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 231/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9982 - mean_io_u: 0.3501\n","Epoch 231: val_loss improved from 0.09543 to 0.09509, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.0984 - accuracy: 0.9982 - mean_io_u: 0.3501 - val_loss: 0.0951 - val_accuracy: 0.9968 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 232/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9983 - mean_io_u: 0.3501\n","Epoch 232: val_loss improved from 0.09509 to 0.09493, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 492ms/step - loss: 0.0962 - accuracy: 0.9983 - mean_io_u: 0.3501 - val_loss: 0.0949 - val_accuracy: 0.9968 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 233/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9983 - mean_io_u: 0.3501\n","Epoch 233: val_loss improved from 0.09493 to 0.09426, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0967 - accuracy: 0.9983 - mean_io_u: 0.3501 - val_loss: 0.0943 - val_accuracy: 0.9968 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 234/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9980 - mean_io_u: 0.3501\n","Epoch 234: val_loss improved from 0.09426 to 0.09281, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 494ms/step - loss: 0.0991 - accuracy: 0.9980 - mean_io_u: 0.3501 - val_loss: 0.0928 - val_accuracy: 0.9951 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 235/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9939 - mean_io_u: 0.3501\n","Epoch 235: val_loss did not improve from 0.09281\n","56/56 [==============================] - 27s 488ms/step - loss: 0.1001 - accuracy: 0.9939 - mean_io_u: 0.3501 - val_loss: 0.0986 - val_accuracy: 0.9886 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 236/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9920 - mean_io_u: 0.3501\n","Epoch 236: val_loss did not improve from 0.09281\n","56/56 [==============================] - 27s 490ms/step - loss: 0.1046 - accuracy: 0.9920 - mean_io_u: 0.3501 - val_loss: 0.1097 - val_accuracy: 0.9870 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 237/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9937 - mean_io_u: 0.3501\n","Epoch 237: val_loss improved from 0.09281 to 0.09129, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.1030 - accuracy: 0.9937 - mean_io_u: 0.3501 - val_loss: 0.0913 - val_accuracy: 0.9942 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 238/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.9963 - mean_io_u: 0.3501\n","Epoch 238: val_loss did not improve from 0.09129\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0974 - accuracy: 0.9963 - mean_io_u: 0.3501 - val_loss: 0.0947 - val_accuracy: 0.9948 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 239/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0939 - accuracy: 0.9973 - mean_io_u: 0.3501\n","Epoch 239: val_loss did not improve from 0.09129\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0939 - accuracy: 0.9973 - mean_io_u: 0.3501 - val_loss: 0.0947 - val_accuracy: 0.9962 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 240/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9978 - mean_io_u: 0.3501\n","Epoch 240: val_loss did not improve from 0.09129\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0919 - accuracy: 0.9978 - mean_io_u: 0.3501 - val_loss: 0.0936 - val_accuracy: 0.9965 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 241/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9979 - mean_io_u: 0.3501\n","Epoch 241: val_loss did not improve from 0.09129\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0930 - accuracy: 0.9979 - mean_io_u: 0.3501 - val_loss: 0.0925 - val_accuracy: 0.9966 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 242/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9979 - mean_io_u: 0.3501\n","Epoch 242: val_loss did not improve from 0.09129\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0923 - accuracy: 0.9979 - mean_io_u: 0.3501 - val_loss: 0.0915 - val_accuracy: 0.9962 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 243/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0932 - accuracy: 0.9980 - mean_io_u: 0.3501\n","Epoch 243: val_loss improved from 0.09129 to 0.09069, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.0932 - accuracy: 0.9980 - mean_io_u: 0.3501 - val_loss: 0.0907 - val_accuracy: 0.9962 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 244/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9981 - mean_io_u: 0.3501\n","Epoch 244: val_loss improved from 0.09069 to 0.09013, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 503ms/step - loss: 0.0920 - accuracy: 0.9981 - mean_io_u: 0.3501 - val_loss: 0.0901 - val_accuracy: 0.9966 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 245/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9981 - mean_io_u: 0.3501\n","Epoch 245: val_loss improved from 0.09013 to 0.08967, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0952 - accuracy: 0.9981 - mean_io_u: 0.3501 - val_loss: 0.0897 - val_accuracy: 0.9966 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 246/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9979 - mean_io_u: 0.3501\n","Epoch 246: val_loss improved from 0.08967 to 0.08929, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 493ms/step - loss: 0.0919 - accuracy: 0.9979 - mean_io_u: 0.3501 - val_loss: 0.0893 - val_accuracy: 0.9962 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 247/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9937 - mean_io_u: 0.3501\n","Epoch 247: val_loss did not improve from 0.08929\n","56/56 [==============================] - 27s 479ms/step - loss: 0.1013 - accuracy: 0.9937 - mean_io_u: 0.3501 - val_loss: 0.0993 - val_accuracy: 0.9870 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 248/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9943 - mean_io_u: 0.3501\n","Epoch 248: val_loss did not improve from 0.08929\n","56/56 [==============================] - 27s 481ms/step - loss: 0.0944 - accuracy: 0.9943 - mean_io_u: 0.3501 - val_loss: 0.0916 - val_accuracy: 0.9929 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 249/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9968 - mean_io_u: 0.3501\n","Epoch 249: val_loss improved from 0.08929 to 0.08928, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0940 - accuracy: 0.9968 - mean_io_u: 0.3501 - val_loss: 0.0893 - val_accuracy: 0.9954 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 250/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9972 - mean_io_u: 0.3501\n","Epoch 250: val_loss improved from 0.08928 to 0.08790, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 504ms/step - loss: 0.0914 - accuracy: 0.9972 - mean_io_u: 0.3501 - val_loss: 0.0879 - val_accuracy: 0.9959 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 251/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0894 - accuracy: 0.9972 - mean_io_u: 0.3501\n","Epoch 251: val_loss did not improve from 0.08790\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0894 - accuracy: 0.9972 - mean_io_u: 0.3501 - val_loss: 0.0885 - val_accuracy: 0.9952 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 252/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9974 - mean_io_u: 0.3501\n","Epoch 252: val_loss improved from 0.08790 to 0.08681, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 494ms/step - loss: 0.0886 - accuracy: 0.9974 - mean_io_u: 0.3501 - val_loss: 0.0868 - val_accuracy: 0.9960 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 253/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9979 - mean_io_u: 0.3501\n","Epoch 253: val_loss improved from 0.08681 to 0.08617, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 502ms/step - loss: 0.0871 - accuracy: 0.9979 - mean_io_u: 0.3501 - val_loss: 0.0862 - val_accuracy: 0.9966 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 254/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9981 - mean_io_u: 0.3501\n","Epoch 254: val_loss improved from 0.08617 to 0.08546, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 492ms/step - loss: 0.0863 - accuracy: 0.9981 - mean_io_u: 0.3501 - val_loss: 0.0855 - val_accuracy: 0.9967 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 255/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9982 - mean_io_u: 0.3501\n","Epoch 255: val_loss improved from 0.08546 to 0.08453, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 504ms/step - loss: 0.0862 - accuracy: 0.9982 - mean_io_u: 0.3501 - val_loss: 0.0845 - val_accuracy: 0.9967 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 256/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9982 - mean_io_u: 0.3501\n","Epoch 256: val_loss did not improve from 0.08453\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0860 - accuracy: 0.9982 - mean_io_u: 0.3501 - val_loss: 0.0848 - val_accuracy: 0.9968 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 257/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9983 - mean_io_u: 0.3501\n","Epoch 257: val_loss improved from 0.08453 to 0.08397, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 492ms/step - loss: 0.0875 - accuracy: 0.9983 - mean_io_u: 0.3501 - val_loss: 0.0840 - val_accuracy: 0.9969 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 258/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9983 - mean_io_u: 0.3501\n","Epoch 258: val_loss improved from 0.08397 to 0.08295, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 493ms/step - loss: 0.0853 - accuracy: 0.9983 - mean_io_u: 0.3501 - val_loss: 0.0829 - val_accuracy: 0.9968 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 259/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9983 - mean_io_u: 0.3501\n","Epoch 259: val_loss improved from 0.08295 to 0.08258, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0861 - accuracy: 0.9983 - mean_io_u: 0.3501 - val_loss: 0.0826 - val_accuracy: 0.9965 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 260/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9982 - mean_io_u: 0.3501\n","Epoch 260: val_loss did not improve from 0.08258\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0896 - accuracy: 0.9982 - mean_io_u: 0.3501 - val_loss: 0.0840 - val_accuracy: 0.9964 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 261/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9982 - mean_io_u: 0.3501\n","Epoch 261: val_loss improved from 0.08258 to 0.08133, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0845 - accuracy: 0.9982 - mean_io_u: 0.3501 - val_loss: 0.0813 - val_accuracy: 0.9966 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 262/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9964 - mean_io_u: 0.3501\n","Epoch 262: val_loss did not improve from 0.08133\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0863 - accuracy: 0.9964 - mean_io_u: 0.3501 - val_loss: 0.0818 - val_accuracy: 0.9954 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 263/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9979 - mean_io_u: 0.3501\n","Epoch 263: val_loss did not improve from 0.08133\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0818 - accuracy: 0.9979 - mean_io_u: 0.3501 - val_loss: 0.0817 - val_accuracy: 0.9965 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 264/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9982 - mean_io_u: 0.3501\n","Epoch 264: val_loss did not improve from 0.08133\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0817 - accuracy: 0.9982 - mean_io_u: 0.3501 - val_loss: 0.0816 - val_accuracy: 0.9966 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 265/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9983 - mean_io_u: 0.3501\n","Epoch 265: val_loss improved from 0.08133 to 0.08036, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.0808 - accuracy: 0.9983 - mean_io_u: 0.3501 - val_loss: 0.0804 - val_accuracy: 0.9968 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 266/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9984 - mean_io_u: 0.3501\n","Epoch 266: val_loss improved from 0.08036 to 0.07986, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 494ms/step - loss: 0.0821 - accuracy: 0.9984 - mean_io_u: 0.3501 - val_loss: 0.0799 - val_accuracy: 0.9968 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 267/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9984 - mean_io_u: 0.3501\n","Epoch 267: val_loss did not improve from 0.07986\n","56/56 [==============================] - 27s 478ms/step - loss: 0.0820 - accuracy: 0.9984 - mean_io_u: 0.3501 - val_loss: 0.0802 - val_accuracy: 0.9961 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 268/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9985 - mean_io_u: 0.3501\n","Epoch 268: val_loss improved from 0.07986 to 0.07875, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 493ms/step - loss: 0.0788 - accuracy: 0.9985 - mean_io_u: 0.3501 - val_loss: 0.0788 - val_accuracy: 0.9969 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 269/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9985 - mean_io_u: 0.3501\n","Epoch 269: val_loss did not improve from 0.07875\n","56/56 [==============================] - 27s 488ms/step - loss: 0.0821 - accuracy: 0.9985 - mean_io_u: 0.3501 - val_loss: 0.0789 - val_accuracy: 0.9969 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 270/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9979 - mean_io_u: 0.3501\n","Epoch 270: val_loss improved from 0.07875 to 0.07784, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 492ms/step - loss: 0.0791 - accuracy: 0.9979 - mean_io_u: 0.3501 - val_loss: 0.0778 - val_accuracy: 0.9959 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 271/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9975 - mean_io_u: 0.3501\n","Epoch 271: val_loss did not improve from 0.07784\n","56/56 [==============================] - 27s 478ms/step - loss: 0.0810 - accuracy: 0.9975 - mean_io_u: 0.3501 - val_loss: 0.0783 - val_accuracy: 0.9944 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 272/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9969 - mean_io_u: 0.3501\n","Epoch 272: val_loss improved from 0.07784 to 0.07381, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.0847 - accuracy: 0.9969 - mean_io_u: 0.3501 - val_loss: 0.0738 - val_accuracy: 0.9942 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 273/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9979 - mean_io_u: 0.3501\n","Epoch 273: val_loss did not improve from 0.07381\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0801 - accuracy: 0.9979 - mean_io_u: 0.3501 - val_loss: 0.0764 - val_accuracy: 0.9965 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 274/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9983 - mean_io_u: 0.3501\n","Epoch 274: val_loss did not improve from 0.07381\n","56/56 [==============================] - 27s 477ms/step - loss: 0.0814 - accuracy: 0.9983 - mean_io_u: 0.3501 - val_loss: 0.0779 - val_accuracy: 0.9968 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 275/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9984 - mean_io_u: 0.3501\n","Epoch 275: val_loss did not improve from 0.07381\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0784 - accuracy: 0.9984 - mean_io_u: 0.3501 - val_loss: 0.0767 - val_accuracy: 0.9969 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 276/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9985 - mean_io_u: 0.3501\n","Epoch 276: val_loss did not improve from 0.07381\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0765 - accuracy: 0.9985 - mean_io_u: 0.3501 - val_loss: 0.0761 - val_accuracy: 0.9970 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 277/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9986 - mean_io_u: 0.3501\n","Epoch 277: val_loss did not improve from 0.07381\n","56/56 [==============================] - 27s 480ms/step - loss: 0.0777 - accuracy: 0.9986 - mean_io_u: 0.3501 - val_loss: 0.0751 - val_accuracy: 0.9970 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 278/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9986 - mean_io_u: 0.3501\n","Epoch 278: val_loss did not improve from 0.07381\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0767 - accuracy: 0.9986 - mean_io_u: 0.3501 - val_loss: 0.0756 - val_accuracy: 0.9970 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 279/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9986 - mean_io_u: 0.3501\n","Epoch 279: val_loss did not improve from 0.07381\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0761 - accuracy: 0.9986 - mean_io_u: 0.3501 - val_loss: 0.0745 - val_accuracy: 0.9971 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 280/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9986 - mean_io_u: 0.3501\n","Epoch 280: val_loss did not improve from 0.07381\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0785 - accuracy: 0.9986 - mean_io_u: 0.3501 - val_loss: 0.0746 - val_accuracy: 0.9971 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 281/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9986 - mean_io_u: 0.3501\n","Epoch 281: val_loss did not improve from 0.07381\n","56/56 [==============================] - 27s 488ms/step - loss: 0.0743 - accuracy: 0.9986 - mean_io_u: 0.3501 - val_loss: 0.0739 - val_accuracy: 0.9971 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 282/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9987 - mean_io_u: 0.3501\n","Epoch 282: val_loss did not improve from 0.07381\n","\n","Epoch 282: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0744 - accuracy: 0.9987 - mean_io_u: 0.3501 - val_loss: 0.0739 - val_accuracy: 0.9969 - val_mean_io_u: 0.3649 - lr: 1.0000e-04\n","Epoch 283/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9987 - mean_io_u: 0.3501\n","Epoch 283: val_loss improved from 0.07381 to 0.07291, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 492ms/step - loss: 0.0733 - accuracy: 0.9987 - mean_io_u: 0.3501 - val_loss: 0.0729 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 284/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9987 - mean_io_u: 0.3501\n","Epoch 284: val_loss improved from 0.07291 to 0.07272, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 502ms/step - loss: 0.0749 - accuracy: 0.9987 - mean_io_u: 0.3501 - val_loss: 0.0727 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 285/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9987 - mean_io_u: 0.3501\n","Epoch 285: val_loss improved from 0.07272 to 0.07266, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 502ms/step - loss: 0.0775 - accuracy: 0.9987 - mean_io_u: 0.3501 - val_loss: 0.0727 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 286/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 286: val_loss improved from 0.07266 to 0.07257, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 502ms/step - loss: 0.0756 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0726 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 287/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 287: val_loss did not improve from 0.07257\n","56/56 [==============================] - 27s 478ms/step - loss: 0.0758 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0728 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 288/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 288: val_loss improved from 0.07257 to 0.07247, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.0725 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0725 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 289/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 289: val_loss improved from 0.07247 to 0.07244, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 502ms/step - loss: 0.0748 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0724 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 290/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9987 - mean_io_u: 0.3501\n","Epoch 290: val_loss improved from 0.07244 to 0.07244, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0754 - accuracy: 0.9987 - mean_io_u: 0.3501 - val_loss: 0.0724 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 291/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 291: val_loss improved from 0.07244 to 0.07223, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0727 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0722 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 292/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9987 - mean_io_u: 0.3501\n","Epoch 292: val_loss did not improve from 0.07223\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0774 - accuracy: 0.9987 - mean_io_u: 0.3501 - val_loss: 0.0725 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 293/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 293: val_loss did not improve from 0.07223\n","56/56 [==============================] - 27s 480ms/step - loss: 0.0749 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0723 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 294/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 294: val_loss did not improve from 0.07223\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0775 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0724 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 295/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 295: val_loss did not improve from 0.07223\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0743 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0724 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 296/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 296: val_loss did not improve from 0.07223\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0752 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0725 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 297/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 297: val_loss improved from 0.07223 to 0.07212, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0732 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0721 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 298/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 298: val_loss improved from 0.07212 to 0.07193, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 493ms/step - loss: 0.0721 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0719 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 299/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 299: val_loss improved from 0.07193 to 0.07182, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 492ms/step - loss: 0.0733 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0718 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 300/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 300: val_loss did not improve from 0.07182\n","56/56 [==============================] - 27s 480ms/step - loss: 0.0737 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0718 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 301/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 301: val_loss did not improve from 0.07182\n","56/56 [==============================] - 27s 480ms/step - loss: 0.0753 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0720 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 302/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 302: val_loss did not improve from 0.07182\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0743 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0718 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 303/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 303: val_loss did not improve from 0.07182\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0762 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0720 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 304/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 304: val_loss did not improve from 0.07182\n","56/56 [==============================] - 27s 488ms/step - loss: 0.0742 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0719 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 305/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 305: val_loss improved from 0.07182 to 0.07154, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 492ms/step - loss: 0.0726 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0715 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 306/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 306: val_loss improved from 0.07154 to 0.07139, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0719 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0714 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 307/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 307: val_loss did not improve from 0.07139\n","56/56 [==============================] - 27s 478ms/step - loss: 0.0747 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0715 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 308/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 308: val_loss improved from 0.07139 to 0.07135, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 492ms/step - loss: 0.0711 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0714 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 309/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 309: val_loss improved from 0.07135 to 0.07128, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 504ms/step - loss: 0.0752 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0713 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 310/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 310: val_loss did not improve from 0.07128\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0741 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0716 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 311/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 311: val_loss did not improve from 0.07128\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0744 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0716 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 312/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 312: val_loss did not improve from 0.07128\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0735 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0715 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 313/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9987 - mean_io_u: 0.3501\n","Epoch 313: val_loss did not improve from 0.07128\n","56/56 [==============================] - 27s 478ms/step - loss: 0.0748 - accuracy: 0.9987 - mean_io_u: 0.3501 - val_loss: 0.0718 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 314/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 314: val_loss improved from 0.07128 to 0.07086, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0732 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0709 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 315/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 315: val_loss did not improve from 0.07086\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0715 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0710 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 316/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 316: val_loss did not improve from 0.07086\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0757 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0712 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 317/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 317: val_loss did not improve from 0.07086\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0733 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0715 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 318/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 318: val_loss did not improve from 0.07086\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0742 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0711 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 319/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 319: val_loss did not improve from 0.07086\n","56/56 [==============================] - 27s 488ms/step - loss: 0.0757 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0712 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 320/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 320: val_loss did not improve from 0.07086\n","56/56 [==============================] - 27s 478ms/step - loss: 0.0731 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0710 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 321/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 321: val_loss did not improve from 0.07086\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0737 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0710 - val_accuracy: 0.9972 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 322/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 322: val_loss did not improve from 0.07086\n","56/56 [==============================] - 27s 480ms/step - loss: 0.0727 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0710 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 323/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 323: val_loss did not improve from 0.07086\n","56/56 [==============================] - 27s 478ms/step - loss: 0.0739 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0712 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 324/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 324: val_loss did not improve from 0.07086\n","\n","Epoch 324: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0765 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0710 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-05\n","Epoch 325/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 325: val_loss did not improve from 0.07086\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0733 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0710 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-06\n","Epoch 326/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 326: val_loss improved from 0.07086 to 0.07075, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0716 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0708 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-06\n","Epoch 327/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 327: val_loss did not improve from 0.07075\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0744 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0708 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-06\n","Epoch 328/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 328: val_loss did not improve from 0.07075\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0742 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0709 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-06\n","Epoch 329/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 329: val_loss did not improve from 0.07075\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0720 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0708 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-06\n","Epoch 330/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 330: val_loss did not improve from 0.07075\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0740 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0708 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-06\n","Epoch 331/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9990 - mean_io_u: 0.3501\n","Epoch 331: val_loss did not improve from 0.07075\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0723 - accuracy: 0.9990 - mean_io_u: 0.3501 - val_loss: 0.0708 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-06\n","Epoch 332/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 332: val_loss did not improve from 0.07075\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0736 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0708 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-06\n","Epoch 333/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 333: val_loss did not improve from 0.07075\n","56/56 [==============================] - 27s 480ms/step - loss: 0.0746 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0709 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-06\n","Epoch 334/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 334: val_loss did not improve from 0.07075\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0742 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0710 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-06\n","Epoch 335/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 335: val_loss did not improve from 0.07075\n","56/56 [==============================] - 27s 480ms/step - loss: 0.0732 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0709 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-06\n","Epoch 336/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 336: val_loss did not improve from 0.07075\n","\n","Epoch 336: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n","56/56 [==============================] - 27s 480ms/step - loss: 0.0732 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0709 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-06\n","Epoch 337/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 337: val_loss did not improve from 0.07075\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0741 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0709 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 338/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9990 - mean_io_u: 0.3501\n","Epoch 338: val_loss improved from 0.07075 to 0.07067, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 27s 491ms/step - loss: 0.0710 - accuracy: 0.9990 - mean_io_u: 0.3501 - val_loss: 0.0707 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 339/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 339: val_loss improved from 0.07067 to 0.07065, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 504ms/step - loss: 0.0728 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0707 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 340/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 340: val_loss did not improve from 0.07065\n","56/56 [==============================] - 27s 480ms/step - loss: 0.0730 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0707 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 341/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 341: val_loss did not improve from 0.07065\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0719 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0707 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 342/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 342: val_loss improved from 0.07065 to 0.07042, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.0702 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0704 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 343/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9990 - mean_io_u: 0.3501\n","Epoch 343: val_loss did not improve from 0.07042\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0716 - accuracy: 0.9990 - mean_io_u: 0.3501 - val_loss: 0.0704 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 344/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9990 - mean_io_u: 0.3501\n","Epoch 344: val_loss improved from 0.07042 to 0.07032, saving model to unet2_hybrid.hdf5\n","56/56 [==============================] - 28s 501ms/step - loss: 0.0703 - accuracy: 0.9990 - mean_io_u: 0.3501 - val_loss: 0.0703 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 345/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 345: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0748 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0706 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 346/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9990 - mean_io_u: 0.3501\n","Epoch 346: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 480ms/step - loss: 0.0736 - accuracy: 0.9990 - mean_io_u: 0.3501 - val_loss: 0.0708 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 347/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 347: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0730 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0708 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 348/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 348: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0734 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0708 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 349/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 349: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0711 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0706 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 350/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9988 - mean_io_u: 0.3501\n","Epoch 350: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 490ms/step - loss: 0.0730 - accuracy: 0.9988 - mean_io_u: 0.3501 - val_loss: 0.0706 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 351/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 351: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0735 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0707 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 352/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9990 - mean_io_u: 0.3501\n","Epoch 352: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0713 - accuracy: 0.9990 - mean_io_u: 0.3501 - val_loss: 0.0706 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 353/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 353: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0741 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0707 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 354/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 354: val_loss did not improve from 0.07032\n","\n","Epoch 354: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n","56/56 [==============================] - 27s 479ms/step - loss: 0.0741 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0709 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-07\n","Epoch 355/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 355: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0732 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0708 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-08\n","Epoch 356/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9990 - mean_io_u: 0.3501\n","Epoch 356: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0720 - accuracy: 0.9990 - mean_io_u: 0.3501 - val_loss: 0.0707 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-08\n","Epoch 357/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 357: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 488ms/step - loss: 0.0745 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0709 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-08\n","Epoch 358/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9989 - mean_io_u: 0.3501\n","Epoch 358: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 481ms/step - loss: 0.0725 - accuracy: 0.9989 - mean_io_u: 0.3501 - val_loss: 0.0708 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-08\n","Epoch 359/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9990 - mean_io_u: 0.3501\n","Epoch 359: val_loss did not improve from 0.07032\n","56/56 [==============================] - 27s 489ms/step - loss: 0.0732 - accuracy: 0.9990 - mean_io_u: 0.3501 - val_loss: 0.0708 - val_accuracy: 0.9973 - val_mean_io_u: 0.3649 - lr: 1.0000e-08\n","Epoch 359: early stopping\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fce6904c7d0>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Train model\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr=ReduceLROnPlateau(monitor='val_loss',\n","                         factor=0.1,\n","                         patience=10,\n","                         verbose=1,\n","                         mode='auto',\n","                         min_delta=0.00003,\n","                         cooldown=0,\n","                         min_lr=0)\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15,verbose=1,mode='min')\n","save_model= ModelCheckpoint('unet2_hybrid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","unet2.fit(images, masks, validation_data=(val_images,val_masks), batch_size=16, epochs=1000,verbose=1,shuffle=True,callbacks=[save_model,reduce_lr,early_stop])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:03:55.359938Z","iopub.status.busy":"2023-04-03T18:03:55.359424Z","iopub.status.idle":"2023-04-03T18:03:55.370023Z","shell.execute_reply":"2023-04-03T18:03:55.368940Z","shell.execute_reply.started":"2023-04-03T18:03:55.359876Z"},"trusted":true},"outputs":[],"source":["np.save('unet2_hybrid-history.npy',unet2.history.history)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:03:58.726919Z","iopub.status.busy":"2023-04-03T18:03:58.726525Z","iopub.status.idle":"2023-04-03T18:03:58.733931Z","shell.execute_reply":"2023-04-03T18:03:58.732771Z","shell.execute_reply.started":"2023-04-03T18:03:58.726884Z"},"trusted":true},"outputs":[],"source":["model_history = np.load('unet2_hybrid-history.npy', allow_pickle='TRUE').item()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:04:01.946985Z","iopub.status.busy":"2023-04-03T18:04:01.946502Z","iopub.status.idle":"2023-04-03T18:04:37.392981Z","shell.execute_reply":"2023-04-03T18:04:37.391894Z","shell.execute_reply.started":"2023-04-03T18:04:01.946949Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<matplotlib.legend.Legend at 0x7fce61867cd0>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#@title Plot accuracy and loss \n","from matplotlib import pyplot as plt\n","## Accuracy\n","plt.plot(model_history['accuracy'])\n","plt.plot(model_history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')\n","plt.show()\n","\n","## Loss\n","plt.plot(model_history['loss'])\n","plt.plot(model_history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:06:35.138071Z","iopub.status.busy":"2023-04-03T18:06:35.136893Z","iopub.status.idle":"2023-04-03T18:07:00.268350Z","shell.execute_reply":"2023-04-03T18:07:00.266775Z","shell.execute_reply.started":"2023-04-03T18:06:35.138008Z"},"trusted":true},"outputs":[],"source":["## Test images\n","test_dir='/kaggle/input/mud-test/mudtest/'\n","test_images_list = os.listdir(r\"{}/images/\".format(test_dir))\n","test_masks_list = []\n","test_images = []\n","for n in test_images_list:\n","  test_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}/images/{}\".format(test_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  test_images.append(a)\n","\n","## Test masks\n","test_masks = []\n","for n in test_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}/labels/{}\".format(test_dir,n))))\n","  test_masks.append(a)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:07:08.894958Z","iopub.status.busy":"2023-04-03T18:07:08.894569Z","iopub.status.idle":"2023-04-03T18:07:09.050122Z","shell.execute_reply":"2023-04-03T18:07:09.048917Z","shell.execute_reply.started":"2023-04-03T18:07:08.894923Z"},"trusted":true},"outputs":[],"source":["for i in range(len(test_images)):\n","  test_images[i] = test_images[i].astype('float32')\n","  test_images[i] = test_images[i].T\n","\n","for i in range(len(test_masks)):\n","  test_masks[i] = test_masks[i].reshape(1,256,256,1)\n","  test_masks[i] = test_masks[i].T\n","for i in range(len(test_images)):\n","  test_images[i] = test_images[i].reshape(-1,256,256,10)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:07:11.863516Z","iopub.status.busy":"2023-04-03T18:07:11.862803Z","iopub.status.idle":"2023-04-03T18:07:11.871710Z","shell.execute_reply":"2023-04-03T18:07:11.870470Z","shell.execute_reply.started":"2023-04-03T18:07:11.863479Z"},"trusted":true},"outputs":[],"source":["#@title Returns an image or array plot of mask prediction\n","\n","def reconstruct_image(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","  # Standardise between 0-1\n","  reconstruction = reconstruction/np.max(reconstruction)\n","\n","  # Round to 0-1, binary pixel-by-pixel classification \n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  # Plot reconstructed mask (prediction)\n","  plt.imshow(reconstruction) \n","'''\n","  Returns array of mask prediction, given model and image\n","'''\n","def reconstruct_array(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","\n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  return reconstruction # Returns array"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:07:15.252966Z","iopub.status.busy":"2023-04-03T18:07:15.251780Z","iopub.status.idle":"2023-04-03T18:07:15.272955Z","shell.execute_reply":"2023-04-03T18:07:15.271603Z","shell.execute_reply.started":"2023-04-03T18:07:15.252917Z"},"trusted":true},"outputs":[],"source":["#@title Metric functions for evaluation\n","def accuracy_eval(model, image, mask): # Gives score of mask vs prediction\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","    return accuracy_score(mask.flatten(), reconstruction)\n","\n","  else: # If a list of images input, find accuracy for each\n","    accuracy = []\n","    for i in range(len(image)):\n","      reconstruction = model.predict(image[i].reshape(1, 256, 256, 10))\n","      reconstruction = np.round(reconstruction).flatten()\n","      accuracy.append(accuracy_score(mask[i].flatten(), reconstruction))\n","    return accuracy\n","\n","def recall_eval(model, image, mask): # Find recall score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return recall_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    recall = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        recall.append(recall_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return recall\n","\n","def precision_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return precision_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    precision = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        precision.append(precision_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return precision\n","\n","def iou_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return jaccard_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    iou = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        iou.append(jaccard_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return iou\n","\n","def f1_score_eval_basic(precision, recall):\n","    prec = np.mean(precision)\n","    rec = np.mean(recall)\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def produce_mask(image): # Outputs rounded image (binary)\n","  return np.round(image)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:07:19.700211Z","iopub.status.busy":"2023-04-03T18:07:19.699430Z","iopub.status.idle":"2023-04-03T18:09:10.081449Z","shell.execute_reply":"2023-04-03T18:09:10.080297Z","shell.execute_reply.started":"2023-04-03T18:07:19.700169Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 70ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 47ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 57ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 47ms/step\n","1/1 [==============================] - 0s 52ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 49ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 47ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 51ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 47ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 50ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 56ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 51ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 47ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n"]}],"source":["accuracy = (accuracy_eval(unet2, test_images, test_masks))\n","precision = (precision_eval(unet2, test_images, test_masks))\n","recall = (recall_eval(unet2, test_images, test_masks))\n","iou = (iou_eval(unet2, test_images, test_masks))"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:09:46.937559Z","iopub.status.busy":"2023-04-03T18:09:46.936787Z","iopub.status.idle":"2023-04-03T18:09:46.943105Z","shell.execute_reply":"2023-04-03T18:09:46.941908Z","shell.execute_reply.started":"2023-04-03T18:09:46.937520Z"},"trusted":true},"outputs":[],"source":["f1_score = (f1_score_eval_basic(precision, recall))"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:09:50.328528Z","iopub.status.busy":"2023-04-03T18:09:50.328084Z","iopub.status.idle":"2023-04-03T18:09:50.337491Z","shell.execute_reply":"2023-04-03T18:09:50.335993Z","shell.execute_reply.started":"2023-04-03T18:09:50.328476Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model accuracy:  0.9851632942830709 0.016082624548894225\n","model precision:  0.9698011466213361 0.05656972171597591\n","model recall:  0.9679576434728195 0.04963683350201659\n","model F1-score:  0.9688785181309529\n","model iou:  0.939939111630021\n"]}],"source":["\n","# Print score eval results for each model\n","print('model accuracy: ', np.mean(accuracy), np.std(accuracy))\n","# Print precision eval results for each model\n","print('model precision: ', np.mean(precision), np.std(precision))\n","# Print recall eval results for each model\n","print('model recall: ', np.mean(recall), np.std(recall))\n","# Print f1-score eval results for each model\n","print('model F1-score: ', np.mean(f1_score))\n","print('model iou: ', np.mean(iou))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
