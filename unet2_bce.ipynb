{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-03T18:20:34.916310Z","iopub.status.busy":"2023-04-03T18:20:34.915392Z","iopub.status.idle":"2023-04-03T18:21:11.193562Z","shell.execute_reply":"2023-04-03T18:21:11.192506Z","shell.execute_reply.started":"2023-04-03T18:20:34.916274Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/davej23/image-segmentation-keras.git\n","  Cloning https://github.com/davej23/image-segmentation-keras.git to /tmp/pip-req-build-irqe_u1_\n","  Running command git clone --filter=blob:none --quiet https://github.com/davej23/image-segmentation-keras.git /tmp/pip-req-build-irqe_u1_\n","  Resolved https://github.com/davej23/image-segmentation-keras.git to commit e01b0a8d5859854cd9d259a618829889166439f5\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting rarfile\n","  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n","Collecting segmentation-models\n","  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n","Collecting rioxarray\n","  Downloading rioxarray-0.9.1.tar.gz (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting keras-applications<=1.0.8,>=1.0.7\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting image-classifiers==1.0.0\n","  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n","Collecting efficientnet==1.0.0\n","  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n","Collecting h5py<=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: Keras>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (2.11.0)\n","Collecting imageio==2.5.0\n","  Downloading imageio-2.5.0-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (0.4.0)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.5.4.60)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.64.1)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (9.4.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (1.21.6)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from rioxarray) (23.0)\n","Requirement already satisfied: pyproj>=2.2 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (3.1.0)\n","Requirement already satisfied: rasterio in /opt/conda/lib/python3.7/site-packages (from rioxarray) (1.2.10)\n","Requirement already satisfied: xarray>=0.17 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (0.20.2)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py<=2.10.0->keras-segmentation==0.3.0) (1.16.0)\n","Requirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.8.0)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.7.3)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (3.5.3)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from pyproj>=2.2->rioxarray) (2022.12.7)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.11.4)\n","Requirement already satisfied: pandas>=1.1 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (1.3.5)\n","Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.4.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (59.8.0)\n","Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (0.7.2)\n","Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.4.7)\n","Requirement already satisfied: click-plugins in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.1.1)\n","Requirement already satisfied: affine in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (2.4.0)\n","Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (22.2.0)\n","Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (8.1.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2023.2)\n","Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.11.2)\n","Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio->rioxarray) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->xarray>=0.17->rioxarray) (3.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (4.38.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (0.11.0)\n","Building wheels for collected packages: keras-segmentation, rioxarray\n","  Building wheel for keras-segmentation (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for keras-segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34377 sha256=6ae58e789dd1df0baa6cd7c777485f186f95f2a12e939a807d4f30815731f29b\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-bndhyp_j/wheels/f4/fb/07/8f81ceb3d9fe936f5e4dcd1a64cbc489e42e6e7f9c2f166785\n","  Building wheel for rioxarray (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rioxarray: filename=rioxarray-0.9.1-py3-none-any.whl size=54590 sha256=125dd23bfa978cac740fb4ba1de94b6672adc4395cd0324cf9e6fc9018115987\n","  Stored in directory: /root/.cache/pip/wheels/03/b2/26/2e2cc1797ac99cc070d2cae87c340bd3429bbb583c90b1c780\n","Successfully built keras-segmentation rioxarray\n","Installing collected packages: rarfile, imageio, h5py, keras-applications, image-classifiers, efficientnet, segmentation-models, keras-segmentation, rioxarray\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.25.0\n","    Uninstalling imageio-2.25.0:\n","      Successfully uninstalled imageio-2.25.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n","tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed efficientnet-1.0.0 h5py-2.10.0 image-classifiers-1.0.0 imageio-2.5.0 keras-applications-1.0.8 keras-segmentation-0.3.0 rarfile-4.0 rioxarray-0.9.1 segmentation-models-1.0.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#@title import packages\n","import keras\n","import numpy as np\n","import os\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.losses import *\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint\n","import sys\n","\n","!pip install rarfile segmentation-models git+https://github.com/davej23/image-segmentation-keras.git rioxarray\n","from rarfile import RarFile\n","from sklearn.metrics import *\n","import rioxarray as rxr"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:21:11.195909Z","iopub.status.busy":"2023-04-03T18:21:11.195522Z","iopub.status.idle":"2023-04-03T18:22:49.889534Z","shell.execute_reply":"2023-04-03T18:22:49.888477Z","shell.execute_reply.started":"2023-04-03T18:21:11.195871Z"},"trusted":true},"outputs":[],"source":["base_dir = r\"/content/gdrive/MyDrive/mudtrain/\"\n","#@title Read training images and normalise\n","training_images_list = os.listdir(r\"{}train/images/\".format(base_dir))\n","training_masks_list = []\n","training_images = []\n","for n in training_images_list:\n","  training_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}train/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  training_images.append(a)\n","\n","## Training masks\n","training_masks = []\n","for n in training_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}train/labels/{}\".format(base_dir,n))))\n","  training_masks.append(a)\n","\n","\n","## Validation images\n","validation_images_list = os.listdir(r\"{}val/images/\".format(base_dir))\n","validation_masks_list = []\n","validation_images = []\n","for n in validation_images_list:\n","  validation_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}val/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  validation_images.append(a)\n","\n","## Validation masks\n","validation_masks = []\n","for n in validation_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}val/labels/{}\".format(base_dir,n))))\n","  validation_masks.append(a)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:22:54.977457Z","iopub.status.busy":"2023-04-03T18:22:54.976397Z","iopub.status.idle":"2023-04-03T18:22:55.609994Z","shell.execute_reply":"2023-04-03T18:22:55.608774Z","shell.execute_reply.started":"2023-04-03T18:22:54.977399Z"},"trusted":true},"outputs":[],"source":["#@title Pre-process data, reshaping and transposing\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].astype('float32')\n","  training_images[i] = training_images[i].T\n","\n","for i in range(len(training_masks)):\n","  training_masks[i] = training_masks[i].reshape(1,256,256)\n","  training_masks[i] = training_masks[i].T\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].astype('float32')\n","  validation_images[i] = validation_images[i].T\n","\n","for i in range(len(validation_masks)):\n","  validation_masks[i] = validation_masks[i].reshape(1,256,256)\n","  validation_masks[i] = validation_masks[i].T\n","\n","\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].reshape(256,256,10)\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].reshape(256,256,10)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:22:58.879031Z","iopub.status.busy":"2023-04-03T18:22:58.877959Z","iopub.status.idle":"2023-04-03T18:23:03.965933Z","shell.execute_reply":"2023-04-03T18:23:03.964753Z","shell.execute_reply.started":"2023-04-03T18:22:58.878991Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 10)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["images=np.vstack([training_images])\n","val_images=np.vstack([validation_images])\n","images.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:23:03.968558Z","iopub.status.busy":"2023-04-03T18:23:03.968022Z","iopub.status.idle":"2023-04-03T18:23:04.127334Z","shell.execute_reply":"2023-04-03T18:23:04.126198Z","shell.execute_reply.started":"2023-04-03T18:23:03.968513Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 1)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["masks=np.vstack([training_masks])\n","val_masks=np.vstack([validation_masks])\n","val_masks.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:23:06.712263Z","iopub.status.busy":"2023-04-03T18:23:06.711769Z","iopub.status.idle":"2023-04-03T18:23:06.954831Z","shell.execute_reply":"2023-04-03T18:23:06.953641Z","shell.execute_reply.started":"2023-04-03T18:23:06.712223Z"},"trusted":true},"outputs":[{"data":{"text/plain":["904"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del training_images,validation_images,training_masks,validation_masks,training_images_list,validation_images_list,\n","training_masks_list,validation_masks_list\n","gc.collect()"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T22:16:22.246886Z","iopub.status.busy":"2023-04-03T22:16:22.245847Z","iopub.status.idle":"2023-04-03T22:16:22.260283Z","shell.execute_reply":"2023-04-03T22:16:22.259179Z","shell.execute_reply.started":"2023-04-03T22:16:22.246845Z"},"trusted":true},"outputs":[],"source":["del images,masks,val_images,val_masks"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:05:33.160617Z","iopub.status.busy":"2023-04-03T15:05:33.159909Z","iopub.status.idle":"2023-04-03T15:05:33.172078Z","shell.execute_reply":"2023-04-03T15:05:33.170984Z","shell.execute_reply.started":"2023-04-03T15:05:33.160577Z"},"trusted":true},"outputs":[],"source":["#@title boundary loss\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import models\n","\n","#Shape of semantic segmentation mask\n","OUTPUT_SHAPE = (256, 256, 1)\n","def boundary_loss(y_true, y_pred):\n","\n","    \"\"\"\n","    Paper Implemented : https://arxiv.org/abs/1905.07852\n","    Using Binary Segmentation mask, generates boundary mask on fly and claculates boundary loss.\n","    :param y_true:\n","    :param y_pred:\n","    :return:\n","    \"\"\"\n","    y_true=tf.cast(y_true,tf.float32)\n","    y_pred=tf.cast(y_pred,tf.float32)\n","    \n","    y_pred_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd = y_pred_bd - (1 - y_pred)\n","    y_true_bd = y_true_bd - (1 - y_true)\n","\n","    y_pred_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd_ext = y_pred_bd_ext - (1 - y_pred)\n","    y_true_bd_ext = y_true_bd_ext - (1 - y_true)\n","\n","    P = K.sum(y_pred_bd * y_true_bd_ext) / K.sum(y_pred_bd) + 1e-7\n","    R = K.sum(y_true_bd * y_pred_bd_ext) / K.sum(y_true_bd) + 1e-7\n","    F1_Score = 2 * P * R / (P + R + 1e-7)\n","    loss = K.mean(1 - F1_Score)\n","    \n","    return loss"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:05:36.594846Z","iopub.status.busy":"2023-04-03T15:05:36.594426Z","iopub.status.idle":"2023-04-03T15:05:39.005124Z","shell.execute_reply":"2023-04-03T15:05:39.004051Z","shell.execute_reply.started":"2023-04-03T15:05:36.594812Z"},"trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, Callback\n","class AlphaScheduler(Callback):\n","  def init(self, alpha, update_fn):\n","    self.alpha = alpha\n","    self.update_fn = update_fn\n","  def on_epoch_end(self, epoch, logs=None):\n","    updated_alpha = self.update_fn(K.get_value(self.alpha))\n","\n","alpha = K.variable(1, dtype='float32')\n","\n","def update_alpha(value):\n","  return np.clip(value - 0.005, 0.005, 1)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:05:40.503351Z","iopub.status.busy":"2023-04-03T15:05:40.502651Z","iopub.status.idle":"2023-04-03T15:05:40.509616Z","shell.execute_reply":"2023-04-03T15:05:40.508337Z","shell.execute_reply.started":"2023-04-03T15:05:40.503312Z"},"trusted":true},"outputs":[],"source":["def gl_sl_wrapper(alpha):\n","    def gl_sl(y_true, y_pred):\n","        return alpha*keras.losses.binary_crossentropy(y_true, y_pred) +  (1-alpha)* boundary_loss(y_true, y_pred)\n","    return gl_sl"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:25:03.901350Z","iopub.status.busy":"2023-04-03T18:25:03.900905Z","iopub.status.idle":"2023-04-03T18:25:03.928614Z","shell.execute_reply":"2023-04-03T18:25:03.927490Z","shell.execute_reply.started":"2023-04-03T18:25:03.901311Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import models, layers, regularizers\n","from tensorflow.keras import backend as K\n","from keras.layers import concatenate, Conv2DTranspose, Activation\n","from keras.layers import BatchNormalization\n","from keras.layers import Conv2D, Input, AvgPool2D\n","\n","#convolutional block\n","def conv_block(x, kernelsize, filters, dropout, batchnorm=True): \n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    if dropout > 0:\n","        conv = layers.Dropout(dropout)(conv)\n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(conv)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    return conv\n","\"\"\"\n","Standard UNet++ [Zhou et.al, 2018]\n","Total params: 9,041,601\n","\"\"\"\n","def unet2plus(input_shape,dropout=0, batchnorm=True):\n","    \n","    filters = [32,64, 128, 256,512]\n","    kernelsize = 3\n","    inputs = Input(input_shape, name='main_input')\n","    \n","    conv1_1 = conv_block(inputs, kernelsize, filters[0],dropout,batchnorm)\n","    pool1 = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)\n","\n","    conv2_1=conv_block(pool1, kernelsize, filters[1],dropout,batchnorm)\n","    pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n","\n","    up1_2 = Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)\n","    conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=3)\n","    conv1_2=conv_block(conv1_2, kernelsize, filters[0],dropout,batchnorm)\n","\n","\n","    conv3_1=conv_block(pool2, kernelsize, filters[2],dropout,batchnorm)\n","    pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n","\n","    up2_2 = Conv2DTranspose(filters[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)\n","    conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=3)\n","    conv2_2=conv_block(conv2_2, kernelsize, filters[1],dropout,batchnorm)\n","\n","    up1_3 = Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)\n","    conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=3)\n","    conv1_3=conv_block(conv1_3, kernelsize, filters[0],dropout,batchnorm)\n","\n","    conv4_1=conv_block(pool3, kernelsize, filters[3],dropout,batchnorm)\n","    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n","\n","    up3_2 = Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)\n","    conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=3)\n","    conv3_2=conv_block(conv3_2, kernelsize, filters[2],dropout,batchnorm)\n","\n","    up2_3 = Conv2DTranspose(filters[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)\n","    conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=3)\n","    conv2_3=conv_block(conv2_3, kernelsize, filters[1],dropout,batchnorm)\n","\n","    up1_4 = Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)\n","    conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=3)\n","    conv1_4=conv_block(conv1_4, kernelsize, filters[0],dropout,batchnorm)\n","\n","    conv5_1=conv_block(pool4, kernelsize, filters[4],dropout,batchnorm)\n","    up4_2 = Conv2DTranspose(filters[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)\n","    conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=3)\n","    conv4_2=conv_block(conv4_2, kernelsize, filters[3],dropout,batchnorm)\n","\n","    up3_3 = Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)\n","    conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=3)\n","    conv3_3=conv_block(conv3_3, kernelsize, filters[2],dropout,batchnorm)\n","\n","    up2_4 = Conv2DTranspose(filters[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)\n","    conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=3)\n","    conv2_4=conv_block(conv2_4, kernelsize, filters[1],dropout,batchnorm)\n","\n","    up1_5 = Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)\n","    conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=3)\n","    conv1_5=conv_block(conv1_5, kernelsize, filters[0],dropout,batchnorm)\n","\n","    conv_final = layers.Conv2D(1, kernel_size=(1,1))(conv1_5)\n","    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n","    outputs = layers.Activation('sigmoid')(conv_final)  \n","    \n","    model = models.Model(inputs=[inputs], outputs=[outputs]) \n","\n","    return model"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:25:08.515205Z","iopub.status.busy":"2023-04-03T18:25:08.514267Z","iopub.status.idle":"2023-04-03T18:25:09.659970Z","shell.execute_reply":"2023-04-03T18:25:09.659166Z","shell.execute_reply.started":"2023-04-03T18:25:08.515152Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," main_input (InputLayer)        [(None, 256, 256, 1  0           []                               \n","                                0)]                                                               \n","                                                                                                  \n"," conv2d_31 (Conv2D)             (None, 256, 256, 32  2912        ['main_input[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_31 (BatchN  (None, 256, 256, 32  128        ['conv2d_31[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_31 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_31[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_32 (Conv2D)             (None, 256, 256, 32  9248        ['activation_31[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_32 (BatchN  (None, 256, 256, 32  128        ['conv2d_32[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_32 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_32[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," pool1 (MaxPooling2D)           (None, 128, 128, 32  0           ['activation_32[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_33 (Conv2D)             (None, 128, 128, 64  18496       ['pool1[0][0]']                  \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_33 (BatchN  (None, 128, 128, 64  256        ['conv2d_33[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_33 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_33[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_34 (Conv2D)             (None, 128, 128, 64  36928       ['activation_33[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_34 (BatchN  (None, 128, 128, 64  256        ['conv2d_34[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_34 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_34[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," pool2 (MaxPooling2D)           (None, 64, 64, 64)   0           ['activation_34[0][0]']          \n","                                                                                                  \n"," conv2d_37 (Conv2D)             (None, 64, 64, 128)  73856       ['pool2[0][0]']                  \n","                                                                                                  \n"," batch_normalization_37 (BatchN  (None, 64, 64, 128)  512        ['conv2d_37[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_37 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_37[0][0]'] \n","                                                                                                  \n"," conv2d_38 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_37[0][0]']          \n","                                                                                                  \n"," batch_normalization_38 (BatchN  (None, 64, 64, 128)  512        ['conv2d_38[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_38 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_38[0][0]'] \n","                                                                                                  \n"," pool3 (MaxPooling2D)           (None, 32, 32, 128)  0           ['activation_38[0][0]']          \n","                                                                                                  \n"," conv2d_43 (Conv2D)             (None, 32, 32, 256)  295168      ['pool3[0][0]']                  \n","                                                                                                  \n"," batch_normalization_43 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_43[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_43 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_43[0][0]'] \n","                                                                                                  \n"," conv2d_44 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_43[0][0]']          \n","                                                                                                  \n"," batch_normalization_44 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_44[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_44 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_44[0][0]'] \n","                                                                                                  \n"," pool4 (MaxPooling2D)           (None, 16, 16, 256)  0           ['activation_44[0][0]']          \n","                                                                                                  \n"," conv2d_51 (Conv2D)             (None, 16, 16, 512)  1180160     ['pool4[0][0]']                  \n","                                                                                                  \n"," batch_normalization_51 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_51[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_51 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_51[0][0]'] \n","                                                                                                  \n"," conv2d_52 (Conv2D)             (None, 16, 16, 512)  2359808     ['activation_51[0][0]']          \n","                                                                                                  \n"," batch_normalization_52 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_52[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_52 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_52[0][0]'] \n","                                                                                                  \n"," up42 (Conv2DTranspose)         (None, 32, 32, 256)  524544      ['activation_52[0][0]']          \n","                                                                                                  \n"," merge42 (Concatenate)          (None, 32, 32, 512)  0           ['up42[0][0]',                   \n","                                                                  'activation_44[0][0]']          \n","                                                                                                  \n"," up32 (Conv2DTranspose)         (None, 64, 64, 128)  131200      ['activation_44[0][0]']          \n","                                                                                                  \n"," conv2d_53 (Conv2D)             (None, 32, 32, 256)  1179904     ['merge42[0][0]']                \n","                                                                                                  \n"," merge32 (Concatenate)          (None, 64, 64, 256)  0           ['up32[0][0]',                   \n","                                                                  'activation_38[0][0]']          \n","                                                                                                  \n"," up22 (Conv2DTranspose)         (None, 128, 128, 64  32832       ['activation_38[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_53 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_53[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_45 (Conv2D)             (None, 64, 64, 128)  295040      ['merge32[0][0]']                \n","                                                                                                  \n"," merge22 (Concatenate)          (None, 128, 128, 12  0           ['up22[0][0]',                   \n","                                8)                                'activation_34[0][0]']          \n","                                                                                                  \n"," up12 (Conv2DTranspose)         (None, 256, 256, 32  8224        ['activation_34[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_53 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_53[0][0]'] \n","                                                                                                  \n"," batch_normalization_45 (BatchN  (None, 64, 64, 128)  512        ['conv2d_45[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_39 (Conv2D)             (None, 128, 128, 64  73792       ['merge22[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," merge12 (Concatenate)          (None, 256, 256, 64  0           ['up12[0][0]',                   \n","                                )                                 'activation_32[0][0]']          \n","                                                                                                  \n"," conv2d_54 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_53[0][0]']          \n","                                                                                                  \n"," activation_45 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_45[0][0]'] \n","                                                                                                  \n"," batch_normalization_39 (BatchN  (None, 128, 128, 64  256        ['conv2d_39[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_35 (Conv2D)             (None, 256, 256, 32  18464       ['merge12[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_54 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_54[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_46 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_45[0][0]']          \n","                                                                                                  \n"," activation_39 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_39[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_35 (BatchN  (None, 256, 256, 32  128        ['conv2d_35[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_54 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_54[0][0]'] \n","                                                                                                  \n"," batch_normalization_46 (BatchN  (None, 64, 64, 128)  512        ['conv2d_46[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_40 (Conv2D)             (None, 128, 128, 64  36928       ['activation_39[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_35 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_35[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," up33 (Conv2DTranspose)         (None, 64, 64, 128)  131200      ['activation_54[0][0]']          \n","                                                                                                  \n"," activation_46 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_46[0][0]'] \n","                                                                                                  \n"," batch_normalization_40 (BatchN  (None, 128, 128, 64  256        ['conv2d_40[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_36 (Conv2D)             (None, 256, 256, 32  9248        ['activation_35[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," merge33 (Concatenate)          (None, 64, 64, 384)  0           ['up33[0][0]',                   \n","                                                                  'activation_38[0][0]',          \n","                                                                  'activation_46[0][0]']          \n","                                                                                                  \n"," activation_40 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_40[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," up23 (Conv2DTranspose)         (None, 128, 128, 64  32832       ['activation_46[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_36 (BatchN  (None, 256, 256, 32  128        ['conv2d_36[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_55 (Conv2D)             (None, 64, 64, 128)  442496      ['merge33[0][0]']                \n","                                                                                                  \n"," merge23 (Concatenate)          (None, 128, 128, 19  0           ['up23[0][0]',                   \n","                                2)                                'activation_34[0][0]',          \n","                                                                  'activation_40[0][0]']          \n","                                                                                                  \n"," activation_36 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_36[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," up13 (Conv2DTranspose)         (None, 256, 256, 32  8224        ['activation_40[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_55 (BatchN  (None, 64, 64, 128)  512        ['conv2d_55[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_47 (Conv2D)             (None, 128, 128, 64  110656      ['merge23[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," merge13 (Concatenate)          (None, 256, 256, 96  0           ['up13[0][0]',                   \n","                                )                                 'activation_32[0][0]',          \n","                                                                  'activation_36[0][0]']          \n","                                                                                                  \n"," activation_55 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_55[0][0]'] \n","                                                                                                  \n"," batch_normalization_47 (BatchN  (None, 128, 128, 64  256        ['conv2d_47[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_41 (Conv2D)             (None, 256, 256, 32  27680       ['merge13[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_56 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_55[0][0]']          \n","                                                                                                  \n"," activation_47 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_47[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_41 (BatchN  (None, 256, 256, 32  128        ['conv2d_41[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_56 (BatchN  (None, 64, 64, 128)  512        ['conv2d_56[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_48 (Conv2D)             (None, 128, 128, 64  36928       ['activation_47[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_41 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_41[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_56 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_56[0][0]'] \n","                                                                                                  \n"," batch_normalization_48 (BatchN  (None, 128, 128, 64  256        ['conv2d_48[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_42 (Conv2D)             (None, 256, 256, 32  9248        ['activation_41[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," up24 (Conv2DTranspose)         (None, 128, 128, 64  32832       ['activation_56[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_48 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_48[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_42 (BatchN  (None, 256, 256, 32  128        ['conv2d_42[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," merge24 (Concatenate)          (None, 128, 128, 25  0           ['up24[0][0]',                   \n","                                6)                                'activation_34[0][0]',          \n","                                                                  'activation_40[0][0]',          \n","                                                                  'activation_48[0][0]']          \n","                                                                                                  \n"," activation_42 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_42[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," up14 (Conv2DTranspose)         (None, 256, 256, 32  8224        ['activation_48[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_57 (Conv2D)             (None, 128, 128, 64  147520      ['merge24[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," merge14 (Concatenate)          (None, 256, 256, 12  0           ['up14[0][0]',                   \n","                                8)                                'activation_32[0][0]',          \n","                                                                  'activation_36[0][0]',          \n","                                                                  'activation_42[0][0]']          \n","                                                                                                  \n"," batch_normalization_57 (BatchN  (None, 128, 128, 64  256        ['conv2d_57[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_49 (Conv2D)             (None, 256, 256, 32  36896       ['merge14[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," activation_57 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_57[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_49 (BatchN  (None, 256, 256, 32  128        ['conv2d_49[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_58 (Conv2D)             (None, 128, 128, 64  36928       ['activation_57[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_49 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_49[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_58 (BatchN  (None, 128, 128, 64  256        ['conv2d_58[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," conv2d_50 (Conv2D)             (None, 256, 256, 32  9248        ['activation_49[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_58 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_58[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_50 (BatchN  (None, 256, 256, 32  128        ['conv2d_50[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," up15 (Conv2DTranspose)         (None, 256, 256, 32  8224        ['activation_58[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_50 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_50[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," merge15 (Concatenate)          (None, 256, 256, 16  0           ['up15[0][0]',                   \n","                                0)                                'activation_32[0][0]',          \n","                                                                  'activation_36[0][0]',          \n","                                                                  'activation_42[0][0]',          \n","                                                                  'activation_50[0][0]']          \n","                                                                                                  \n"," conv2d_59 (Conv2D)             (None, 256, 256, 32  46112       ['merge15[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_59 (BatchN  (None, 256, 256, 32  128        ['conv2d_59[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_59 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_59[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_60 (Conv2D)             (None, 256, 256, 32  9248        ['activation_59[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_60 (BatchN  (None, 256, 256, 32  128        ['conv2d_60[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_60 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_60[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_61 (Conv2D)             (None, 256, 256, 1)  33          ['activation_60[0][0]']          \n","                                                                                                  \n"," batch_normalization_61 (BatchN  (None, 256, 256, 1)  4          ['conv2d_61[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_61 (Activation)     (None, 256, 256, 1)  0           ['batch_normalization_61[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 9,058,789\n","Trainable params: 9,051,491\n","Non-trainable params: 7,298\n","__________________________________________________________________________________________________\n"]}],"source":["from keras import metrics\n","unet2= unet2plus(input_shape=(256,256,10))#binary_crossentropy\n","unet2.compile(optimizer = adam_v2.Adam(learning_rate = 1e-4), loss =binary_crossentropy, metrics = ['accuracy'])\n","unet2.summary()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T18:25:22.052268Z","iopub.status.busy":"2023-04-03T18:25:22.051768Z","iopub.status.idle":"2023-04-03T22:15:32.312104Z","shell.execute_reply":"2023-04-03T22:15:32.311092Z","shell.execute_reply.started":"2023-04-03T18:25:22.052229Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.4543 - accuracy: 0.8862\n","Epoch 1: val_loss improved from inf to 0.61509, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 46s 612ms/step - loss: 0.4543 - accuracy: 0.8862 - val_loss: 0.6151 - val_accuracy: 0.6696 - lr: 1.0000e-04\n","Epoch 2/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3929 - accuracy: 0.9496\n","Epoch 2: val_loss improved from 0.61509 to 0.56934, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.3929 - accuracy: 0.9496 - val_loss: 0.5693 - val_accuracy: 0.7888 - lr: 1.0000e-04\n","Epoch 3/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3886 - accuracy: 0.9560\n","Epoch 3: val_loss improved from 0.56934 to 0.53044, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.3886 - accuracy: 0.9560 - val_loss: 0.5304 - val_accuracy: 0.8155 - lr: 1.0000e-04\n","Epoch 4/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3817 - accuracy: 0.9616\n","Epoch 4: val_loss improved from 0.53044 to 0.48890, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.3817 - accuracy: 0.9616 - val_loss: 0.4889 - val_accuracy: 0.8741 - lr: 1.0000e-04\n","Epoch 5/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3752 - accuracy: 0.9650\n","Epoch 5: val_loss improved from 0.48890 to 0.42021, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 470ms/step - loss: 0.3752 - accuracy: 0.9650 - val_loss: 0.4202 - val_accuracy: 0.9309 - lr: 1.0000e-04\n","Epoch 6/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3690 - accuracy: 0.9696\n","Epoch 6: val_loss did not improve from 0.42021\n","56/56 [==============================] - 26s 472ms/step - loss: 0.3690 - accuracy: 0.9696 - val_loss: 0.4686 - val_accuracy: 0.8941 - lr: 1.0000e-04\n","Epoch 7/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3657 - accuracy: 0.9721\n","Epoch 7: val_loss improved from 0.42021 to 0.37067, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 485ms/step - loss: 0.3657 - accuracy: 0.9721 - val_loss: 0.3707 - val_accuracy: 0.9720 - lr: 1.0000e-04\n","Epoch 8/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3620 - accuracy: 0.9742\n","Epoch 8: val_loss improved from 0.37067 to 0.36186, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.3620 - accuracy: 0.9742 - val_loss: 0.3619 - val_accuracy: 0.9786 - lr: 1.0000e-04\n","Epoch 9/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3593 - accuracy: 0.9733\n","Epoch 9: val_loss did not improve from 0.36186\n","56/56 [==============================] - 26s 471ms/step - loss: 0.3593 - accuracy: 0.9733 - val_loss: 0.3931 - val_accuracy: 0.9326 - lr: 1.0000e-04\n","Epoch 10/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3608 - accuracy: 0.9733\n","Epoch 10: val_loss improved from 0.36186 to 0.35731, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.3608 - accuracy: 0.9733 - val_loss: 0.3573 - val_accuracy: 0.9827 - lr: 1.0000e-04\n","Epoch 11/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3535 - accuracy: 0.9754\n","Epoch 11: val_loss improved from 0.35731 to 0.34727, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 470ms/step - loss: 0.3535 - accuracy: 0.9754 - val_loss: 0.3473 - val_accuracy: 0.9837 - lr: 1.0000e-04\n","Epoch 12/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3479 - accuracy: 0.9812\n","Epoch 12: val_loss improved from 0.34727 to 0.34326, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.3479 - accuracy: 0.9812 - val_loss: 0.3433 - val_accuracy: 0.9854 - lr: 1.0000e-04\n","Epoch 13/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3430 - accuracy: 0.9839\n","Epoch 13: val_loss did not improve from 0.34326\n","56/56 [==============================] - 26s 461ms/step - loss: 0.3430 - accuracy: 0.9839 - val_loss: 0.3450 - val_accuracy: 0.9847 - lr: 1.0000e-04\n","Epoch 14/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3421 - accuracy: 0.9816\n","Epoch 14: val_loss improved from 0.34326 to 0.33856, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.3421 - accuracy: 0.9816 - val_loss: 0.3386 - val_accuracy: 0.9868 - lr: 1.0000e-04\n","Epoch 15/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3388 - accuracy: 0.9835\n","Epoch 15: val_loss did not improve from 0.33856\n","56/56 [==============================] - 26s 473ms/step - loss: 0.3388 - accuracy: 0.9835 - val_loss: 0.3423 - val_accuracy: 0.9825 - lr: 1.0000e-04\n","Epoch 16/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.9803\n","Epoch 16: val_loss improved from 0.33856 to 0.33435, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 475ms/step - loss: 0.3402 - accuracy: 0.9803 - val_loss: 0.3344 - val_accuracy: 0.9867 - lr: 1.0000e-04\n","Epoch 17/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3380 - accuracy: 0.9811\n","Epoch 17: val_loss improved from 0.33435 to 0.32719, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.3380 - accuracy: 0.9811 - val_loss: 0.3272 - val_accuracy: 0.9850 - lr: 1.0000e-04\n","Epoch 18/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3334 - accuracy: 0.9812\n","Epoch 18: val_loss did not improve from 0.32719\n","56/56 [==============================] - 26s 473ms/step - loss: 0.3334 - accuracy: 0.9812 - val_loss: 0.3313 - val_accuracy: 0.9870 - lr: 1.0000e-04\n","Epoch 19/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3357 - accuracy: 0.9801\n","Epoch 19: val_loss improved from 0.32719 to 0.32581, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.3357 - accuracy: 0.9801 - val_loss: 0.3258 - val_accuracy: 0.9863 - lr: 1.0000e-04\n","Epoch 20/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.9845\n","Epoch 20: val_loss improved from 0.32581 to 0.32420, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.3296 - accuracy: 0.9845 - val_loss: 0.3242 - val_accuracy: 0.9847 - lr: 1.0000e-04\n","Epoch 21/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3256 - accuracy: 0.9859\n","Epoch 21: val_loss improved from 0.32420 to 0.32415, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.3256 - accuracy: 0.9859 - val_loss: 0.3242 - val_accuracy: 0.9873 - lr: 1.0000e-04\n","Epoch 22/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3260 - accuracy: 0.9856\n","Epoch 22: val_loss did not improve from 0.32415\n","56/56 [==============================] - 26s 471ms/step - loss: 0.3260 - accuracy: 0.9856 - val_loss: 0.3252 - val_accuracy: 0.9876 - lr: 1.0000e-04\n","Epoch 23/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3197 - accuracy: 0.9873\n","Epoch 23: val_loss improved from 0.32415 to 0.32189, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.3197 - accuracy: 0.9873 - val_loss: 0.3219 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 24/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.9859\n","Epoch 24: val_loss improved from 0.32189 to 0.32076, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 470ms/step - loss: 0.3207 - accuracy: 0.9859 - val_loss: 0.3208 - val_accuracy: 0.9866 - lr: 1.0000e-04\n","Epoch 25/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3192 - accuracy: 0.9866\n","Epoch 25: val_loss improved from 0.32076 to 0.31658, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.3192 - accuracy: 0.9866 - val_loss: 0.3166 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 26/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3154 - accuracy: 0.9877\n","Epoch 26: val_loss improved from 0.31658 to 0.31436, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.3154 - accuracy: 0.9877 - val_loss: 0.3144 - val_accuracy: 0.9876 - lr: 1.0000e-04\n","Epoch 27/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3157 - accuracy: 0.9857\n","Epoch 27: val_loss improved from 0.31436 to 0.31200, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 485ms/step - loss: 0.3157 - accuracy: 0.9857 - val_loss: 0.3120 - val_accuracy: 0.9877 - lr: 1.0000e-04\n","Epoch 28/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3110 - accuracy: 0.9875\n","Epoch 28: val_loss improved from 0.31200 to 0.31179, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 476ms/step - loss: 0.3110 - accuracy: 0.9875 - val_loss: 0.3118 - val_accuracy: 0.9864 - lr: 1.0000e-04\n","Epoch 29/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.9875\n","Epoch 29: val_loss improved from 0.31179 to 0.30943, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.3100 - accuracy: 0.9875 - val_loss: 0.3094 - val_accuracy: 0.9867 - lr: 1.0000e-04\n","Epoch 30/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.9876\n","Epoch 30: val_loss improved from 0.30943 to 0.30738, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.3088 - accuracy: 0.9876 - val_loss: 0.3074 - val_accuracy: 0.9874 - lr: 1.0000e-04\n","Epoch 31/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3042 - accuracy: 0.9893\n","Epoch 31: val_loss improved from 0.30738 to 0.30432, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.3042 - accuracy: 0.9893 - val_loss: 0.3043 - val_accuracy: 0.9885 - lr: 1.0000e-04\n","Epoch 32/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.9895\n","Epoch 32: val_loss improved from 0.30432 to 0.30219, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.3003 - accuracy: 0.9895 - val_loss: 0.3022 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 33/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.9815\n","Epoch 33: val_loss improved from 0.30219 to 0.29360, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.3108 - accuracy: 0.9815 - val_loss: 0.2936 - val_accuracy: 0.9867 - lr: 1.0000e-04\n","Epoch 34/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2997 - accuracy: 0.9884\n","Epoch 34: val_loss did not improve from 0.29360\n","56/56 [==============================] - 26s 459ms/step - loss: 0.2997 - accuracy: 0.9884 - val_loss: 0.3007 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 35/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.9891\n","Epoch 35: val_loss did not improve from 0.29360\n","56/56 [==============================] - 26s 471ms/step - loss: 0.2973 - accuracy: 0.9891 - val_loss: 0.2962 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 36/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2967 - accuracy: 0.9884\n","Epoch 36: val_loss did not improve from 0.29360\n","56/56 [==============================] - 26s 471ms/step - loss: 0.2967 - accuracy: 0.9884 - val_loss: 0.3117 - val_accuracy: 0.9788 - lr: 1.0000e-04\n","Epoch 37/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2979 - accuracy: 0.9853\n","Epoch 37: val_loss did not improve from 0.29360\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2979 - accuracy: 0.9853 - val_loss: 0.2984 - val_accuracy: 0.9874 - lr: 1.0000e-04\n","Epoch 38/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.9891\n","Epoch 38: val_loss improved from 0.29360 to 0.29033, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.2923 - accuracy: 0.9891 - val_loss: 0.2903 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 39/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.9895\n","Epoch 39: val_loss improved from 0.29033 to 0.29000, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.2897 - accuracy: 0.9895 - val_loss: 0.2900 - val_accuracy: 0.9881 - lr: 1.0000e-04\n","Epoch 40/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.9894\n","Epoch 40: val_loss improved from 0.29000 to 0.28682, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.2885 - accuracy: 0.9894 - val_loss: 0.2868 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 41/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9901\n","Epoch 41: val_loss improved from 0.28682 to 0.28459, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.2858 - accuracy: 0.9901 - val_loss: 0.2846 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 42/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2871 - accuracy: 0.9894\n","Epoch 42: val_loss did not improve from 0.28459\n","56/56 [==============================] - 26s 471ms/step - loss: 0.2871 - accuracy: 0.9894 - val_loss: 0.2888 - val_accuracy: 0.9887 - lr: 1.0000e-04\n","Epoch 43/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9889\n","Epoch 43: val_loss did not improve from 0.28459\n","56/56 [==============================] - 26s 461ms/step - loss: 0.2863 - accuracy: 0.9889 - val_loss: 0.2851 - val_accuracy: 0.9885 - lr: 1.0000e-04\n","Epoch 44/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2823 - accuracy: 0.9900\n","Epoch 44: val_loss improved from 0.28459 to 0.28416, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 482ms/step - loss: 0.2823 - accuracy: 0.9900 - val_loss: 0.2842 - val_accuracy: 0.9893 - lr: 1.0000e-04\n","Epoch 45/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.9901\n","Epoch 45: val_loss improved from 0.28416 to 0.27898, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.2811 - accuracy: 0.9901 - val_loss: 0.2790 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 46/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2781 - accuracy: 0.9907\n","Epoch 46: val_loss did not improve from 0.27898\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2781 - accuracy: 0.9907 - val_loss: 0.2800 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 47/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2773 - accuracy: 0.9900\n","Epoch 47: val_loss improved from 0.27898 to 0.27882, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 485ms/step - loss: 0.2773 - accuracy: 0.9900 - val_loss: 0.2788 - val_accuracy: 0.9886 - lr: 1.0000e-04\n","Epoch 48/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2738 - accuracy: 0.9909\n","Epoch 48: val_loss did not improve from 0.27882\n","56/56 [==============================] - 26s 460ms/step - loss: 0.2738 - accuracy: 0.9909 - val_loss: 0.2802 - val_accuracy: 0.9879 - lr: 1.0000e-04\n","Epoch 49/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2729 - accuracy: 0.9891\n","Epoch 49: val_loss did not improve from 0.27882\n","56/56 [==============================] - 26s 473ms/step - loss: 0.2729 - accuracy: 0.9891 - val_loss: 0.2793 - val_accuracy: 0.9851 - lr: 1.0000e-04\n","Epoch 50/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2718 - accuracy: 0.9899\n","Epoch 50: val_loss improved from 0.27882 to 0.26908, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2718 - accuracy: 0.9899 - val_loss: 0.2691 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 51/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2724 - accuracy: 0.9884\n","Epoch 51: val_loss did not improve from 0.26908\n","56/56 [==============================] - 26s 460ms/step - loss: 0.2724 - accuracy: 0.9884 - val_loss: 0.2733 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 52/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2683 - accuracy: 0.9911\n","Epoch 52: val_loss did not improve from 0.26908\n","56/56 [==============================] - 26s 471ms/step - loss: 0.2683 - accuracy: 0.9911 - val_loss: 0.2726 - val_accuracy: 0.9886 - lr: 1.0000e-04\n","Epoch 53/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.9912\n","Epoch 53: val_loss improved from 0.26908 to 0.26729, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.2651 - accuracy: 0.9912 - val_loss: 0.2673 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 54/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9913\n","Epoch 54: val_loss improved from 0.26729 to 0.26551, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.2657 - accuracy: 0.9913 - val_loss: 0.2655 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 55/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.9903\n","Epoch 55: val_loss did not improve from 0.26551\n","56/56 [==============================] - 26s 462ms/step - loss: 0.2663 - accuracy: 0.9903 - val_loss: 0.2699 - val_accuracy: 0.9873 - lr: 1.0000e-04\n","Epoch 56/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2611 - accuracy: 0.9909\n","Epoch 56: val_loss improved from 0.26551 to 0.26404, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2611 - accuracy: 0.9909 - val_loss: 0.2640 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 57/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9901\n","Epoch 57: val_loss did not improve from 0.26404\n","56/56 [==============================] - 26s 461ms/step - loss: 0.2620 - accuracy: 0.9901 - val_loss: 0.2652 - val_accuracy: 0.9862 - lr: 1.0000e-04\n","Epoch 58/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2619 - accuracy: 0.9898\n","Epoch 58: val_loss improved from 0.26404 to 0.26245, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.2619 - accuracy: 0.9898 - val_loss: 0.2625 - val_accuracy: 0.9891 - lr: 1.0000e-04\n","Epoch 59/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.9907\n","Epoch 59: val_loss improved from 0.26245 to 0.25510, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.2589 - accuracy: 0.9907 - val_loss: 0.2551 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 60/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.9920\n","Epoch 60: val_loss did not improve from 0.25510\n","56/56 [==============================] - 26s 462ms/step - loss: 0.2541 - accuracy: 0.9920 - val_loss: 0.2552 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 61/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.9920\n","Epoch 61: val_loss improved from 0.25510 to 0.25283, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.2535 - accuracy: 0.9920 - val_loss: 0.2528 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 62/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.9925\n","Epoch 62: val_loss improved from 0.25283 to 0.25020, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.2518 - accuracy: 0.9925 - val_loss: 0.2502 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 63/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9919\n","Epoch 63: val_loss improved from 0.25020 to 0.24770, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.2508 - accuracy: 0.9919 - val_loss: 0.2477 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 64/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.9918\n","Epoch 64: val_loss did not improve from 0.24770\n","56/56 [==============================] - 26s 460ms/step - loss: 0.2516 - accuracy: 0.9918 - val_loss: 0.2507 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 65/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.9911\n","Epoch 65: val_loss did not improve from 0.24770\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2484 - accuracy: 0.9911 - val_loss: 0.2493 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 66/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.9919\n","Epoch 66: val_loss improved from 0.24770 to 0.24653, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2462 - accuracy: 0.9919 - val_loss: 0.2465 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 67/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.9922\n","Epoch 67: val_loss improved from 0.24653 to 0.24451, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 485ms/step - loss: 0.2471 - accuracy: 0.9922 - val_loss: 0.2445 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 68/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 0.9923\n","Epoch 68: val_loss improved from 0.24451 to 0.24225, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.2447 - accuracy: 0.9923 - val_loss: 0.2422 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 69/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9927\n","Epoch 69: val_loss improved from 0.24225 to 0.24066, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.2421 - accuracy: 0.9927 - val_loss: 0.2407 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 70/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9927\n","Epoch 70: val_loss did not improve from 0.24066\n","56/56 [==============================] - 26s 460ms/step - loss: 0.2415 - accuracy: 0.9927 - val_loss: 0.2436 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 71/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2428 - accuracy: 0.9901\n","Epoch 71: val_loss did not improve from 0.24066\n","56/56 [==============================] - 26s 460ms/step - loss: 0.2428 - accuracy: 0.9901 - val_loss: 0.2409 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 72/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2390 - accuracy: 0.9912\n","Epoch 72: val_loss improved from 0.24066 to 0.23584, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 476ms/step - loss: 0.2390 - accuracy: 0.9912 - val_loss: 0.2358 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 73/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2367 - accuracy: 0.9922\n","Epoch 73: val_loss improved from 0.23584 to 0.23496, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.2367 - accuracy: 0.9922 - val_loss: 0.2350 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 74/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.9928\n","Epoch 74: val_loss improved from 0.23496 to 0.23464, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.2362 - accuracy: 0.9928 - val_loss: 0.2346 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 75/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2324 - accuracy: 0.9934\n","Epoch 75: val_loss improved from 0.23464 to 0.23293, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.2324 - accuracy: 0.9934 - val_loss: 0.2329 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 76/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2314 - accuracy: 0.9930\n","Epoch 76: val_loss improved from 0.23293 to 0.23219, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.2314 - accuracy: 0.9930 - val_loss: 0.2322 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 77/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.9932\n","Epoch 77: val_loss improved from 0.23219 to 0.23035, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 485ms/step - loss: 0.2301 - accuracy: 0.9932 - val_loss: 0.2304 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 78/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.9927\n","Epoch 78: val_loss did not improve from 0.23035\n","56/56 [==============================] - 26s 459ms/step - loss: 0.2283 - accuracy: 0.9927 - val_loss: 0.2312 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 79/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.9905\n","Epoch 79: val_loss improved from 0.23035 to 0.22632, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2335 - accuracy: 0.9905 - val_loss: 0.2263 - val_accuracy: 0.9891 - lr: 1.0000e-04\n","Epoch 80/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2368 - accuracy: 0.9884\n","Epoch 80: val_loss improved from 0.22632 to 0.22387, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.2368 - accuracy: 0.9884 - val_loss: 0.2239 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 81/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.9899\n","Epoch 81: val_loss did not improve from 0.22387\n","56/56 [==============================] - 26s 473ms/step - loss: 0.2335 - accuracy: 0.9899 - val_loss: 0.2248 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 82/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.9928\n","Epoch 82: val_loss did not improve from 0.22387\n","56/56 [==============================] - 26s 473ms/step - loss: 0.2248 - accuracy: 0.9928 - val_loss: 0.2263 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 83/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.9929\n","Epoch 83: val_loss did not improve from 0.22387\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2248 - accuracy: 0.9929 - val_loss: 0.2258 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 84/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9931\n","Epoch 84: val_loss did not improve from 0.22387\n","56/56 [==============================] - 26s 461ms/step - loss: 0.2226 - accuracy: 0.9931 - val_loss: 0.2245 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 85/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.9935\n","Epoch 85: val_loss improved from 0.22387 to 0.22228, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.2200 - accuracy: 0.9935 - val_loss: 0.2223 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 86/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 0.9935\n","Epoch 86: val_loss improved from 0.22228 to 0.21907, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.2184 - accuracy: 0.9935 - val_loss: 0.2191 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 87/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2186 - accuracy: 0.9938\n","Epoch 87: val_loss improved from 0.21907 to 0.21885, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2186 - accuracy: 0.9938 - val_loss: 0.2189 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 88/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2203 - accuracy: 0.9932\n","Epoch 88: val_loss did not improve from 0.21885\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2203 - accuracy: 0.9932 - val_loss: 0.2240 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 89/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.9879\n","Epoch 89: val_loss improved from 0.21885 to 0.20778, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 486ms/step - loss: 0.2211 - accuracy: 0.9879 - val_loss: 0.2078 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 90/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.9909\n","Epoch 90: val_loss did not improve from 0.20778\n","56/56 [==============================] - 26s 460ms/step - loss: 0.2175 - accuracy: 0.9909 - val_loss: 0.2162 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 91/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9932\n","Epoch 91: val_loss did not improve from 0.20778\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2134 - accuracy: 0.9932 - val_loss: 0.2160 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 92/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9926\n","Epoch 92: val_loss did not improve from 0.20778\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2132 - accuracy: 0.9926 - val_loss: 0.2129 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 93/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2154 - accuracy: 0.9914\n","Epoch 93: val_loss improved from 0.20778 to 0.20697, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.2154 - accuracy: 0.9914 - val_loss: 0.2070 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 94/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9927\n","Epoch 94: val_loss did not improve from 0.20697\n","56/56 [==============================] - 26s 460ms/step - loss: 0.2111 - accuracy: 0.9927 - val_loss: 0.2082 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 95/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2093 - accuracy: 0.9934\n","Epoch 95: val_loss improved from 0.20697 to 0.20674, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.2093 - accuracy: 0.9934 - val_loss: 0.2067 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 96/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9934\n","Epoch 96: val_loss did not improve from 0.20674\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2092 - accuracy: 0.9934 - val_loss: 0.2088 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 97/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2066 - accuracy: 0.9938\n","Epoch 97: val_loss did not improve from 0.20674\n","56/56 [==============================] - 26s 462ms/step - loss: 0.2066 - accuracy: 0.9938 - val_loss: 0.2073 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 98/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9939\n","Epoch 98: val_loss improved from 0.20674 to 0.20601, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.2061 - accuracy: 0.9939 - val_loss: 0.2060 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 99/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.9940\n","Epoch 99: val_loss improved from 0.20601 to 0.20387, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 485ms/step - loss: 0.2028 - accuracy: 0.9940 - val_loss: 0.2039 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 100/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.9936\n","Epoch 100: val_loss improved from 0.20387 to 0.20247, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.2059 - accuracy: 0.9936 - val_loss: 0.2025 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 101/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2062 - accuracy: 0.9922\n","Epoch 101: val_loss improved from 0.20247 to 0.20203, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.2062 - accuracy: 0.9922 - val_loss: 0.2020 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 102/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2025 - accuracy: 0.9934\n","Epoch 102: val_loss did not improve from 0.20203\n","56/56 [==============================] - 26s 461ms/step - loss: 0.2025 - accuracy: 0.9934 - val_loss: 0.2029 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 103/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1993 - accuracy: 0.9940\n","Epoch 103: val_loss improved from 0.20203 to 0.19975, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1993 - accuracy: 0.9940 - val_loss: 0.1997 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 104/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.9944\n","Epoch 104: val_loss improved from 0.19975 to 0.19781, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 485ms/step - loss: 0.1974 - accuracy: 0.9944 - val_loss: 0.1978 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 105/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9945\n","Epoch 105: val_loss improved from 0.19781 to 0.19635, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1941 - accuracy: 0.9945 - val_loss: 0.1963 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 106/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9947\n","Epoch 106: val_loss improved from 0.19635 to 0.19465, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1942 - accuracy: 0.9947 - val_loss: 0.1946 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 107/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1951 - accuracy: 0.9945\n","Epoch 107: val_loss improved from 0.19465 to 0.19316, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1951 - accuracy: 0.9945 - val_loss: 0.1932 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 108/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1951 - accuracy: 0.9947\n","Epoch 108: val_loss improved from 0.19316 to 0.19163, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1951 - accuracy: 0.9947 - val_loss: 0.1916 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 109/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9947\n","Epoch 109: val_loss improved from 0.19163 to 0.19141, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1931 - accuracy: 0.9947 - val_loss: 0.1914 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 110/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.9947\n","Epoch 110: val_loss improved from 0.19141 to 0.18984, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1912 - accuracy: 0.9947 - val_loss: 0.1898 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 111/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9950\n","Epoch 111: val_loss improved from 0.18984 to 0.18899, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1909 - accuracy: 0.9950 - val_loss: 0.1890 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 112/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1907 - accuracy: 0.9950\n","Epoch 112: val_loss improved from 0.18899 to 0.18569, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1907 - accuracy: 0.9950 - val_loss: 0.1857 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 113/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1854 - accuracy: 0.9952\n","Epoch 113: val_loss did not improve from 0.18569\n","56/56 [==============================] - 26s 463ms/step - loss: 0.1854 - accuracy: 0.9952 - val_loss: 0.1867 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 114/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.9951\n","Epoch 114: val_loss did not improve from 0.18569\n","56/56 [==============================] - 26s 462ms/step - loss: 0.1864 - accuracy: 0.9951 - val_loss: 0.1861 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 115/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 0.9943\n","Epoch 115: val_loss did not improve from 0.18569\n","56/56 [==============================] - 26s 460ms/step - loss: 0.1859 - accuracy: 0.9943 - val_loss: 0.1915 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 116/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1855 - accuracy: 0.9942\n","Epoch 116: val_loss improved from 0.18569 to 0.18430, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1855 - accuracy: 0.9942 - val_loss: 0.1843 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 117/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1825 - accuracy: 0.9948\n","Epoch 117: val_loss improved from 0.18430 to 0.18172, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1825 - accuracy: 0.9948 - val_loss: 0.1817 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 118/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9951\n","Epoch 118: val_loss improved from 0.18172 to 0.18097, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1815 - accuracy: 0.9951 - val_loss: 0.1810 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 119/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1817 - accuracy: 0.9947\n","Epoch 119: val_loss did not improve from 0.18097\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1817 - accuracy: 0.9947 - val_loss: 0.1814 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 120/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1795 - accuracy: 0.9949\n","Epoch 120: val_loss improved from 0.18097 to 0.17994, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1795 - accuracy: 0.9949 - val_loss: 0.1799 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 121/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1791 - accuracy: 0.9949\n","Epoch 121: val_loss improved from 0.17994 to 0.17737, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1791 - accuracy: 0.9949 - val_loss: 0.1774 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 122/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1792 - accuracy: 0.9949\n","Epoch 122: val_loss improved from 0.17737 to 0.17655, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1792 - accuracy: 0.9949 - val_loss: 0.1766 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 123/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9950\n","Epoch 123: val_loss did not improve from 0.17655\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1774 - accuracy: 0.9950 - val_loss: 0.1766 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 124/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1778 - accuracy: 0.9953\n","Epoch 124: val_loss improved from 0.17655 to 0.17552, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1778 - accuracy: 0.9953 - val_loss: 0.1755 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 125/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1735 - accuracy: 0.9955\n","Epoch 125: val_loss improved from 0.17552 to 0.17436, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1735 - accuracy: 0.9955 - val_loss: 0.1744 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 126/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.9954\n","Epoch 126: val_loss improved from 0.17436 to 0.17283, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1757 - accuracy: 0.9954 - val_loss: 0.1728 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 127/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1735 - accuracy: 0.9955\n","Epoch 127: val_loss did not improve from 0.17283\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1735 - accuracy: 0.9955 - val_loss: 0.1733 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 128/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9957\n","Epoch 128: val_loss improved from 0.17283 to 0.17129, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1719 - accuracy: 0.9957 - val_loss: 0.1713 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 129/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9957\n","Epoch 129: val_loss improved from 0.17129 to 0.16999, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1722 - accuracy: 0.9957 - val_loss: 0.1700 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 130/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1698 - accuracy: 0.9957\n","Epoch 130: val_loss improved from 0.16999 to 0.16864, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1698 - accuracy: 0.9957 - val_loss: 0.1686 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 131/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9957\n","Epoch 131: val_loss improved from 0.16864 to 0.16608, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1712 - accuracy: 0.9957 - val_loss: 0.1661 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 132/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9959\n","Epoch 132: val_loss improved from 0.16608 to 0.16534, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1673 - accuracy: 0.9959 - val_loss: 0.1653 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 133/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9959\n","Epoch 133: val_loss did not improve from 0.16534\n","56/56 [==============================] - 26s 461ms/step - loss: 0.1673 - accuracy: 0.9959 - val_loss: 0.1658 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 134/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9956\n","Epoch 134: val_loss improved from 0.16534 to 0.16436, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1677 - accuracy: 0.9956 - val_loss: 0.1644 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 135/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1664 - accuracy: 0.9957\n","Epoch 135: val_loss improved from 0.16436 to 0.16388, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1664 - accuracy: 0.9957 - val_loss: 0.1639 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 136/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1680 - accuracy: 0.9957\n","Epoch 136: val_loss improved from 0.16388 to 0.16257, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1680 - accuracy: 0.9957 - val_loss: 0.1626 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 137/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.9959\n","Epoch 137: val_loss did not improve from 0.16257\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1645 - accuracy: 0.9959 - val_loss: 0.1628 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 138/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9960\n","Epoch 138: val_loss improved from 0.16257 to 0.16059, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 486ms/step - loss: 0.1644 - accuracy: 0.9960 - val_loss: 0.1606 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 139/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9962\n","Epoch 139: val_loss improved from 0.16059 to 0.15925, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1610 - accuracy: 0.9962 - val_loss: 0.1593 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 140/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1625 - accuracy: 0.9957\n","Epoch 140: val_loss improved from 0.15925 to 0.15740, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1625 - accuracy: 0.9957 - val_loss: 0.1574 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 141/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9894\n","Epoch 141: val_loss did not improve from 0.15740\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1684 - accuracy: 0.9894 - val_loss: 0.1895 - val_accuracy: 0.9621 - lr: 1.0000e-04\n","Epoch 142/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1655 - accuracy: 0.9919\n","Epoch 142: val_loss did not improve from 0.15740\n","56/56 [==============================] - 26s 461ms/step - loss: 0.1655 - accuracy: 0.9919 - val_loss: 0.1763 - val_accuracy: 0.9811 - lr: 1.0000e-04\n","Epoch 143/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1605 - accuracy: 0.9934\n","Epoch 143: val_loss improved from 0.15740 to 0.15508, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1605 - accuracy: 0.9934 - val_loss: 0.1551 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 144/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1611 - accuracy: 0.9941\n","Epoch 144: val_loss did not improve from 0.15508\n","56/56 [==============================] - 26s 460ms/step - loss: 0.1611 - accuracy: 0.9941 - val_loss: 0.1584 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 145/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1583 - accuracy: 0.9956\n","Epoch 145: val_loss did not improve from 0.15508\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1583 - accuracy: 0.9956 - val_loss: 0.1588 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 146/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9959\n","Epoch 146: val_loss did not improve from 0.15508\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1553 - accuracy: 0.9959 - val_loss: 0.1565 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 147/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1545 - accuracy: 0.9960\n","Epoch 147: val_loss did not improve from 0.15508\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1545 - accuracy: 0.9960 - val_loss: 0.1552 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 148/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1532 - accuracy: 0.9961\n","Epoch 148: val_loss improved from 0.15508 to 0.15258, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1532 - accuracy: 0.9961 - val_loss: 0.1526 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 149/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1526 - accuracy: 0.9962\n","Epoch 149: val_loss improved from 0.15258 to 0.15216, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1526 - accuracy: 0.9962 - val_loss: 0.1522 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 150/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9962\n","Epoch 150: val_loss improved from 0.15216 to 0.14921, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1527 - accuracy: 0.9962 - val_loss: 0.1492 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 151/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1528 - accuracy: 0.9962\n","Epoch 151: val_loss improved from 0.14921 to 0.14917, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1528 - accuracy: 0.9962 - val_loss: 0.1492 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 152/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1516 - accuracy: 0.9963\n","Epoch 152: val_loss improved from 0.14917 to 0.14778, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1516 - accuracy: 0.9963 - val_loss: 0.1478 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 153/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1496 - accuracy: 0.9964\n","Epoch 153: val_loss did not improve from 0.14778\n","56/56 [==============================] - 26s 460ms/step - loss: 0.1496 - accuracy: 0.9964 - val_loss: 0.1481 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 154/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1475 - accuracy: 0.9965\n","Epoch 154: val_loss improved from 0.14778 to 0.14652, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 486ms/step - loss: 0.1475 - accuracy: 0.9965 - val_loss: 0.1465 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 155/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.9966\n","Epoch 155: val_loss improved from 0.14652 to 0.14564, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1467 - accuracy: 0.9966 - val_loss: 0.1456 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 156/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9964\n","Epoch 156: val_loss improved from 0.14564 to 0.14491, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 482ms/step - loss: 0.1491 - accuracy: 0.9964 - val_loss: 0.1449 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 157/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.9965\n","Epoch 157: val_loss improved from 0.14491 to 0.14480, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1431 - accuracy: 0.9965 - val_loss: 0.1448 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 158/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9967\n","Epoch 158: val_loss improved from 0.14480 to 0.14247, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1434 - accuracy: 0.9967 - val_loss: 0.1425 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 159/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9966\n","Epoch 159: val_loss improved from 0.14247 to 0.14163, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1446 - accuracy: 0.9966 - val_loss: 0.1416 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 160/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9966\n","Epoch 160: val_loss improved from 0.14163 to 0.14018, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1433 - accuracy: 0.9966 - val_loss: 0.1402 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 161/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.9965\n","Epoch 161: val_loss improved from 0.14018 to 0.14000, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 477ms/step - loss: 0.1422 - accuracy: 0.9965 - val_loss: 0.1400 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 162/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9967\n","Epoch 162: val_loss improved from 0.14000 to 0.13952, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 475ms/step - loss: 0.1403 - accuracy: 0.9967 - val_loss: 0.1395 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 163/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9966\n","Epoch 163: val_loss improved from 0.13952 to 0.13705, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 482ms/step - loss: 0.1434 - accuracy: 0.9966 - val_loss: 0.1371 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 164/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9967\n","Epoch 164: val_loss did not improve from 0.13705\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1414 - accuracy: 0.9967 - val_loss: 0.1380 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 165/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1389 - accuracy: 0.9968\n","Epoch 165: val_loss did not improve from 0.13705\n","56/56 [==============================] - 26s 461ms/step - loss: 0.1389 - accuracy: 0.9968 - val_loss: 0.1379 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 166/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9968\n","Epoch 166: val_loss improved from 0.13705 to 0.13591, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1361 - accuracy: 0.9968 - val_loss: 0.1359 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 167/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9968\n","Epoch 167: val_loss improved from 0.13591 to 0.13494, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1362 - accuracy: 0.9968 - val_loss: 0.1349 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 168/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1376 - accuracy: 0.9970\n","Epoch 168: val_loss improved from 0.13494 to 0.13486, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 485ms/step - loss: 0.1376 - accuracy: 0.9970 - val_loss: 0.1349 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 169/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9970\n","Epoch 169: val_loss improved from 0.13486 to 0.13343, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1375 - accuracy: 0.9970 - val_loss: 0.1334 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 170/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1368 - accuracy: 0.9968\n","Epoch 170: val_loss improved from 0.13343 to 0.13279, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1368 - accuracy: 0.9968 - val_loss: 0.1328 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 171/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1347 - accuracy: 0.9969\n","Epoch 171: val_loss improved from 0.13279 to 0.13222, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 474ms/step - loss: 0.1347 - accuracy: 0.9969 - val_loss: 0.1322 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 172/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9969\n","Epoch 172: val_loss improved from 0.13222 to 0.13048, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1369 - accuracy: 0.9969 - val_loss: 0.1305 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 173/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1350 - accuracy: 0.9968\n","Epoch 173: val_loss did not improve from 0.13048\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1350 - accuracy: 0.9968 - val_loss: 0.1311 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 174/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9969\n","Epoch 174: val_loss improved from 0.13048 to 0.12989, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1335 - accuracy: 0.9969 - val_loss: 0.1299 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 175/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9968\n","Epoch 175: val_loss improved from 0.12989 to 0.12978, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1335 - accuracy: 0.9968 - val_loss: 0.1298 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 176/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9971\n","Epoch 176: val_loss improved from 0.12978 to 0.12788, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1318 - accuracy: 0.9971 - val_loss: 0.1279 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 177/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1303 - accuracy: 0.9970\n","Epoch 177: val_loss improved from 0.12788 to 0.12715, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1303 - accuracy: 0.9970 - val_loss: 0.1271 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 178/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1329 - accuracy: 0.9970\n","Epoch 178: val_loss improved from 0.12715 to 0.12570, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1329 - accuracy: 0.9970 - val_loss: 0.1257 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 179/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1312 - accuracy: 0.9950\n","Epoch 179: val_loss improved from 0.12570 to 0.12247, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1312 - accuracy: 0.9950 - val_loss: 0.1225 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 180/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.9960\n","Epoch 180: val_loss did not improve from 0.12247\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1326 - accuracy: 0.9960 - val_loss: 0.1264 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 181/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1269 - accuracy: 0.9969\n","Epoch 181: val_loss did not improve from 0.12247\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1269 - accuracy: 0.9969 - val_loss: 0.1270 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 182/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9967\n","Epoch 182: val_loss did not improve from 0.12247\n","56/56 [==============================] - 26s 461ms/step - loss: 0.1271 - accuracy: 0.9967 - val_loss: 0.1244 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 183/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 0.9970\n","Epoch 183: val_loss did not improve from 0.12247\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1262 - accuracy: 0.9970 - val_loss: 0.1240 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 184/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9970\n","Epoch 184: val_loss did not improve from 0.12247\n","56/56 [==============================] - 26s 460ms/step - loss: 0.1281 - accuracy: 0.9970 - val_loss: 0.1239 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 185/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.9972\n","Epoch 185: val_loss did not improve from 0.12247\n","56/56 [==============================] - 26s 460ms/step - loss: 0.1254 - accuracy: 0.9972 - val_loss: 0.1230 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 186/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1221 - accuracy: 0.9974\n","Epoch 186: val_loss improved from 0.12247 to 0.12161, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1221 - accuracy: 0.9974 - val_loss: 0.1216 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 187/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1238 - accuracy: 0.9973\n","Epoch 187: val_loss improved from 0.12161 to 0.12064, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1238 - accuracy: 0.9973 - val_loss: 0.1206 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 188/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1224 - accuracy: 0.9972\n","Epoch 188: val_loss improved from 0.12064 to 0.11960, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1224 - accuracy: 0.9972 - val_loss: 0.1196 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 189/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1228 - accuracy: 0.9974\n","Epoch 189: val_loss improved from 0.11960 to 0.11957, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1228 - accuracy: 0.9974 - val_loss: 0.1196 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 190/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9974\n","Epoch 190: val_loss improved from 0.11957 to 0.11833, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1216 - accuracy: 0.9974 - val_loss: 0.1183 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 191/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9974\n","Epoch 191: val_loss improved from 0.11833 to 0.11775, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1211 - accuracy: 0.9974 - val_loss: 0.1177 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 192/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1206 - accuracy: 0.9974\n","Epoch 192: val_loss improved from 0.11775 to 0.11688, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1206 - accuracy: 0.9974 - val_loss: 0.1169 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 193/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.9974\n","Epoch 193: val_loss did not improve from 0.11688\n","56/56 [==============================] - 26s 461ms/step - loss: 0.1196 - accuracy: 0.9974 - val_loss: 0.1171 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 194/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9975\n","Epoch 194: val_loss improved from 0.11688 to 0.11661, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1180 - accuracy: 0.9975 - val_loss: 0.1166 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 195/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.9975\n","Epoch 195: val_loss improved from 0.11661 to 0.11587, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1196 - accuracy: 0.9975 - val_loss: 0.1159 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 196/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9899\n","Epoch 196: val_loss did not improve from 0.11587\n","56/56 [==============================] - 26s 460ms/step - loss: 0.1287 - accuracy: 0.9899 - val_loss: 0.1925 - val_accuracy: 0.9411 - lr: 1.0000e-04\n","Epoch 197/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1302 - accuracy: 0.9888\n","Epoch 197: val_loss did not improve from 0.11587\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1302 - accuracy: 0.9888 - val_loss: 0.1236 - val_accuracy: 0.9846 - lr: 1.0000e-04\n","Epoch 198/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1269 - accuracy: 0.9902\n","Epoch 198: val_loss did not improve from 0.11587\n","56/56 [==============================] - 26s 462ms/step - loss: 0.1269 - accuracy: 0.9902 - val_loss: 0.2074 - val_accuracy: 0.9423 - lr: 1.0000e-04\n","Epoch 199/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9944\n","Epoch 199: val_loss improved from 0.11587 to 0.11482, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1219 - accuracy: 0.9944 - val_loss: 0.1148 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 200/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1192 - accuracy: 0.9956\n","Epoch 200: val_loss did not improve from 0.11482\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1192 - accuracy: 0.9956 - val_loss: 0.1194 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 201/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.9966\n","Epoch 201: val_loss did not improve from 0.11482\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1159 - accuracy: 0.9966 - val_loss: 0.1186 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 202/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9963\n","Epoch 202: val_loss did not improve from 0.11482\n","56/56 [==============================] - 26s 460ms/step - loss: 0.1170 - accuracy: 0.9963 - val_loss: 0.1302 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 203/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9958\n","Epoch 203: val_loss did not improve from 0.11482\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1169 - accuracy: 0.9958 - val_loss: 0.1155 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 204/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9968\n","Epoch 204: val_loss improved from 0.11482 to 0.11435, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1161 - accuracy: 0.9968 - val_loss: 0.1143 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 205/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9971\n","Epoch 205: val_loss improved from 0.11435 to 0.11212, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1130 - accuracy: 0.9971 - val_loss: 0.1121 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 206/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1095 - accuracy: 0.9971\n","Epoch 206: val_loss did not improve from 0.11212\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1095 - accuracy: 0.9971 - val_loss: 0.1126 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 207/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9973\n","Epoch 207: val_loss improved from 0.11212 to 0.10932, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 482ms/step - loss: 0.1114 - accuracy: 0.9973 - val_loss: 0.1093 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 208/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1122 - accuracy: 0.9973\n","Epoch 208: val_loss did not improve from 0.10932\n","56/56 [==============================] - 26s 462ms/step - loss: 0.1122 - accuracy: 0.9973 - val_loss: 0.1101 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 209/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9974\n","Epoch 209: val_loss improved from 0.10932 to 0.10887, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1092 - accuracy: 0.9974 - val_loss: 0.1089 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 210/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9975\n","Epoch 210: val_loss improved from 0.10887 to 0.10810, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 475ms/step - loss: 0.1099 - accuracy: 0.9975 - val_loss: 0.1081 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 211/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9975\n","Epoch 211: val_loss improved from 0.10810 to 0.10743, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1096 - accuracy: 0.9975 - val_loss: 0.1074 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 212/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9976\n","Epoch 212: val_loss improved from 0.10743 to 0.10707, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1087 - accuracy: 0.9976 - val_loss: 0.1071 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 213/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9976\n","Epoch 213: val_loss improved from 0.10707 to 0.10603, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1082 - accuracy: 0.9976 - val_loss: 0.1060 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 214/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9976\n","Epoch 214: val_loss improved from 0.10603 to 0.10509, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1059 - accuracy: 0.9976 - val_loss: 0.1051 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 215/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9977\n","Epoch 215: val_loss improved from 0.10509 to 0.10482, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.1054 - accuracy: 0.9977 - val_loss: 0.1048 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 216/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1083 - accuracy: 0.9977\n","Epoch 216: val_loss improved from 0.10482 to 0.10425, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1083 - accuracy: 0.9977 - val_loss: 0.1043 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 217/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.9976\n","Epoch 217: val_loss improved from 0.10425 to 0.10162, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 477ms/step - loss: 0.1091 - accuracy: 0.9976 - val_loss: 0.1016 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 218/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9978\n","Epoch 218: val_loss did not improve from 0.10162\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1044 - accuracy: 0.9978 - val_loss: 0.1033 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 219/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.9978\n","Epoch 219: val_loss did not improve from 0.10162\n","56/56 [==============================] - 26s 461ms/step - loss: 0.1034 - accuracy: 0.9978 - val_loss: 0.1025 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 220/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9978\n","Epoch 220: val_loss did not improve from 0.10162\n","56/56 [==============================] - 26s 473ms/step - loss: 0.1043 - accuracy: 0.9978 - val_loss: 0.1018 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 221/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9978\n","Epoch 221: val_loss improved from 0.10162 to 0.10114, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1043 - accuracy: 0.9978 - val_loss: 0.1011 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 222/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1031 - accuracy: 0.9978\n","Epoch 222: val_loss improved from 0.10114 to 0.10112, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.1031 - accuracy: 0.9978 - val_loss: 0.1011 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 223/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9978\n","Epoch 223: val_loss improved from 0.10112 to 0.09850, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 474ms/step - loss: 0.1050 - accuracy: 0.9978 - val_loss: 0.0985 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 224/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9978\n","Epoch 224: val_loss did not improve from 0.09850\n","56/56 [==============================] - 26s 460ms/step - loss: 0.1022 - accuracy: 0.9978 - val_loss: 0.0990 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 225/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9973\n","Epoch 225: val_loss did not improve from 0.09850\n","56/56 [==============================] - 26s 472ms/step - loss: 0.1015 - accuracy: 0.9973 - val_loss: 0.0994 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 226/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9978\n","Epoch 226: val_loss improved from 0.09850 to 0.09809, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.1012 - accuracy: 0.9978 - val_loss: 0.0981 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 227/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9979\n","Epoch 227: val_loss improved from 0.09809 to 0.09690, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0988 - accuracy: 0.9979 - val_loss: 0.0969 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 228/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9980\n","Epoch 228: val_loss did not improve from 0.09690\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0986 - accuracy: 0.9980 - val_loss: 0.0972 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 229/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9979\n","Epoch 229: val_loss did not improve from 0.09690\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0997 - accuracy: 0.9979 - val_loss: 0.0974 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 230/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9980\n","Epoch 230: val_loss improved from 0.09690 to 0.09551, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0978 - accuracy: 0.9980 - val_loss: 0.0955 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 231/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9980\n","Epoch 231: val_loss did not improve from 0.09551\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0979 - accuracy: 0.9980 - val_loss: 0.0957 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 232/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9980\n","Epoch 232: val_loss improved from 0.09551 to 0.09499, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0986 - accuracy: 0.9980 - val_loss: 0.0950 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 233/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9980\n","Epoch 233: val_loss improved from 0.09499 to 0.09472, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0967 - accuracy: 0.9980 - val_loss: 0.0947 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 234/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.9980\n","Epoch 234: val_loss improved from 0.09472 to 0.09467, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 486ms/step - loss: 0.0974 - accuracy: 0.9980 - val_loss: 0.0947 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 235/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0954 - accuracy: 0.9980\n","Epoch 235: val_loss improved from 0.09467 to 0.09356, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0954 - accuracy: 0.9980 - val_loss: 0.0936 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 236/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9981\n","Epoch 236: val_loss improved from 0.09356 to 0.09322, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0940 - accuracy: 0.9981 - val_loss: 0.0932 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 237/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9981\n","Epoch 237: val_loss improved from 0.09322 to 0.09277, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0953 - accuracy: 0.9981 - val_loss: 0.0928 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 238/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9981\n","Epoch 238: val_loss improved from 0.09277 to 0.09241, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0951 - accuracy: 0.9981 - val_loss: 0.0924 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 239/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0932 - accuracy: 0.9981\n","Epoch 239: val_loss improved from 0.09241 to 0.09189, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0932 - accuracy: 0.9981 - val_loss: 0.0919 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 240/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9981\n","Epoch 240: val_loss improved from 0.09189 to 0.09132, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0929 - accuracy: 0.9981 - val_loss: 0.0913 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 241/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9981\n","Epoch 241: val_loss improved from 0.09132 to 0.09076, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0936 - accuracy: 0.9981 - val_loss: 0.0908 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 242/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9981\n","Epoch 242: val_loss improved from 0.09076 to 0.09024, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0934 - accuracy: 0.9981 - val_loss: 0.0902 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 243/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9982\n","Epoch 243: val_loss improved from 0.09024 to 0.08922, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0898 - accuracy: 0.9982 - val_loss: 0.0892 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 244/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 0.9983\n","Epoch 244: val_loss did not improve from 0.08922\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0926 - accuracy: 0.9983 - val_loss: 0.0894 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 245/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9981\n","Epoch 245: val_loss improved from 0.08922 to 0.08814, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0918 - accuracy: 0.9981 - val_loss: 0.0881 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 246/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9981\n","Epoch 246: val_loss did not improve from 0.08814\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0911 - accuracy: 0.9981 - val_loss: 0.0895 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 247/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9982\n","Epoch 247: val_loss improved from 0.08814 to 0.08733, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0887 - accuracy: 0.9982 - val_loss: 0.0873 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 248/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9982\n","Epoch 248: val_loss did not improve from 0.08733\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0891 - accuracy: 0.9982 - val_loss: 0.0885 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 249/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9983\n","Epoch 249: val_loss improved from 0.08733 to 0.08650, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0869 - accuracy: 0.9983 - val_loss: 0.0865 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 250/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9981\n","Epoch 250: val_loss did not improve from 0.08650\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0879 - accuracy: 0.9981 - val_loss: 0.0871 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 251/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9983\n","Epoch 251: val_loss improved from 0.08650 to 0.08576, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0877 - accuracy: 0.9983 - val_loss: 0.0858 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 252/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9982\n","Epoch 252: val_loss improved from 0.08576 to 0.08565, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0872 - accuracy: 0.9982 - val_loss: 0.0856 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 253/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9982\n","Epoch 253: val_loss improved from 0.08565 to 0.08467, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0882 - accuracy: 0.9982 - val_loss: 0.0847 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 254/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9984\n","Epoch 254: val_loss improved from 0.08467 to 0.08458, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0854 - accuracy: 0.9984 - val_loss: 0.0846 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 255/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9983\n","Epoch 255: val_loss improved from 0.08458 to 0.08378, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0878 - accuracy: 0.9983 - val_loss: 0.0838 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 256/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9983\n","Epoch 256: val_loss did not improve from 0.08378\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0856 - accuracy: 0.9983 - val_loss: 0.0844 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 257/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9982\n","Epoch 257: val_loss improved from 0.08378 to 0.08320, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0861 - accuracy: 0.9982 - val_loss: 0.0832 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 258/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9984\n","Epoch 258: val_loss improved from 0.08320 to 0.08222, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0853 - accuracy: 0.9984 - val_loss: 0.0822 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 259/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9983\n","Epoch 259: val_loss did not improve from 0.08222\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0871 - accuracy: 0.9983 - val_loss: 0.0823 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 260/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9983\n","Epoch 260: val_loss did not improve from 0.08222\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0825 - accuracy: 0.9983 - val_loss: 0.0824 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 261/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9983\n","Epoch 261: val_loss improved from 0.08222 to 0.08029, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0830 - accuracy: 0.9983 - val_loss: 0.0803 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 262/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9983\n","Epoch 262: val_loss did not improve from 0.08029\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0847 - accuracy: 0.9983 - val_loss: 0.0825 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 263/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9984\n","Epoch 263: val_loss did not improve from 0.08029\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0827 - accuracy: 0.9984 - val_loss: 0.0814 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 264/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9985\n","Epoch 264: val_loss did not improve from 0.08029\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0810 - accuracy: 0.9985 - val_loss: 0.0811 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 265/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9914\n","Epoch 265: val_loss did not improve from 0.08029\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0958 - accuracy: 0.9914 - val_loss: 0.0878 - val_accuracy: 0.9840 - lr: 1.0000e-04\n","Epoch 266/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9923\n","Epoch 266: val_loss did not improve from 0.08029\n","56/56 [==============================] - 26s 463ms/step - loss: 0.0911 - accuracy: 0.9923 - val_loss: 0.0847 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 267/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0894 - accuracy: 0.9940\n","Epoch 267: val_loss did not improve from 0.08029\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0894 - accuracy: 0.9940 - val_loss: 0.1051 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 268/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9967\n","Epoch 268: val_loss did not improve from 0.08029\n","56/56 [==============================] - 26s 462ms/step - loss: 0.0827 - accuracy: 0.9967 - val_loss: 0.0875 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 269/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9975\n","Epoch 269: val_loss did not improve from 0.08029\n","56/56 [==============================] - 26s 462ms/step - loss: 0.0838 - accuracy: 0.9975 - val_loss: 0.0846 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 270/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9978\n","Epoch 270: val_loss did not improve from 0.08029\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0830 - accuracy: 0.9978 - val_loss: 0.0817 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 271/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9981\n","Epoch 271: val_loss did not improve from 0.08029\n","\n","Epoch 271: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n","56/56 [==============================] - 26s 462ms/step - loss: 0.0811 - accuracy: 0.9981 - val_loss: 0.0807 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 272/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9982\n","Epoch 272: val_loss improved from 0.08029 to 0.07898, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0798 - accuracy: 0.9982 - val_loss: 0.0790 - val_accuracy: 0.9967 - lr: 1.0000e-05\n","Epoch 273/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9983\n","Epoch 273: val_loss improved from 0.07898 to 0.07826, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 486ms/step - loss: 0.0809 - accuracy: 0.9983 - val_loss: 0.0783 - val_accuracy: 0.9968 - lr: 1.0000e-05\n","Epoch 274/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9983\n","Epoch 274: val_loss improved from 0.07826 to 0.07763, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0791 - accuracy: 0.9983 - val_loss: 0.0776 - val_accuracy: 0.9968 - lr: 1.0000e-05\n","Epoch 275/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9982\n","Epoch 275: val_loss did not improve from 0.07763\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0823 - accuracy: 0.9982 - val_loss: 0.0776 - val_accuracy: 0.9968 - lr: 1.0000e-05\n","Epoch 276/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9983\n","Epoch 276: val_loss improved from 0.07763 to 0.07734, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0788 - accuracy: 0.9983 - val_loss: 0.0773 - val_accuracy: 0.9968 - lr: 1.0000e-05\n","Epoch 277/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9983\n","Epoch 277: val_loss improved from 0.07734 to 0.07729, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0801 - accuracy: 0.9983 - val_loss: 0.0773 - val_accuracy: 0.9968 - lr: 1.0000e-05\n","Epoch 278/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9983\n","Epoch 278: val_loss improved from 0.07729 to 0.07697, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0771 - accuracy: 0.9983 - val_loss: 0.0770 - val_accuracy: 0.9968 - lr: 1.0000e-05\n","Epoch 279/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9984\n","Epoch 279: val_loss improved from 0.07697 to 0.07674, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0775 - accuracy: 0.9984 - val_loss: 0.0767 - val_accuracy: 0.9969 - lr: 1.0000e-05\n","Epoch 280/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9984\n","Epoch 280: val_loss did not improve from 0.07674\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0797 - accuracy: 0.9984 - val_loss: 0.0768 - val_accuracy: 0.9969 - lr: 1.0000e-05\n","Epoch 281/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9984\n","Epoch 281: val_loss did not improve from 0.07674\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0789 - accuracy: 0.9984 - val_loss: 0.0768 - val_accuracy: 0.9969 - lr: 1.0000e-05\n","Epoch 282/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9984\n","Epoch 282: val_loss did not improve from 0.07674\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0802 - accuracy: 0.9984 - val_loss: 0.0769 - val_accuracy: 0.9969 - lr: 1.0000e-05\n","Epoch 283/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9984\n","Epoch 283: val_loss improved from 0.07674 to 0.07659, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0784 - accuracy: 0.9984 - val_loss: 0.0766 - val_accuracy: 0.9969 - lr: 1.0000e-05\n","Epoch 284/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9984\n","Epoch 284: val_loss improved from 0.07659 to 0.07656, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0791 - accuracy: 0.9984 - val_loss: 0.0766 - val_accuracy: 0.9969 - lr: 1.0000e-05\n","Epoch 285/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9984\n","Epoch 285: val_loss improved from 0.07656 to 0.07638, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 475ms/step - loss: 0.0774 - accuracy: 0.9984 - val_loss: 0.0764 - val_accuracy: 0.9969 - lr: 1.0000e-05\n","Epoch 286/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9985\n","Epoch 286: val_loss improved from 0.07638 to 0.07628, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0773 - accuracy: 0.9985 - val_loss: 0.0763 - val_accuracy: 0.9969 - lr: 1.0000e-05\n","Epoch 287/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9985\n","Epoch 287: val_loss did not improve from 0.07628\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0788 - accuracy: 0.9985 - val_loss: 0.0764 - val_accuracy: 0.9969 - lr: 1.0000e-05\n","Epoch 288/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9985\n","Epoch 288: val_loss did not improve from 0.07628\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0803 - accuracy: 0.9985 - val_loss: 0.0765 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 289/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9985\n","Epoch 289: val_loss improved from 0.07628 to 0.07623, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0776 - accuracy: 0.9985 - val_loss: 0.0762 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 290/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9985\n","Epoch 290: val_loss did not improve from 0.07623\n","56/56 [==============================] - 26s 464ms/step - loss: 0.0786 - accuracy: 0.9985 - val_loss: 0.0763 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 291/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9984\n","Epoch 291: val_loss did not improve from 0.07623\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0814 - accuracy: 0.9984 - val_loss: 0.0765 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 292/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9985\n","Epoch 292: val_loss improved from 0.07623 to 0.07604, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 485ms/step - loss: 0.0782 - accuracy: 0.9985 - val_loss: 0.0760 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 293/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9985\n","Epoch 293: val_loss did not improve from 0.07604\n","56/56 [==============================] - 26s 462ms/step - loss: 0.0771 - accuracy: 0.9985 - val_loss: 0.0761 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 294/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9985\n","Epoch 294: val_loss did not improve from 0.07604\n","56/56 [==============================] - 26s 462ms/step - loss: 0.0786 - accuracy: 0.9985 - val_loss: 0.0761 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 295/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9985\n","Epoch 295: val_loss did not improve from 0.07604\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0799 - accuracy: 0.9985 - val_loss: 0.0762 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 296/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9985\n","Epoch 296: val_loss did not improve from 0.07604\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0782 - accuracy: 0.9985 - val_loss: 0.0761 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 297/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9985\n","Epoch 297: val_loss did not improve from 0.07604\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0800 - accuracy: 0.9985 - val_loss: 0.0761 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 298/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9985\n","Epoch 298: val_loss improved from 0.07604 to 0.07587, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0775 - accuracy: 0.9985 - val_loss: 0.0759 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 299/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9985\n","Epoch 299: val_loss improved from 0.07587 to 0.07582, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0809 - accuracy: 0.9985 - val_loss: 0.0758 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 300/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9986\n","Epoch 300: val_loss did not improve from 0.07582\n","56/56 [==============================] - 26s 462ms/step - loss: 0.0790 - accuracy: 0.9986 - val_loss: 0.0759 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 301/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9986\n","Epoch 301: val_loss improved from 0.07582 to 0.07581, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0780 - accuracy: 0.9986 - val_loss: 0.0758 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 302/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9984\n","Epoch 302: val_loss did not improve from 0.07581\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0805 - accuracy: 0.9984 - val_loss: 0.0761 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 303/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9985\n","Epoch 303: val_loss improved from 0.07581 to 0.07543, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0777 - accuracy: 0.9985 - val_loss: 0.0754 - val_accuracy: 0.9970 - lr: 1.0000e-05\n","Epoch 304/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9986\n","Epoch 304: val_loss did not improve from 0.07543\n","56/56 [==============================] - 26s 474ms/step - loss: 0.0782 - accuracy: 0.9986 - val_loss: 0.0756 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 305/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9986\n","Epoch 305: val_loss did not improve from 0.07543\n","56/56 [==============================] - 26s 463ms/step - loss: 0.0770 - accuracy: 0.9986 - val_loss: 0.0757 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 306/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9986\n","Epoch 306: val_loss improved from 0.07543 to 0.07537, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0768 - accuracy: 0.9986 - val_loss: 0.0754 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 307/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9986\n","Epoch 307: val_loss improved from 0.07537 to 0.07522, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 474ms/step - loss: 0.0763 - accuracy: 0.9986 - val_loss: 0.0752 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 308/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9986\n","Epoch 308: val_loss did not improve from 0.07522\n","56/56 [==============================] - 26s 463ms/step - loss: 0.0788 - accuracy: 0.9986 - val_loss: 0.0754 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 309/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9985\n","Epoch 309: val_loss did not improve from 0.07522\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0795 - accuracy: 0.9985 - val_loss: 0.0752 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 310/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9986\n","Epoch 310: val_loss did not improve from 0.07522\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0782 - accuracy: 0.9986 - val_loss: 0.0753 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 311/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9986\n","Epoch 311: val_loss did not improve from 0.07522\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0776 - accuracy: 0.9986 - val_loss: 0.0753 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 312/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9985\n","Epoch 312: val_loss improved from 0.07522 to 0.07511, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0783 - accuracy: 0.9985 - val_loss: 0.0751 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 313/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9986\n","Epoch 313: val_loss improved from 0.07511 to 0.07500, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0760 - accuracy: 0.9986 - val_loss: 0.0750 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 314/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9986\n","Epoch 314: val_loss did not improve from 0.07500\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0811 - accuracy: 0.9986 - val_loss: 0.0753 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 315/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9987\n","Epoch 315: val_loss did not improve from 0.07500\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0775 - accuracy: 0.9987 - val_loss: 0.0752 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 316/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9987\n","Epoch 316: val_loss improved from 0.07500 to 0.07500, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0791 - accuracy: 0.9987 - val_loss: 0.0750 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 317/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9985\n","Epoch 317: val_loss did not improve from 0.07500\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0764 - accuracy: 0.9985 - val_loss: 0.0752 - val_accuracy: 0.9969 - lr: 1.0000e-05\n","Epoch 318/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9986\n","Epoch 318: val_loss did not improve from 0.07500\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0768 - accuracy: 0.9986 - val_loss: 0.0752 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 319/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9987\n","Epoch 319: val_loss improved from 0.07500 to 0.07491, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0779 - accuracy: 0.9987 - val_loss: 0.0749 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 320/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9987\n","Epoch 320: val_loss did not improve from 0.07491\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0770 - accuracy: 0.9987 - val_loss: 0.0751 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 321/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9987\n","Epoch 321: val_loss improved from 0.07491 to 0.07489, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0751 - accuracy: 0.9987 - val_loss: 0.0749 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 322/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9987\n","Epoch 322: val_loss improved from 0.07489 to 0.07481, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0771 - accuracy: 0.9987 - val_loss: 0.0748 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 323/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9985\n","Epoch 323: val_loss improved from 0.07481 to 0.07475, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0794 - accuracy: 0.9985 - val_loss: 0.0747 - val_accuracy: 0.9969 - lr: 1.0000e-05\n","Epoch 324/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9987\n","Epoch 324: val_loss improved from 0.07475 to 0.07463, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0751 - accuracy: 0.9987 - val_loss: 0.0746 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 325/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9987\n","Epoch 325: val_loss improved from 0.07463 to 0.07443, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0777 - accuracy: 0.9987 - val_loss: 0.0744 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 326/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9987\n","Epoch 326: val_loss improved from 0.07443 to 0.07431, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 475ms/step - loss: 0.0760 - accuracy: 0.9987 - val_loss: 0.0743 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 327/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9987\n","Epoch 327: val_loss did not improve from 0.07431\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0780 - accuracy: 0.9987 - val_loss: 0.0746 - val_accuracy: 0.9971 - lr: 1.0000e-05\n","Epoch 328/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9987\n","Epoch 328: val_loss did not improve from 0.07431\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0773 - accuracy: 0.9987 - val_loss: 0.0745 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 329/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9987\n","Epoch 329: val_loss did not improve from 0.07431\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0767 - accuracy: 0.9987 - val_loss: 0.0744 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 330/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9987\n","Epoch 330: val_loss did not improve from 0.07431\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0762 - accuracy: 0.9987 - val_loss: 0.0744 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 331/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9987\n","Epoch 331: val_loss improved from 0.07431 to 0.07421, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0758 - accuracy: 0.9987 - val_loss: 0.0742 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 332/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9987\n","Epoch 332: val_loss improved from 0.07421 to 0.07415, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 486ms/step - loss: 0.0777 - accuracy: 0.9987 - val_loss: 0.0742 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 333/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9988\n","Epoch 333: val_loss did not improve from 0.07415\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0773 - accuracy: 0.9988 - val_loss: 0.0744 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 334/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9988\n","Epoch 334: val_loss did not improve from 0.07415\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0753 - accuracy: 0.9988 - val_loss: 0.0742 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 335/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9987\n","Epoch 335: val_loss improved from 0.07415 to 0.07412, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0764 - accuracy: 0.9987 - val_loss: 0.0741 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 336/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9988\n","Epoch 336: val_loss improved from 0.07412 to 0.07412, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0758 - accuracy: 0.9988 - val_loss: 0.0741 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 337/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9988\n","Epoch 337: val_loss improved from 0.07412 to 0.07408, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0769 - accuracy: 0.9988 - val_loss: 0.0741 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 338/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9988\n","Epoch 338: val_loss improved from 0.07408 to 0.07390, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0742 - accuracy: 0.9988 - val_loss: 0.0739 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 339/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9988\n","Epoch 339: val_loss did not improve from 0.07390\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0774 - accuracy: 0.9988 - val_loss: 0.0741 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 340/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9988\n","Epoch 340: val_loss did not improve from 0.07390\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0786 - accuracy: 0.9988 - val_loss: 0.0743 - val_accuracy: 0.9972 - lr: 1.0000e-05\n","Epoch 341/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9987\n","Epoch 341: val_loss improved from 0.07390 to 0.07389, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0765 - accuracy: 0.9987 - val_loss: 0.0739 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 342/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9988\n","Epoch 342: val_loss did not improve from 0.07389\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0747 - accuracy: 0.9988 - val_loss: 0.0739 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 343/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9988\n","Epoch 343: val_loss improved from 0.07389 to 0.07355, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0730 - accuracy: 0.9988 - val_loss: 0.0735 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 344/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9988\n","Epoch 344: val_loss improved from 0.07355 to 0.07334, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0791 - accuracy: 0.9988 - val_loss: 0.0733 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 345/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9988\n","Epoch 345: val_loss did not improve from 0.07334\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0765 - accuracy: 0.9988 - val_loss: 0.0738 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 346/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9988\n","Epoch 346: val_loss did not improve from 0.07334\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0757 - accuracy: 0.9988 - val_loss: 0.0735 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 347/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9989\n","Epoch 347: val_loss did not improve from 0.07334\n","56/56 [==============================] - 26s 474ms/step - loss: 0.0756 - accuracy: 0.9989 - val_loss: 0.0737 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 348/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9988\n","Epoch 348: val_loss did not improve from 0.07334\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0762 - accuracy: 0.9988 - val_loss: 0.0736 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 349/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9988\n","Epoch 349: val_loss did not improve from 0.07334\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0765 - accuracy: 0.9988 - val_loss: 0.0736 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 350/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9988\n","Epoch 350: val_loss did not improve from 0.07334\n","56/56 [==============================] - 26s 462ms/step - loss: 0.0761 - accuracy: 0.9988 - val_loss: 0.0735 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 351/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9988\n","Epoch 351: val_loss improved from 0.07334 to 0.07321, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0740 - accuracy: 0.9988 - val_loss: 0.0732 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 352/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9988\n","Epoch 352: val_loss improved from 0.07321 to 0.07300, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 475ms/step - loss: 0.0772 - accuracy: 0.9988 - val_loss: 0.0730 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 353/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9989\n","Epoch 353: val_loss did not improve from 0.07300\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0750 - accuracy: 0.9989 - val_loss: 0.0732 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 354/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9988\n","Epoch 354: val_loss did not improve from 0.07300\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0772 - accuracy: 0.9988 - val_loss: 0.0731 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 355/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9988\n","Epoch 355: val_loss did not improve from 0.07300\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0776 - accuracy: 0.9988 - val_loss: 0.0735 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 356/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9989\n","Epoch 356: val_loss did not improve from 0.07300\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0736 - accuracy: 0.9989 - val_loss: 0.0733 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 357/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9988\n","Epoch 357: val_loss improved from 0.07300 to 0.07296, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0786 - accuracy: 0.9988 - val_loss: 0.0730 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 358/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9989\n","Epoch 358: val_loss improved from 0.07296 to 0.07278, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0767 - accuracy: 0.9989 - val_loss: 0.0728 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 359/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9988\n","Epoch 359: val_loss did not improve from 0.07278\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0773 - accuracy: 0.9988 - val_loss: 0.0730 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 360/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9988\n","Epoch 360: val_loss did not improve from 0.07278\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0744 - accuracy: 0.9988 - val_loss: 0.0730 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 361/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9989\n","Epoch 361: val_loss did not improve from 0.07278\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0771 - accuracy: 0.9989 - val_loss: 0.0729 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 362/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9989\n","Epoch 362: val_loss did not improve from 0.07278\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0746 - accuracy: 0.9989 - val_loss: 0.0730 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 363/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9988\n","Epoch 363: val_loss did not improve from 0.07278\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0755 - accuracy: 0.9988 - val_loss: 0.0731 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 364/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9988\n","Epoch 364: val_loss improved from 0.07278 to 0.07275, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0755 - accuracy: 0.9988 - val_loss: 0.0727 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 365/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9988\n","Epoch 365: val_loss did not improve from 0.07275\n","56/56 [==============================] - 26s 462ms/step - loss: 0.0754 - accuracy: 0.9988 - val_loss: 0.0729 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 366/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9989\n","Epoch 366: val_loss did not improve from 0.07275\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0749 - accuracy: 0.9989 - val_loss: 0.0728 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 367/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9989\n","Epoch 367: val_loss did not improve from 0.07275\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0749 - accuracy: 0.9989 - val_loss: 0.0729 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 368/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9990\n","Epoch 368: val_loss improved from 0.07275 to 0.07272, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 485ms/step - loss: 0.0748 - accuracy: 0.9990 - val_loss: 0.0727 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 369/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9989\n","Epoch 369: val_loss improved from 0.07272 to 0.07269, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0731 - accuracy: 0.9989 - val_loss: 0.0727 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 370/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9990\n","Epoch 370: val_loss improved from 0.07269 to 0.07240, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0734 - accuracy: 0.9990 - val_loss: 0.0724 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 371/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9989\n","Epoch 371: val_loss did not improve from 0.07240\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0736 - accuracy: 0.9989 - val_loss: 0.0724 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 372/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9989\n","Epoch 372: val_loss improved from 0.07240 to 0.07222, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0748 - accuracy: 0.9989 - val_loss: 0.0722 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 373/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9989\n","Epoch 373: val_loss did not improve from 0.07222\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0741 - accuracy: 0.9989 - val_loss: 0.0723 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 374/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9989\n","Epoch 374: val_loss did not improve from 0.07222\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0779 - accuracy: 0.9989 - val_loss: 0.0726 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 375/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9989\n","Epoch 375: val_loss did not improve from 0.07222\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0735 - accuracy: 0.9989 - val_loss: 0.0723 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 376/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9989\n","Epoch 376: val_loss did not improve from 0.07222\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0761 - accuracy: 0.9989 - val_loss: 0.0723 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 377/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9990\n","Epoch 377: val_loss did not improve from 0.07222\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0754 - accuracy: 0.9990 - val_loss: 0.0725 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 378/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9989\n","Epoch 378: val_loss did not improve from 0.07222\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0745 - accuracy: 0.9989 - val_loss: 0.0723 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 379/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9990\n","Epoch 379: val_loss did not improve from 0.07222\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0734 - accuracy: 0.9990 - val_loss: 0.0723 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 380/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9990\n","Epoch 380: val_loss improved from 0.07222 to 0.07215, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0736 - accuracy: 0.9990 - val_loss: 0.0721 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 381/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9989\n","Epoch 381: val_loss improved from 0.07215 to 0.07169, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0748 - accuracy: 0.9989 - val_loss: 0.0717 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 382/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9990\n","Epoch 382: val_loss did not improve from 0.07169\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0730 - accuracy: 0.9990 - val_loss: 0.0719 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 383/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9989\n","Epoch 383: val_loss improved from 0.07169 to 0.07166, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0741 - accuracy: 0.9989 - val_loss: 0.0717 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 384/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9989\n","Epoch 384: val_loss did not improve from 0.07166\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0733 - accuracy: 0.9989 - val_loss: 0.0718 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 385/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9990\n","Epoch 385: val_loss improved from 0.07166 to 0.07157, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 487ms/step - loss: 0.0734 - accuracy: 0.9990 - val_loss: 0.0716 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 386/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9990\n","Epoch 386: val_loss did not improve from 0.07157\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0734 - accuracy: 0.9990 - val_loss: 0.0717 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 387/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9989\n","Epoch 387: val_loss did not improve from 0.07157\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0750 - accuracy: 0.9989 - val_loss: 0.0717 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 388/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9989\n","Epoch 388: val_loss did not improve from 0.07157\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0739 - accuracy: 0.9989 - val_loss: 0.0718 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 389/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9990\n","Epoch 389: val_loss did not improve from 0.07157\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0743 - accuracy: 0.9990 - val_loss: 0.0717 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 390/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9990\n","Epoch 390: val_loss improved from 0.07157 to 0.07155, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0735 - accuracy: 0.9990 - val_loss: 0.0716 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 391/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9990\n","Epoch 391: val_loss did not improve from 0.07155\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0748 - accuracy: 0.9990 - val_loss: 0.0718 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 392/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9990\n","Epoch 392: val_loss did not improve from 0.07155\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0726 - accuracy: 0.9990 - val_loss: 0.0716 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 393/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9990\n","Epoch 393: val_loss improved from 0.07155 to 0.07142, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0741 - accuracy: 0.9990 - val_loss: 0.0714 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 394/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9990\n","Epoch 394: val_loss did not improve from 0.07142\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0741 - accuracy: 0.9990 - val_loss: 0.0715 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 395/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9990\n","Epoch 395: val_loss improved from 0.07142 to 0.07134, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 482ms/step - loss: 0.0729 - accuracy: 0.9990 - val_loss: 0.0713 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 396/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9990\n","Epoch 396: val_loss improved from 0.07134 to 0.07133, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0752 - accuracy: 0.9990 - val_loss: 0.0713 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 397/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9990\n","Epoch 397: val_loss did not improve from 0.07133\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0738 - accuracy: 0.9990 - val_loss: 0.0713 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 398/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9990\n","Epoch 398: val_loss did not improve from 0.07133\n","56/56 [==============================] - 26s 473ms/step - loss: 0.0753 - accuracy: 0.9990 - val_loss: 0.0716 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 399/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9991\n","Epoch 399: val_loss did not improve from 0.07133\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0731 - accuracy: 0.9991 - val_loss: 0.0715 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 400/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9991\n","Epoch 400: val_loss improved from 0.07133 to 0.07129, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0721 - accuracy: 0.9991 - val_loss: 0.0713 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 401/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9990\n","Epoch 401: val_loss improved from 0.07129 to 0.07121, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0752 - accuracy: 0.9990 - val_loss: 0.0712 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 402/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9991\n","Epoch 402: val_loss did not improve from 0.07121\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0737 - accuracy: 0.9991 - val_loss: 0.0713 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 403/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9991\n","Epoch 403: val_loss did not improve from 0.07121\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0723 - accuracy: 0.9991 - val_loss: 0.0712 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 404/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9990\n","Epoch 404: val_loss improved from 0.07121 to 0.07086, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 484ms/step - loss: 0.0721 - accuracy: 0.9990 - val_loss: 0.0709 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 405/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9991\n","Epoch 405: val_loss improved from 0.07086 to 0.07078, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0736 - accuracy: 0.9991 - val_loss: 0.0708 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 406/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9990\n","Epoch 406: val_loss improved from 0.07078 to 0.07067, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 474ms/step - loss: 0.0736 - accuracy: 0.9990 - val_loss: 0.0707 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 407/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9990\n","Epoch 407: val_loss did not improve from 0.07067\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0722 - accuracy: 0.9990 - val_loss: 0.0710 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 408/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9991\n","Epoch 408: val_loss did not improve from 0.07067\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0759 - accuracy: 0.9991 - val_loss: 0.0711 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 409/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9990\n","Epoch 409: val_loss did not improve from 0.07067\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0741 - accuracy: 0.9990 - val_loss: 0.0709 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 410/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9989\n","Epoch 410: val_loss did not improve from 0.07067\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0754 - accuracy: 0.9989 - val_loss: 0.0709 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 411/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9990\n","Epoch 411: val_loss did not improve from 0.07067\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0756 - accuracy: 0.9990 - val_loss: 0.0713 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 412/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9990\n","Epoch 412: val_loss did not improve from 0.07067\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0726 - accuracy: 0.9990 - val_loss: 0.0712 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 413/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9990\n","Epoch 413: val_loss did not improve from 0.07067\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0738 - accuracy: 0.9990 - val_loss: 0.0711 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 414/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9991\n","Epoch 414: val_loss did not improve from 0.07067\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0727 - accuracy: 0.9991 - val_loss: 0.0709 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 415/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9991\n","Epoch 415: val_loss did not improve from 0.07067\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0715 - accuracy: 0.9991 - val_loss: 0.0707 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 416/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9991\n","Epoch 416: val_loss improved from 0.07067 to 0.07055, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0707 - accuracy: 0.9991 - val_loss: 0.0705 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 417/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9991\n","Epoch 417: val_loss improved from 0.07055 to 0.07054, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0729 - accuracy: 0.9991 - val_loss: 0.0705 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 418/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9991\n","Epoch 418: val_loss improved from 0.07054 to 0.07019, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0729 - accuracy: 0.9991 - val_loss: 0.0702 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 419/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9991\n","Epoch 419: val_loss did not improve from 0.07019\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0707 - accuracy: 0.9991 - val_loss: 0.0703 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 420/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9991\n","Epoch 420: val_loss improved from 0.07019 to 0.07013, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0742 - accuracy: 0.9991 - val_loss: 0.0701 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 421/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9991\n","Epoch 421: val_loss did not improve from 0.07013\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0744 - accuracy: 0.9991 - val_loss: 0.0702 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 422/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9991\n","Epoch 422: val_loss did not improve from 0.07013\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0722 - accuracy: 0.9991 - val_loss: 0.0703 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 423/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9991\n","Epoch 423: val_loss improved from 0.07013 to 0.06998, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0721 - accuracy: 0.9991 - val_loss: 0.0700 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 424/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9991\n","Epoch 424: val_loss did not improve from 0.06998\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0723 - accuracy: 0.9991 - val_loss: 0.0703 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 425/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9991\n","Epoch 425: val_loss did not improve from 0.06998\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0731 - accuracy: 0.9991 - val_loss: 0.0704 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 426/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9991\n","Epoch 426: val_loss did not improve from 0.06998\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0717 - accuracy: 0.9991 - val_loss: 0.0702 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 427/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9991\n","Epoch 427: val_loss did not improve from 0.06998\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0738 - accuracy: 0.9991 - val_loss: 0.0703 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 428/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9991\n","Epoch 428: val_loss did not improve from 0.06998\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0709 - accuracy: 0.9991 - val_loss: 0.0700 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 429/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9991\n","Epoch 429: val_loss improved from 0.06998 to 0.06992, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0726 - accuracy: 0.9991 - val_loss: 0.0699 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 430/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9991\n","Epoch 430: val_loss did not improve from 0.06992\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0720 - accuracy: 0.9991 - val_loss: 0.0699 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 431/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9992\n","Epoch 431: val_loss did not improve from 0.06992\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0702 - accuracy: 0.9992 - val_loss: 0.0700 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 432/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9991\n","Epoch 432: val_loss improved from 0.06992 to 0.06965, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0735 - accuracy: 0.9991 - val_loss: 0.0696 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 433/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9991\n","Epoch 433: val_loss did not improve from 0.06965\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0729 - accuracy: 0.9991 - val_loss: 0.0698 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 434/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9991\n","Epoch 434: val_loss did not improve from 0.06965\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0728 - accuracy: 0.9991 - val_loss: 0.0698 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 435/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9991\n","Epoch 435: val_loss did not improve from 0.06965\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0742 - accuracy: 0.9991 - val_loss: 0.0698 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 436/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9991\n","Epoch 436: val_loss did not improve from 0.06965\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0723 - accuracy: 0.9991 - val_loss: 0.0698 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 437/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9991\n","Epoch 437: val_loss did not improve from 0.06965\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0721 - accuracy: 0.9991 - val_loss: 0.0698 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 438/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9992\n","Epoch 438: val_loss did not improve from 0.06965\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0715 - accuracy: 0.9992 - val_loss: 0.0698 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 439/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9992\n","Epoch 439: val_loss did not improve from 0.06965\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0722 - accuracy: 0.9992 - val_loss: 0.0697 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 440/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9992\n","Epoch 440: val_loss did not improve from 0.06965\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0705 - accuracy: 0.9992 - val_loss: 0.0698 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 441/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9991\n","Epoch 441: val_loss improved from 0.06965 to 0.06939, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 482ms/step - loss: 0.0714 - accuracy: 0.9991 - val_loss: 0.0694 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 442/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9992\n","Epoch 442: val_loss did not improve from 0.06939\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0722 - accuracy: 0.9992 - val_loss: 0.0696 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 443/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9991\n","Epoch 443: val_loss did not improve from 0.06939\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0728 - accuracy: 0.9991 - val_loss: 0.0695 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 444/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9992\n","Epoch 444: val_loss did not improve from 0.06939\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0697 - accuracy: 0.9992 - val_loss: 0.0695 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 445/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9992\n","Epoch 445: val_loss improved from 0.06939 to 0.06938, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0718 - accuracy: 0.9992 - val_loss: 0.0694 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 446/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9991\n","Epoch 446: val_loss improved from 0.06938 to 0.06915, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0708 - accuracy: 0.9991 - val_loss: 0.0691 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 447/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9992\n","Epoch 447: val_loss did not improve from 0.06915\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0721 - accuracy: 0.9992 - val_loss: 0.0694 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 448/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9992\n","Epoch 448: val_loss did not improve from 0.06915\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0725 - accuracy: 0.9992 - val_loss: 0.0695 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 449/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9991\n","Epoch 449: val_loss improved from 0.06915 to 0.06906, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0729 - accuracy: 0.9991 - val_loss: 0.0691 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 450/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9991\n","Epoch 450: val_loss did not improve from 0.06906\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0739 - accuracy: 0.9991 - val_loss: 0.0691 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 451/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9992\n","Epoch 451: val_loss did not improve from 0.06906\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0709 - accuracy: 0.9992 - val_loss: 0.0694 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 452/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9992\n","Epoch 452: val_loss improved from 0.06906 to 0.06897, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 482ms/step - loss: 0.0709 - accuracy: 0.9992 - val_loss: 0.0690 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 453/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9991\n","Epoch 453: val_loss improved from 0.06897 to 0.06881, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0727 - accuracy: 0.9991 - val_loss: 0.0688 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 454/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9992\n","Epoch 454: val_loss did not improve from 0.06881\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0731 - accuracy: 0.9992 - val_loss: 0.0693 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 455/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9992\n","Epoch 455: val_loss did not improve from 0.06881\n","56/56 [==============================] - 26s 470ms/step - loss: 0.0717 - accuracy: 0.9992 - val_loss: 0.0692 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 456/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9992\n","Epoch 456: val_loss did not improve from 0.06881\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0727 - accuracy: 0.9992 - val_loss: 0.0692 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 457/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9992\n","Epoch 457: val_loss improved from 0.06881 to 0.06873, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0722 - accuracy: 0.9992 - val_loss: 0.0687 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 458/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9992\n","Epoch 458: val_loss did not improve from 0.06873\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0695 - accuracy: 0.9992 - val_loss: 0.0690 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 459/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9992\n","Epoch 459: val_loss did not improve from 0.06873\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0700 - accuracy: 0.9992 - val_loss: 0.0689 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 460/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9992\n","Epoch 460: val_loss did not improve from 0.06873\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0714 - accuracy: 0.9992 - val_loss: 0.0688 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 461/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9992\n","Epoch 461: val_loss did not improve from 0.06873\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0716 - accuracy: 0.9992 - val_loss: 0.0688 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 462/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9992\n","Epoch 462: val_loss did not improve from 0.06873\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0717 - accuracy: 0.9992 - val_loss: 0.0689 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 463/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9992\n","Epoch 463: val_loss did not improve from 0.06873\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0712 - accuracy: 0.9992 - val_loss: 0.0690 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 464/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9992\n","Epoch 464: val_loss improved from 0.06873 to 0.06866, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0716 - accuracy: 0.9992 - val_loss: 0.0687 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 465/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9992\n","Epoch 465: val_loss improved from 0.06866 to 0.06846, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0703 - accuracy: 0.9992 - val_loss: 0.0685 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 466/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9991\n","Epoch 466: val_loss did not improve from 0.06846\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0730 - accuracy: 0.9991 - val_loss: 0.0688 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 467/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9992\n","Epoch 467: val_loss did not improve from 0.06846\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0686 - accuracy: 0.9992 - val_loss: 0.0686 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 468/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9992\n","Epoch 468: val_loss did not improve from 0.06846\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0713 - accuracy: 0.9992 - val_loss: 0.0685 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 469/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9992\n","Epoch 469: val_loss did not improve from 0.06846\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0700 - accuracy: 0.9992 - val_loss: 0.0685 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 470/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9992\n","Epoch 470: val_loss did not improve from 0.06846\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0717 - accuracy: 0.9992 - val_loss: 0.0686 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 471/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9992\n","Epoch 471: val_loss improved from 0.06846 to 0.06841, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0697 - accuracy: 0.9992 - val_loss: 0.0684 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 472/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9992\n","Epoch 472: val_loss improved from 0.06841 to 0.06838, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 470ms/step - loss: 0.0695 - accuracy: 0.9992 - val_loss: 0.0684 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 473/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9992\n","Epoch 473: val_loss did not improve from 0.06838\n","56/56 [==============================] - 26s 462ms/step - loss: 0.0703 - accuracy: 0.9992 - val_loss: 0.0686 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 474/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9992\n","Epoch 474: val_loss improved from 0.06838 to 0.06820, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 481ms/step - loss: 0.0691 - accuracy: 0.9992 - val_loss: 0.0682 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 475/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9992\n","Epoch 475: val_loss improved from 0.06820 to 0.06800, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0692 - accuracy: 0.9992 - val_loss: 0.0680 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 476/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9992\n","Epoch 476: val_loss did not improve from 0.06800\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0701 - accuracy: 0.9992 - val_loss: 0.0681 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 477/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9992\n","Epoch 477: val_loss did not improve from 0.06800\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0737 - accuracy: 0.9992 - val_loss: 0.0681 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 478/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9992\n","Epoch 478: val_loss did not improve from 0.06800\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0698 - accuracy: 0.9992 - val_loss: 0.0680 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 479/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9993\n","Epoch 479: val_loss did not improve from 0.06800\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0694 - accuracy: 0.9993 - val_loss: 0.0681 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 480/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9992\n","Epoch 480: val_loss did not improve from 0.06800\n","56/56 [==============================] - 26s 458ms/step - loss: 0.0701 - accuracy: 0.9992 - val_loss: 0.0682 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 481/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9992\n","Epoch 481: val_loss improved from 0.06800 to 0.06790, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0698 - accuracy: 0.9992 - val_loss: 0.0679 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 482/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9993\n","Epoch 482: val_loss did not improve from 0.06790\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0697 - accuracy: 0.9993 - val_loss: 0.0680 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 483/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9993\n","Epoch 483: val_loss did not improve from 0.06790\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0691 - accuracy: 0.9993 - val_loss: 0.0679 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 484/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9993\n","Epoch 484: val_loss improved from 0.06790 to 0.06774, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 474ms/step - loss: 0.0693 - accuracy: 0.9993 - val_loss: 0.0677 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 485/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9992\n","Epoch 485: val_loss improved from 0.06774 to 0.06748, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0708 - accuracy: 0.9992 - val_loss: 0.0675 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 486/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9992\n","Epoch 486: val_loss did not improve from 0.06748\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0697 - accuracy: 0.9992 - val_loss: 0.0677 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 487/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9992\n","Epoch 487: val_loss did not improve from 0.06748\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0717 - accuracy: 0.9992 - val_loss: 0.0676 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 488/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9993\n","Epoch 488: val_loss did not improve from 0.06748\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0677 - accuracy: 0.9993 - val_loss: 0.0677 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 489/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9993\n","Epoch 489: val_loss did not improve from 0.06748\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0705 - accuracy: 0.9993 - val_loss: 0.0678 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 490/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9993\n","Epoch 490: val_loss did not improve from 0.06748\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0699 - accuracy: 0.9993 - val_loss: 0.0678 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 491/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9992\n","Epoch 491: val_loss did not improve from 0.06748\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0719 - accuracy: 0.9992 - val_loss: 0.0677 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 492/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9993\n","Epoch 492: val_loss did not improve from 0.06748\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0704 - accuracy: 0.9993 - val_loss: 0.0677 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 493/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9993\n","Epoch 493: val_loss did not improve from 0.06748\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0688 - accuracy: 0.9993 - val_loss: 0.0676 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 494/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9993\n","Epoch 494: val_loss did not improve from 0.06748\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0668 - accuracy: 0.9993 - val_loss: 0.0676 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 495/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9993\n","Epoch 495: val_loss improved from 0.06748 to 0.06736, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 470ms/step - loss: 0.0707 - accuracy: 0.9993 - val_loss: 0.0674 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 496/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9993\n","Epoch 496: val_loss did not improve from 0.06736\n","56/56 [==============================] - 26s 459ms/step - loss: 0.0708 - accuracy: 0.9993 - val_loss: 0.0674 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 497/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9993\n","Epoch 497: val_loss did not improve from 0.06736\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0685 - accuracy: 0.9993 - val_loss: 0.0674 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 498/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9993\n","Epoch 498: val_loss did not improve from 0.06736\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0696 - accuracy: 0.9993 - val_loss: 0.0674 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 499/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9993\n","Epoch 499: val_loss improved from 0.06736 to 0.06726, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0684 - accuracy: 0.9993 - val_loss: 0.0673 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 500/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9993\n","Epoch 500: val_loss improved from 0.06726 to 0.06716, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 482ms/step - loss: 0.0710 - accuracy: 0.9993 - val_loss: 0.0672 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 501/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9993\n","Epoch 501: val_loss did not improve from 0.06716\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0721 - accuracy: 0.9993 - val_loss: 0.0673 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 502/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9993\n","Epoch 502: val_loss did not improve from 0.06716\n","56/56 [==============================] - 26s 470ms/step - loss: 0.0689 - accuracy: 0.9993 - val_loss: 0.0674 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 503/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9993\n","Epoch 503: val_loss did not improve from 0.06716\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0710 - accuracy: 0.9993 - val_loss: 0.0673 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 504/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9993\n","Epoch 504: val_loss improved from 0.06716 to 0.06708, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 483ms/step - loss: 0.0703 - accuracy: 0.9993 - val_loss: 0.0671 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 505/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9993\n","Epoch 505: val_loss did not improve from 0.06708\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0693 - accuracy: 0.9993 - val_loss: 0.0672 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 506/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9993\n","Epoch 506: val_loss improved from 0.06708 to 0.06701, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0673 - accuracy: 0.9993 - val_loss: 0.0670 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 507/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9993\n","Epoch 507: val_loss improved from 0.06701 to 0.06580, saving model to unet2_bce.hdf5\n","56/56 [==============================] - 27s 482ms/step - loss: 0.0729 - accuracy: 0.9993 - val_loss: 0.0658 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 508/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9993\n","Epoch 508: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0707 - accuracy: 0.9993 - val_loss: 0.0670 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 509/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9993\n","Epoch 509: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0681 - accuracy: 0.9993 - val_loss: 0.0672 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 510/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9993\n","Epoch 510: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0692 - accuracy: 0.9993 - val_loss: 0.0666 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 511/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9993\n","Epoch 511: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0713 - accuracy: 0.9993 - val_loss: 0.0672 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 512/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9993\n","Epoch 512: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0700 - accuracy: 0.9993 - val_loss: 0.0674 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 513/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9993\n","Epoch 513: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0704 - accuracy: 0.9993 - val_loss: 0.0671 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 514/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9993\n","Epoch 514: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0674 - accuracy: 0.9993 - val_loss: 0.0670 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 515/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9993\n","Epoch 515: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0679 - accuracy: 0.9993 - val_loss: 0.0669 - val_accuracy: 0.9977 - lr: 1.0000e-05\n","Epoch 516/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9993\n","Epoch 516: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0689 - accuracy: 0.9993 - val_loss: 0.0665 - val_accuracy: 0.9976 - lr: 1.0000e-05\n","Epoch 517/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9993\n","Epoch 517: val_loss did not improve from 0.06580\n","\n","Epoch 517: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n","56/56 [==============================] - 26s 461ms/step - loss: 0.0681 - accuracy: 0.9993 - val_loss: 0.0667 - val_accuracy: 0.9977 - lr: 1.0000e-05\n","Epoch 518/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9993\n","Epoch 518: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0671 - accuracy: 0.9993 - val_loss: 0.0664 - val_accuracy: 0.9977 - lr: 1.0000e-06\n","Epoch 519/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9994\n","Epoch 519: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0662 - accuracy: 0.9994 - val_loss: 0.0663 - val_accuracy: 0.9977 - lr: 1.0000e-06\n","Epoch 520/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9994\n","Epoch 520: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 471ms/step - loss: 0.0675 - accuracy: 0.9994 - val_loss: 0.0663 - val_accuracy: 0.9977 - lr: 1.0000e-06\n","Epoch 521/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9993\n","Epoch 521: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 472ms/step - loss: 0.0690 - accuracy: 0.9993 - val_loss: 0.0664 - val_accuracy: 0.9977 - lr: 1.0000e-06\n","Epoch 522/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9993\n","Epoch 522: val_loss did not improve from 0.06580\n","56/56 [==============================] - 26s 460ms/step - loss: 0.0691 - accuracy: 0.9993 - val_loss: 0.0664 - val_accuracy: 0.9977 - lr: 1.0000e-06\n","Epoch 522: early stopping\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f3076cf5790>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Train model\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr=ReduceLROnPlateau(monitor='val_loss',\n","                         factor=0.1,\n","                         patience=10,\n","                         verbose=1,\n","                         mode='auto',\n","                         min_delta=0.00003,\n","                         cooldown=0,\n","                         min_lr=0)\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15,verbose=1,mode='min')\n","save_model= ModelCheckpoint('unet2_bce.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","unet2.fit(images, masks, validation_data=(val_images,val_masks), batch_size=16, epochs=1000,verbose=1,shuffle=True,callbacks=[save_model,reduce_lr,early_stop])"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T22:15:57.360982Z","iopub.status.busy":"2023-04-03T22:15:57.360611Z","iopub.status.idle":"2023-04-03T22:15:57.370289Z","shell.execute_reply":"2023-04-03T22:15:57.369098Z","shell.execute_reply.started":"2023-04-03T22:15:57.360948Z"},"trusted":true},"outputs":[],"source":["np.save('unet2_bce-history.npy',unet2.history.history)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T22:16:00.105558Z","iopub.status.busy":"2023-04-03T22:16:00.105195Z","iopub.status.idle":"2023-04-03T22:16:00.111466Z","shell.execute_reply":"2023-04-03T22:16:00.110318Z","shell.execute_reply.started":"2023-04-03T22:16:00.105524Z"},"trusted":true},"outputs":[],"source":["model_history = np.load('unet2_bce-history.npy', allow_pickle='TRUE').item()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T22:16:02.775724Z","iopub.status.busy":"2023-04-03T22:16:02.774719Z","iopub.status.idle":"2023-04-03T22:16:03.248917Z","shell.execute_reply":"2023-04-03T22:16:03.247955Z","shell.execute_reply.started":"2023-04-03T22:16:02.775647Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<matplotlib.legend.Legend at 0x7f3069926190>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#@title Plot accuracy and loss \n","from matplotlib import pyplot as plt\n","## Accuracy\n","plt.plot(model_history['accuracy'])\n","plt.plot(model_history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')\n","plt.show()\n","\n","## Loss\n","plt.plot(model_history['loss'])\n","plt.plot(model_history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T22:16:53.944559Z","iopub.status.busy":"2023-04-03T22:16:53.944192Z","iopub.status.idle":"2023-04-03T22:17:17.711415Z","shell.execute_reply":"2023-04-03T22:17:17.710141Z","shell.execute_reply.started":"2023-04-03T22:16:53.944527Z"},"trusted":true},"outputs":[],"source":["## Test images\n","test_dir=r'/content/gdrive/MyDrive/mudtest/'\n","test_images_list = os.listdir(r\"{}/images/\".format(test_dir))\n","test_masks_list = []\n","test_images = []\n","for n in test_images_list:\n","  test_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}/images/{}\".format(test_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  test_images.append(a)\n","\n","## Test masks\n","test_masks = []\n","for n in test_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}/labels/{}\".format(test_dir,n))))\n","  test_masks.append(a)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T22:17:42.384697Z","iopub.status.busy":"2023-04-03T22:17:42.383771Z","iopub.status.idle":"2023-04-03T22:17:42.537348Z","shell.execute_reply":"2023-04-03T22:17:42.536199Z","shell.execute_reply.started":"2023-04-03T22:17:42.384646Z"},"trusted":true},"outputs":[],"source":["for i in range(len(test_images)):\n","  test_images[i] = test_images[i].astype('float32')\n","  test_images[i] = test_images[i].T\n","\n","for i in range(len(test_masks)):\n","  test_masks[i] = test_masks[i].reshape(1,256,256,1)\n","  test_masks[i] = test_masks[i].T\n","for i in range(len(test_images)):\n","  test_images[i] = test_images[i].reshape(-1,256,256,10)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T22:17:45.032213Z","iopub.status.busy":"2023-04-03T22:17:45.031084Z","iopub.status.idle":"2023-04-03T22:17:45.040694Z","shell.execute_reply":"2023-04-03T22:17:45.039716Z","shell.execute_reply.started":"2023-04-03T22:17:45.032160Z"},"trusted":true},"outputs":[],"source":["#@title Returns an image or array plot of mask prediction\n","\n","def reconstruct_image(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","  # Standardise between 0-1\n","  reconstruction = reconstruction/np.max(reconstruction)\n","\n","  # Round to 0-1, binary pixel-by-pixel classification \n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  # Plot reconstructed mask (prediction)\n","  plt.imshow(reconstruction) \n","'''\n","  Returns array of mask prediction, given model and image\n","'''\n","def reconstruct_array(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","\n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  return reconstruction # Returns array"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T22:17:48.178894Z","iopub.status.busy":"2023-04-03T22:17:48.178527Z","iopub.status.idle":"2023-04-03T22:17:48.197782Z","shell.execute_reply":"2023-04-03T22:17:48.196754Z","shell.execute_reply.started":"2023-04-03T22:17:48.178860Z"},"trusted":true},"outputs":[],"source":["#@title Metric functions for evaluation\n","def accuracy_eval(model, image, mask): # Gives score of mask vs prediction\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","    return accuracy_score(mask.flatten(), reconstruction)\n","\n","  else: # If a list of images input, find accuracy for each\n","    accuracy = []\n","    for i in range(len(image)):\n","      reconstruction = model.predict(image[i].reshape(1, 256, 256, 10))\n","      reconstruction = np.round(reconstruction).flatten()\n","      accuracy.append(accuracy_score(mask[i].flatten(), reconstruction))\n","    return accuracy\n","\n","def recall_eval(model, image, mask): # Find recall score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return recall_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    recall = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        recall.append(recall_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return recall\n","\n","def precision_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return precision_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    precision = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        precision.append(precision_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return precision\n","\n","def iou_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return jaccard_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    iou = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        iou.append(jaccard_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return iou\n","\n","def f1_score_eval_basic(precision, recall):\n","    prec = np.mean(precision)\n","    rec = np.mean(recall)\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def produce_mask(image): # Outputs rounded image (binary)\n","  return np.round(image)\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T22:17:52.871236Z","iopub.status.busy":"2023-04-03T22:17:52.870159Z","iopub.status.idle":"2023-04-03T22:19:29.081982Z","shell.execute_reply":"2023-04-03T22:19:29.079759Z","shell.execute_reply.started":"2023-04-03T22:17:52.871188Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 52ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 52ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 51ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 59ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 28ms/step\n"]}],"source":["accuracy = (accuracy_eval(unet2, test_images, test_masks))\n","precision = (precision_eval(unet2, test_images, test_masks))\n","recall = (recall_eval(unet2, test_images, test_masks))\n","iou = (iou_eval(unet2, test_images, test_masks))"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T22:19:41.954874Z","iopub.status.busy":"2023-04-03T22:19:41.954502Z","iopub.status.idle":"2023-04-03T22:19:41.961069Z","shell.execute_reply":"2023-04-03T22:19:41.959761Z","shell.execute_reply.started":"2023-04-03T22:19:41.954839Z"},"trusted":true},"outputs":[],"source":["f1_score = (f1_score_eval_basic(precision, recall))"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T22:19:45.193822Z","iopub.status.busy":"2023-04-03T22:19:45.192909Z","iopub.status.idle":"2023-04-03T22:19:45.203758Z","shell.execute_reply":"2023-04-03T22:19:45.202335Z","shell.execute_reply.started":"2023-04-03T22:19:45.193781Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model accuracy:  0.9852966079138276 0.012789758625299286\n","model precision:  0.9639441090051364 0.0540290313587763\n","model recall:  0.9682437283521315 0.053439079394282635\n","model F1-score:  0.966089134794228\n","model iou:  0.9352748686861707\n"]}],"source":["\n","# Print score eval results for each model\n","print('model accuracy: ', np.mean(accuracy), np.std(accuracy))\n","# Print precision eval results for each model\n","print('model precision: ', np.mean(precision), np.std(precision))\n","# Print recall eval results for each model\n","print('model recall: ', np.mean(recall), np.std(recall))\n","# Print f1-score eval results for each model\n","print('model F1-score: ', np.mean(f1_score))\n","print('model iou: ', np.mean(iou))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
