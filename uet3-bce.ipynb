{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-03T15:46:01.535832Z","iopub.status.busy":"2023-04-03T15:46:01.535139Z","iopub.status.idle":"2023-04-03T15:46:37.309357Z","shell.execute_reply":"2023-04-03T15:46:37.308300Z","shell.execute_reply.started":"2023-04-03T15:46:01.535797Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/davej23/image-segmentation-keras.git\n","  Cloning https://github.com/davej23/image-segmentation-keras.git to /tmp/pip-req-build-1p7m729z\n","  Running command git clone --filter=blob:none --quiet https://github.com/davej23/image-segmentation-keras.git /tmp/pip-req-build-1p7m729z\n","  Resolved https://github.com/davej23/image-segmentation-keras.git to commit e01b0a8d5859854cd9d259a618829889166439f5\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting rarfile\n","  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n","Collecting segmentation-models\n","  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n","Collecting rioxarray\n","  Downloading rioxarray-0.9.1.tar.gz (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting keras-applications<=1.0.8,>=1.0.7\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting image-classifiers==1.0.0\n","  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n","Collecting efficientnet==1.0.0\n","  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n","Collecting h5py<=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: Keras>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (2.11.0)\n","Collecting imageio==2.5.0\n","  Downloading imageio-2.5.0-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (0.4.0)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.5.4.60)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.64.1)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (9.4.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (1.21.6)\n","Requirement already satisfied: xarray>=0.17 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (0.20.2)\n","Requirement already satisfied: rasterio in /opt/conda/lib/python3.7/site-packages (from rioxarray) (1.2.10)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from rioxarray) (23.0)\n","Requirement already satisfied: pyproj>=2.2 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (3.1.0)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py<=2.10.0->keras-segmentation==0.3.0) (1.16.0)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (3.5.3)\n","Requirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.8.0)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.7.3)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from pyproj>=2.2->rioxarray) (2022.12.7)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.11.4)\n","Requirement already satisfied: pandas>=1.1 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (1.3.5)\n","Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.4.0)\n","Requirement already satisfied: click-plugins in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.1.1)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (59.8.0)\n","Requirement already satisfied: affine in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (2.4.0)\n","Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.4.7)\n","Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (8.1.3)\n","Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (0.7.2)\n","Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (22.2.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2023.2)\n","Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.11.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.0)\n","Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio->rioxarray) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->xarray>=0.17->rioxarray) (3.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (4.38.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (0.11.0)\n","Building wheels for collected packages: keras-segmentation, rioxarray\n","  Building wheel for keras-segmentation (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for keras-segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34377 sha256=70396648a7a6a8512453e33314a7c8fba57d8ccca0d32e3708917d29c65df7f4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-lu1toq3q/wheels/f4/fb/07/8f81ceb3d9fe936f5e4dcd1a64cbc489e42e6e7f9c2f166785\n","  Building wheel for rioxarray (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rioxarray: filename=rioxarray-0.9.1-py3-none-any.whl size=54590 sha256=8487baeadf7ae9aa1b20ad1a7b1c0bf99b04ea2e6d5c19fed413e18ba2ac1677\n","  Stored in directory: /root/.cache/pip/wheels/03/b2/26/2e2cc1797ac99cc070d2cae87c340bd3429bbb583c90b1c780\n","Successfully built keras-segmentation rioxarray\n","Installing collected packages: rarfile, imageio, h5py, keras-applications, image-classifiers, efficientnet, segmentation-models, keras-segmentation, rioxarray\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.25.0\n","    Uninstalling imageio-2.25.0:\n","      Successfully uninstalled imageio-2.25.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n","tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed efficientnet-1.0.0 h5py-2.10.0 image-classifiers-1.0.0 imageio-2.5.0 keras-applications-1.0.8 keras-segmentation-0.3.0 rarfile-4.0 rioxarray-0.9.1 segmentation-models-1.0.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#@title import packages\n","import keras\n","import numpy as np\n","import os\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.losses import *\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint\n","import sys\n","\n","!pip install rarfile segmentation-models git+https://github.com/davej23/image-segmentation-keras.git rioxarray\n","from rarfile import RarFile\n","from sklearn.metrics import *\n","import rioxarray as rxr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:46:37.311777Z","iopub.status.busy":"2023-04-03T15:46:37.311319Z","iopub.status.idle":"2023-04-03T15:48:20.258225Z","shell.execute_reply":"2023-04-03T15:48:20.257164Z","shell.execute_reply.started":"2023-04-03T15:46:37.311738Z"},"trusted":true},"outputs":[],"source":["base_dir = r\"/content/gdrive/MyDrive/mudtrain/\"\n","#@title Read training images and normalise\n","training_images_list = os.listdir(r\"{}train/images/\".format(base_dir))\n","training_masks_list = []\n","training_images = []\n","for n in training_images_list:\n","  training_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}train/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  training_images.append(a)\n","\n","## Training masks\n","training_masks = []\n","for n in training_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}train/labels/{}\".format(base_dir,n))))\n","  training_masks.append(a)\n","\n","\n","## Validation images\n","validation_images_list = os.listdir(r\"{}val/images/\".format(base_dir))\n","validation_masks_list = []\n","validation_images = []\n","for n in validation_images_list:\n","  validation_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}val/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  validation_images.append(a)\n","\n","## Validation masks\n","validation_masks = []\n","for n in validation_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}val/labels/{}\".format(base_dir,n))))\n","  validation_masks.append(a)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:49:50.129423Z","iopub.status.busy":"2023-04-03T15:49:50.128988Z","iopub.status.idle":"2023-04-03T15:49:50.729208Z","shell.execute_reply":"2023-04-03T15:49:50.728155Z","shell.execute_reply.started":"2023-04-03T15:49:50.129388Z"},"trusted":true},"outputs":[],"source":["#@title Pre-process data, reshaping and transposing\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].astype('float32')\n","  training_images[i] = training_images[i].T\n","\n","for i in range(len(training_masks)):\n","  training_masks[i] = training_masks[i].reshape(1,256,256)\n","  training_masks[i] = training_masks[i].T\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].astype('float32')\n","  validation_images[i] = validation_images[i].T\n","\n","for i in range(len(validation_masks)):\n","  validation_masks[i] = validation_masks[i].reshape(1,256,256)\n","  validation_masks[i] = validation_masks[i].T\n","\n","\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].reshape(256,256,10)\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].reshape(256,256,10)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:49:54.027490Z","iopub.status.busy":"2023-04-03T15:49:54.026547Z","iopub.status.idle":"2023-04-03T15:49:58.800186Z","shell.execute_reply":"2023-04-03T15:49:58.799163Z","shell.execute_reply.started":"2023-04-03T15:49:54.027438Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 10)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["images=np.vstack([training_images])\n","val_images=np.vstack([validation_images])\n","images.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:49:58.802723Z","iopub.status.busy":"2023-04-03T15:49:58.802029Z","iopub.status.idle":"2023-04-03T15:49:58.962400Z","shell.execute_reply":"2023-04-03T15:49:58.961251Z","shell.execute_reply.started":"2023-04-03T15:49:58.802683Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 1)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["masks=np.vstack([training_masks])\n","val_masks=np.vstack([validation_masks])\n","val_masks.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:50:01.940807Z","iopub.status.busy":"2023-04-03T15:50:01.940212Z","iopub.status.idle":"2023-04-03T15:50:02.176822Z","shell.execute_reply":"2023-04-03T15:50:02.175882Z","shell.execute_reply.started":"2023-04-03T15:50:01.940771Z"},"trusted":true},"outputs":[{"data":{"text/plain":["904"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del training_images,validation_images,training_masks,validation_masks,test_images,test_masks,test_images_list,\n","training_images_list,validation_images_list,\n","training_masks_list,validation_masks_list\n","gc.collect()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T20:30:07.118762Z","iopub.status.busy":"2023-04-03T20:30:07.118391Z","iopub.status.idle":"2023-04-03T20:30:07.505564Z","shell.execute_reply":"2023-04-03T20:30:07.503862Z","shell.execute_reply.started":"2023-04-03T20:30:07.118726Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'images' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/761246598.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"]}],"source":["del images,masks,val_images,val_masks"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:50:07.129347Z","iopub.status.busy":"2023-04-03T15:50:07.128930Z","iopub.status.idle":"2023-04-03T15:50:07.449993Z","shell.execute_reply":"2023-04-03T15:50:07.448962Z","shell.execute_reply.started":"2023-04-03T15:50:07.129311Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import models, layers, regularizers\n","from tensorflow.keras import backend as K\n","\n","#convolutional block\n","\n","def conv_block(x,filters, kernelsize=3, dropout=0, batchnorm=True): \n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    if dropout > 0:\n","        conv = layers.Dropout(dropout)(conv)\n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(conv)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    return conv\n","\n","def conv_block_one(x,filters, kernelsize=3, batchnorm=True): \n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    return conv\n","\n","def unet3plus(input_shape):\n","\n","    filters = [32,64, 128, 256, 512]\n","    kernelsize = 3\n","    inputs =layers.Input(input_shape) \n","    \n","    \"\"\" Encoder\"\"\"\n","    # block 1\n","    e1 = conv_block(inputs, filters[0])\n","\n","    # block 2\n","    e2 = layers.MaxPool2D(pool_size=(2, 2))(e1) \n","    e2 = conv_block(e2, filters[1]) \n","\n","    # block 3\n","    e3 = layers.MaxPool2D(pool_size=(2, 2))(e2) \n","    e3 = conv_block(e3, filters[2])\n","\n","    # block 4\n","    e4 = layers.MaxPool2D(pool_size=(2, 2))(e3) \n","    e4 = conv_block(e4, filters[3]) \n","\n","    # block 5\n","    # bottleneck layer\n","    e5 = layers.MaxPool2D(pool_size=(2, 2))(e4) \n","    e5 = conv_block(e5, filters[4])  \n","\n","    \"\"\" Decoder \"\"\"\n","    cat_channels = filters[0]\n","    cat_blocks = len(filters)\n","    upsample_channels = cat_blocks * cat_channels\n","\n","    \"\"\" d4 \"\"\"\n","    e1_d4 = layers.MaxPool2D(pool_size=(8, 8))(e1)  \n","    e1_d4 = conv_block_one(e1_d4, cat_channels) \n","\n","    e2_d4 = layers.MaxPool2D(pool_size=(4, 4))(e2)  \n","    e2_d4 = conv_block_one(e2_d4, cat_channels)  \n","\n","    e3_d4 = layers.MaxPool2D(pool_size=(2, 2))(e3)  \n","    e3_d4 = conv_block_one(e3_d4, cat_channels) \n","\n","    e4_d4 = conv_block_one(e4, cat_channels) \n","\n","    e5_d4 = layers.UpSampling2D(size=(2, 2))(e5) \n","    e5_d4 = conv_block_one(e5_d4, cat_channels)  \n","\n","    d4 = layers.concatenate([e1_d4, e2_d4, e3_d4, e4_d4, e5_d4])\n","    d4 = conv_block_one(d4, upsample_channels) \n","\n","    \"\"\" d3 \"\"\"\n","    e1_d3 = layers.MaxPool2D(pool_size=(4, 4))(e1) \n","    e1_d3 = conv_block_one(e1_d3, cat_channels)  \n","\n","    e2_d3 = layers.MaxPool2D(pool_size=(2, 2))(e2) \n","    e2_d3 = conv_block_one(e2_d3, cat_channels) \n","\n","    e3_d3 = conv_block_one(e3, cat_channels) \n","\n","    e4_d3 = layers.UpSampling2D(size=(2, 2))(d4) \n","    e4_d3 = conv_block_one(e4_d3, cat_channels)\n","\n","    e5_d3 = layers.UpSampling2D(size=(4, 4))(e5) \n","    e5_d3 = conv_block_one(e5_d3, cat_channels)\n","\n","    d3 = layers.concatenate([e1_d3, e2_d3, e3_d3, e4_d3, e5_d3])\n","    d3 = conv_block_one(d3, upsample_channels) \n","\n","    \"\"\" d2 \"\"\"\n","    e1_d2 = layers.MaxPool2D(pool_size=(2, 2))(e1)\n","    e1_d2 = conv_block_one(e1_d2, cat_channels) \n","\n","    e2_d2 = conv_block_one(e2, cat_channels) \n","\n","    d3_d2 = layers.UpSampling2D(size=(2, 2))(d3)  \n","    d3_d2 = conv_block_one(d3_d2, cat_channels) \n","\n","    d4_d2 = layers.UpSampling2D(size=(4, 4))(d4) \n","    d4_d2 = conv_block_one(d4_d2, cat_channels) \n","\n","    e5_d2 = layers.UpSampling2D(size=(8, 8))(e5)\n","    e5_d2 = conv_block_one(e5_d2, cat_channels) \n","\n","    d2 = layers.concatenate([e1_d2, e2_d2, d3_d2, d4_d2, e5_d2])\n","    d2 = conv_block_one(d2, upsample_channels) \n","\n","    \"\"\" d1 \"\"\"\n","    e1_d1 = conv_block_one(e1, cat_channels)  \n","\n","    d2_d1 = layers.UpSampling2D(size=(2, 2))(d2)\n","    d2_d1 = conv_block_one(d2_d1, cat_channels)\n","\n","    d3_d1 = layers.UpSampling2D(size=(4, 4))(d3) \n","    d3_d1 = conv_block_one(d3_d1, cat_channels) \n","\n","    d4_d1 = layers.UpSampling2D(size=(8, 8))(d4) \n","    d4_d1 = conv_block_one(d4_d1, cat_channels) \n","\n","    e5_d1 = layers.UpSampling2D(size=(16, 16))(e5) \n","    e5_d1 = conv_block_one(e5_d1, cat_channels) \n","\n","    d1 = layers.concatenate([e1_d1, d2_d1, d3_d1, d4_d1, e5_d1, ])\n","    d1 = conv_block_one(d1, upsample_channels) \n","\n","    conv_final = layers.Conv2D(1, kernel_size=(1,1))( d1)\n","    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n","    outputs = layers.Activation('sigmoid')(conv_final) \n","    \n","    return tf.keras.Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:50:12.202905Z","iopub.status.busy":"2023-04-03T15:50:12.202316Z","iopub.status.idle":"2023-04-03T15:50:15.688251Z","shell.execute_reply":"2023-04-03T15:50:15.687404Z","shell.execute_reply.started":"2023-04-03T15:50:12.202867Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 256, 256, 1  0           []                               \n","                                0)]                                                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 256, 256, 32  2912        ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 256, 256, 32  128        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," activation (Activation)        (None, 256, 256, 32  0           ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 256, 256, 32  9248        ['activation[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 256, 256, 32  128        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_1 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 128, 128, 32  0           ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 128, 128, 64  18496       ['max_pooling2d[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 128, 128, 64  256        ['conv2d_2[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_2 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_2[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 128, 128, 64  36928       ['activation_2[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 128, 128, 64  256        ['conv2d_3[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_3 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_3[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 64, 64, 128)  73856       ['max_pooling2d_1[0][0]']        \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_4[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_4 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 64, 64, 128)  147584      ['activation_4[0][0]']           \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_5[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_5 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0          ['activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 32, 32, 256)  295168      ['max_pooling2d_2[0][0]']        \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_6[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_6 (Activation)      (None, 32, 32, 256)  0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 32, 32, 256)  590080      ['activation_6[0][0]']           \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_7[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_7 (Activation)      (None, 32, 32, 256)  0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0          ['activation_7[0][0]']           \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 16, 16, 512)  1180160     ['max_pooling2d_3[0][0]']        \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_8[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_8 (Activation)      (None, 16, 16, 512)  0           ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 16, 16, 512)  2359808     ['activation_8[0][0]']           \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_9[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_9 (Activation)      (None, 16, 16, 512)  0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 32)  0           ['activation_1[0][0]']           \n","                                                                                                  \n"," max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 64)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 128)  0          ['activation_5[0][0]']           \n","                                                                                                  \n"," up_sampling2d (UpSampling2D)   (None, 32, 32, 512)  0           ['activation_9[0][0]']           \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 32, 32, 32)   9248        ['max_pooling2d_4[0][0]']        \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 32, 32, 32)   18464       ['max_pooling2d_5[0][0]']        \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 32, 32, 32)   36896       ['max_pooling2d_6[0][0]']        \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 32, 32, 32)   73760       ['activation_7[0][0]']           \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 32, 32, 32)   147488      ['up_sampling2d[0][0]']          \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 32, 32, 32)  128         ['conv2d_10[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 32, 32, 32)  128         ['conv2d_11[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 32, 32, 32)  128         ['conv2d_12[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 32, 32, 32)  128         ['conv2d_13[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 32, 32, 32)  128         ['conv2d_14[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_10 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_10[0][0]'] \n","                                                                                                  \n"," activation_11 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," activation_12 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," activation_13 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," activation_14 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_14[0][0]'] \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 32, 32, 160)  0           ['activation_10[0][0]',          \n","                                                                  'activation_11[0][0]',          \n","                                                                  'activation_12[0][0]',          \n","                                                                  'activation_13[0][0]',          \n","                                                                  'activation_14[0][0]']          \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 32, 32, 160)  230560      ['concatenate[0][0]']            \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 32, 32, 160)  640        ['conv2d_15[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_15 (Activation)     (None, 32, 32, 160)  0           ['batch_normalization_15[0][0]'] \n","                                                                                                  \n"," max_pooling2d_7 (MaxPooling2D)  (None, 64, 64, 32)  0           ['activation_1[0][0]']           \n","                                                                                                  \n"," max_pooling2d_8 (MaxPooling2D)  (None, 64, 64, 64)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 160)  0          ['activation_15[0][0]']          \n","                                                                                                  \n"," up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 512)  0          ['activation_9[0][0]']           \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 64, 64, 32)   9248        ['max_pooling2d_7[0][0]']        \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 64, 64, 32)   18464       ['max_pooling2d_8[0][0]']        \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 64, 64, 32)   36896       ['activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 64, 64, 32)   46112       ['up_sampling2d_1[0][0]']        \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 64, 64, 32)   147488      ['up_sampling2d_2[0][0]']        \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 64, 64, 32)  128         ['conv2d_16[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 64, 64, 32)  128         ['conv2d_17[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 64, 64, 32)  128         ['conv2d_18[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_19 (BatchN  (None, 64, 64, 32)  128         ['conv2d_19[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_20 (BatchN  (None, 64, 64, 32)  128         ['conv2d_20[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_16 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_16[0][0]'] \n","                                                                                                  \n"," activation_17 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_17[0][0]'] \n","                                                                                                  \n"," activation_18 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_18[0][0]'] \n","                                                                                                  \n"," activation_19 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_19[0][0]'] \n","                                                                                                  \n"," activation_20 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_20[0][0]'] \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 64, 64, 160)  0           ['activation_16[0][0]',          \n","                                                                  'activation_17[0][0]',          \n","                                                                  'activation_18[0][0]',          \n","                                                                  'activation_19[0][0]',          \n","                                                                  'activation_20[0][0]']          \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 64, 64, 160)  230560      ['concatenate_1[0][0]']          \n","                                                                                                  \n"," batch_normalization_21 (BatchN  (None, 64, 64, 160)  640        ['conv2d_21[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_21 (Activation)     (None, 64, 64, 160)  0           ['batch_normalization_21[0][0]'] \n","                                                                                                  \n"," max_pooling2d_9 (MaxPooling2D)  (None, 128, 128, 32  0          ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 16  0          ['activation_21[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," up_sampling2d_4 (UpSampling2D)  (None, 128, 128, 16  0          ['activation_15[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," up_sampling2d_5 (UpSampling2D)  (None, 128, 128, 51  0          ['activation_9[0][0]']           \n","                                2)                                                                \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 128, 128, 32  9248        ['max_pooling2d_9[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 128, 128, 32  18464       ['activation_3[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_24 (Conv2D)             (None, 128, 128, 32  46112       ['up_sampling2d_3[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_25 (Conv2D)             (None, 128, 128, 32  46112       ['up_sampling2d_4[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_26 (Conv2D)             (None, 128, 128, 32  147488      ['up_sampling2d_5[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_22 (BatchN  (None, 128, 128, 32  128        ['conv2d_22[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_23 (BatchN  (None, 128, 128, 32  128        ['conv2d_23[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_24 (BatchN  (None, 128, 128, 32  128        ['conv2d_24[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_25 (BatchN  (None, 128, 128, 32  128        ['conv2d_25[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_26 (BatchN  (None, 128, 128, 32  128        ['conv2d_26[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_22 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_22[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_23 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_23[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_24 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_24[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_25 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_25[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_26 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_26[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 128, 128, 16  0           ['activation_22[0][0]',          \n","                                0)                                'activation_23[0][0]',          \n","                                                                  'activation_24[0][0]',          \n","                                                                  'activation_25[0][0]',          \n","                                                                  'activation_26[0][0]']          \n","                                                                                                  \n"," conv2d_27 (Conv2D)             (None, 128, 128, 16  230560      ['concatenate_2[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," batch_normalization_27 (BatchN  (None, 128, 128, 16  640        ['conv2d_27[0][0]']              \n"," ormalization)                  0)                                                                \n","                                                                                                  \n"," activation_27 (Activation)     (None, 128, 128, 16  0           ['batch_normalization_27[0][0]'] \n","                                0)                                                                \n","                                                                                                  \n"," up_sampling2d_6 (UpSampling2D)  (None, 256, 256, 16  0          ['activation_27[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," up_sampling2d_7 (UpSampling2D)  (None, 256, 256, 16  0          ['activation_21[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," up_sampling2d_8 (UpSampling2D)  (None, 256, 256, 16  0          ['activation_15[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," up_sampling2d_9 (UpSampling2D)  (None, 256, 256, 51  0          ['activation_9[0][0]']           \n","                                2)                                                                \n","                                                                                                  \n"," conv2d_28 (Conv2D)             (None, 256, 256, 32  9248        ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_29 (Conv2D)             (None, 256, 256, 32  46112       ['up_sampling2d_6[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_30 (Conv2D)             (None, 256, 256, 32  46112       ['up_sampling2d_7[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_31 (Conv2D)             (None, 256, 256, 32  46112       ['up_sampling2d_8[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_32 (Conv2D)             (None, 256, 256, 32  147488      ['up_sampling2d_9[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_28 (BatchN  (None, 256, 256, 32  128        ['conv2d_28[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_29 (BatchN  (None, 256, 256, 32  128        ['conv2d_29[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_30 (BatchN  (None, 256, 256, 32  128        ['conv2d_30[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_31 (BatchN  (None, 256, 256, 32  128        ['conv2d_31[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," batch_normalization_32 (BatchN  (None, 256, 256, 32  128        ['conv2d_32[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_28 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_28[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_29 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_29[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_30 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_30[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_31 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_31[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," activation_32 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_32[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 256, 256, 16  0           ['activation_28[0][0]',          \n","                                0)                                'activation_29[0][0]',          \n","                                                                  'activation_30[0][0]',          \n","                                                                  'activation_31[0][0]',          \n","                                                                  'activation_32[0][0]']          \n","                                                                                                  \n"," conv2d_33 (Conv2D)             (None, 256, 256, 16  230560      ['concatenate_3[0][0]']          \n","                                0)                                                                \n","                                                                                                  \n"," batch_normalization_33 (BatchN  (None, 256, 256, 16  640        ['conv2d_33[0][0]']              \n"," ormalization)                  0)                                                                \n","                                                                                                  \n"," activation_33 (Activation)     (None, 256, 256, 16  0           ['batch_normalization_33[0][0]'] \n","                                0)                                                                \n","                                                                                                  \n"," conv2d_34 (Conv2D)             (None, 256, 256, 1)  161         ['activation_33[0][0]']          \n","                                                                                                  \n"," batch_normalization_34 (BatchN  (None, 256, 256, 1)  4          ['conv2d_34[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_34 (Activation)     (None, 256, 256, 1)  0           ['batch_normalization_34[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 6,756,261\n","Trainable params: 6,749,731\n","Non-trainable params: 6,530\n","__________________________________________________________________________________________________\n"]}],"source":["unet = unet3plus(input_shape=(256,256,10))\n","# gl_sl_wrapper(alpha)\n","unet.compile(optimizer = adam_v2.Adam(learning_rate = 1e-4), loss =binary_crossentropy, metrics = ['accuracy'])\n","unet.summary()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T15:50:46.395754Z","iopub.status.busy":"2023-04-03T15:50:46.395018Z","iopub.status.idle":"2023-04-03T20:25:37.737635Z","shell.execute_reply":"2023-04-03T20:25:37.736643Z","shell.execute_reply.started":"2023-04-03T15:50:46.395715Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.4374 - accuracy: 0.8951\n","Epoch 1: val_loss improved from inf to 0.52305, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 60s 428ms/step - loss: 0.4374 - accuracy: 0.8951 - val_loss: 0.5231 - val_accuracy: 0.8281 - lr: 1.0000e-04\n","Epoch 2/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3905 - accuracy: 0.9506\n","Epoch 2: val_loss improved from 0.52305 to 0.48334, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 410ms/step - loss: 0.3905 - accuracy: 0.9506 - val_loss: 0.4833 - val_accuracy: 0.8223 - lr: 1.0000e-04\n","Epoch 3/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3862 - accuracy: 0.9530\n","Epoch 3: val_loss improved from 0.48334 to 0.36290, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.3862 - accuracy: 0.9530 - val_loss: 0.3629 - val_accuracy: 0.9683 - lr: 1.0000e-04\n","Epoch 4/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3754 - accuracy: 0.9610\n","Epoch 4: val_loss did not improve from 0.36290\n","112/112 [==============================] - 46s 409ms/step - loss: 0.3754 - accuracy: 0.9610 - val_loss: 0.3650 - val_accuracy: 0.9742 - lr: 1.0000e-04\n","Epoch 5/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3635 - accuracy: 0.9716\n","Epoch 5: val_loss improved from 0.36290 to 0.35594, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.3635 - accuracy: 0.9716 - val_loss: 0.3559 - val_accuracy: 0.9800 - lr: 1.0000e-04\n","Epoch 6/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3608 - accuracy: 0.9687\n","Epoch 6: val_loss improved from 0.35594 to 0.34992, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 394ms/step - loss: 0.3608 - accuracy: 0.9687 - val_loss: 0.3499 - val_accuracy: 0.9826 - lr: 1.0000e-04\n","Epoch 7/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3554 - accuracy: 0.9731\n","Epoch 7: val_loss did not improve from 0.34992\n","112/112 [==============================] - 44s 390ms/step - loss: 0.3554 - accuracy: 0.9731 - val_loss: 0.3723 - val_accuracy: 0.9709 - lr: 1.0000e-04\n","Epoch 8/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3520 - accuracy: 0.9754\n","Epoch 8: val_loss did not improve from 0.34992\n","112/112 [==============================] - 44s 390ms/step - loss: 0.3520 - accuracy: 0.9754 - val_loss: 0.3515 - val_accuracy: 0.9825 - lr: 1.0000e-04\n","Epoch 9/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3479 - accuracy: 0.9740\n","Epoch 9: val_loss improved from 0.34992 to 0.34314, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 394ms/step - loss: 0.3479 - accuracy: 0.9740 - val_loss: 0.3431 - val_accuracy: 0.9799 - lr: 1.0000e-04\n","Epoch 10/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.9769\n","Epoch 10: val_loss improved from 0.34314 to 0.33591, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.3437 - accuracy: 0.9769 - val_loss: 0.3359 - val_accuracy: 0.9853 - lr: 1.0000e-04\n","Epoch 11/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 0.9781\n","Epoch 11: val_loss improved from 0.33591 to 0.33248, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.3384 - accuracy: 0.9781 - val_loss: 0.3325 - val_accuracy: 0.9829 - lr: 1.0000e-04\n","Epoch 12/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3295 - accuracy: 0.9841\n","Epoch 12: val_loss improved from 0.33248 to 0.32710, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.3295 - accuracy: 0.9841 - val_loss: 0.3271 - val_accuracy: 0.9867 - lr: 1.0000e-04\n","Epoch 13/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3261 - accuracy: 0.9830\n","Epoch 13: val_loss improved from 0.32710 to 0.32350, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.3261 - accuracy: 0.9830 - val_loss: 0.3235 - val_accuracy: 0.9849 - lr: 1.0000e-04\n","Epoch 14/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3241 - accuracy: 0.9813\n","Epoch 14: val_loss improved from 0.32350 to 0.31519, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.3241 - accuracy: 0.9813 - val_loss: 0.3152 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 15/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.9817\n","Epoch 15: val_loss improved from 0.31519 to 0.30696, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.3208 - accuracy: 0.9817 - val_loss: 0.3070 - val_accuracy: 0.9848 - lr: 1.0000e-04\n","Epoch 16/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.9809\n","Epoch 16: val_loss did not improve from 0.30696\n","112/112 [==============================] - 44s 390ms/step - loss: 0.3165 - accuracy: 0.9809 - val_loss: 0.3074 - val_accuracy: 0.9874 - lr: 1.0000e-04\n","Epoch 17/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.9845\n","Epoch 17: val_loss did not improve from 0.30696\n","112/112 [==============================] - 45s 406ms/step - loss: 0.3111 - accuracy: 0.9845 - val_loss: 0.3146 - val_accuracy: 0.9849 - lr: 1.0000e-04\n","Epoch 18/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3114 - accuracy: 0.9845\n","Epoch 18: val_loss improved from 0.30696 to 0.30328, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.3114 - accuracy: 0.9845 - val_loss: 0.3033 - val_accuracy: 0.9874 - lr: 1.0000e-04\n","Epoch 19/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3153 - accuracy: 0.9788\n","Epoch 19: val_loss improved from 0.30328 to 0.29686, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.3153 - accuracy: 0.9788 - val_loss: 0.2969 - val_accuracy: 0.9819 - lr: 1.0000e-04\n","Epoch 20/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3039 - accuracy: 0.9832\n","Epoch 20: val_loss improved from 0.29686 to 0.29156, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.3039 - accuracy: 0.9832 - val_loss: 0.2916 - val_accuracy: 0.9877 - lr: 1.0000e-04\n","Epoch 21/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.9823\n","Epoch 21: val_loss did not improve from 0.29156\n","112/112 [==============================] - 46s 407ms/step - loss: 0.3008 - accuracy: 0.9823 - val_loss: 0.2937 - val_accuracy: 0.9832 - lr: 1.0000e-04\n","Epoch 22/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2950 - accuracy: 0.9838\n","Epoch 22: val_loss improved from 0.29156 to 0.28586, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.2950 - accuracy: 0.9838 - val_loss: 0.2859 - val_accuracy: 0.9865 - lr: 1.0000e-04\n","Epoch 23/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2911 - accuracy: 0.9869\n","Epoch 23: val_loss did not improve from 0.28586\n","112/112 [==============================] - 46s 407ms/step - loss: 0.2911 - accuracy: 0.9869 - val_loss: 0.2988 - val_accuracy: 0.9814 - lr: 1.0000e-04\n","Epoch 24/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.9860\n","Epoch 24: val_loss improved from 0.28586 to 0.28058, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.2888 - accuracy: 0.9860 - val_loss: 0.2806 - val_accuracy: 0.9874 - lr: 1.0000e-04\n","Epoch 25/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9870\n","Epoch 25: val_loss improved from 0.28058 to 0.28028, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 394ms/step - loss: 0.2881 - accuracy: 0.9870 - val_loss: 0.2803 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 26/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.9846\n","Epoch 26: val_loss improved from 0.28028 to 0.26607, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.2847 - accuracy: 0.9846 - val_loss: 0.2661 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 27/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.9872\n","Epoch 27: val_loss did not improve from 0.26607\n","112/112 [==============================] - 46s 408ms/step - loss: 0.2785 - accuracy: 0.9872 - val_loss: 0.2724 - val_accuracy: 0.9887 - lr: 1.0000e-04\n","Epoch 28/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.9891\n","Epoch 28: val_loss did not improve from 0.26607\n","112/112 [==============================] - 46s 407ms/step - loss: 0.2733 - accuracy: 0.9891 - val_loss: 0.2699 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 29/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2744 - accuracy: 0.9868\n","Epoch 29: val_loss improved from 0.26607 to 0.26160, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.2744 - accuracy: 0.9868 - val_loss: 0.2616 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 30/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.9873\n","Epoch 30: val_loss did not improve from 0.26160\n","112/112 [==============================] - 46s 407ms/step - loss: 0.2733 - accuracy: 0.9873 - val_loss: 0.2700 - val_accuracy: 0.9856 - lr: 1.0000e-04\n","Epoch 31/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.9881\n","Epoch 31: val_loss improved from 0.26160 to 0.25686, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.2667 - accuracy: 0.9881 - val_loss: 0.2569 - val_accuracy: 0.9877 - lr: 1.0000e-04\n","Epoch 32/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9883\n","Epoch 32: val_loss did not improve from 0.25686\n","112/112 [==============================] - 46s 407ms/step - loss: 0.2648 - accuracy: 0.9883 - val_loss: 0.2643 - val_accuracy: 0.9887 - lr: 1.0000e-04\n","Epoch 33/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.9892\n","Epoch 33: val_loss improved from 0.25686 to 0.25684, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.2606 - accuracy: 0.9892 - val_loss: 0.2568 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 34/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2604 - accuracy: 0.9867\n","Epoch 34: val_loss improved from 0.25684 to 0.24079, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.2604 - accuracy: 0.9867 - val_loss: 0.2408 - val_accuracy: 0.9868 - lr: 1.0000e-04\n","Epoch 35/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2656 - accuracy: 0.9846\n","Epoch 35: val_loss improved from 0.24079 to 0.23770, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.2656 - accuracy: 0.9846 - val_loss: 0.2377 - val_accuracy: 0.9877 - lr: 1.0000e-04\n","Epoch 36/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.9864\n","Epoch 36: val_loss did not improve from 0.23770\n","112/112 [==============================] - 44s 389ms/step - loss: 0.2564 - accuracy: 0.9864 - val_loss: 0.2412 - val_accuracy: 0.9887 - lr: 1.0000e-04\n","Epoch 37/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.9892\n","Epoch 37: val_loss did not improve from 0.23770\n","112/112 [==============================] - 44s 390ms/step - loss: 0.2512 - accuracy: 0.9892 - val_loss: 0.2449 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 38/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.9895\n","Epoch 38: val_loss did not improve from 0.23770\n","112/112 [==============================] - 44s 390ms/step - loss: 0.2472 - accuracy: 0.9895 - val_loss: 0.2427 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 39/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.9900\n","Epoch 39: val_loss improved from 0.23770 to 0.23612, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.2449 - accuracy: 0.9900 - val_loss: 0.2361 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 40/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.9902\n","Epoch 40: val_loss improved from 0.23612 to 0.23566, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.2429 - accuracy: 0.9902 - val_loss: 0.2357 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 41/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2401 - accuracy: 0.9885\n","Epoch 41: val_loss did not improve from 0.23566\n","112/112 [==============================] - 46s 407ms/step - loss: 0.2401 - accuracy: 0.9885 - val_loss: 0.2527 - val_accuracy: 0.9726 - lr: 1.0000e-04\n","Epoch 42/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.9888\n","Epoch 42: val_loss did not improve from 0.23566\n","112/112 [==============================] - 46s 407ms/step - loss: 0.2375 - accuracy: 0.9888 - val_loss: 0.2374 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 43/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2340 - accuracy: 0.9902\n","Epoch 43: val_loss improved from 0.23566 to 0.23060, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.2340 - accuracy: 0.9902 - val_loss: 0.2306 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 44/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2331 - accuracy: 0.9892\n","Epoch 44: val_loss did not improve from 0.23060\n","112/112 [==============================] - 44s 389ms/step - loss: 0.2331 - accuracy: 0.9892 - val_loss: 0.2324 - val_accuracy: 0.9876 - lr: 1.0000e-04\n","Epoch 45/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2286 - accuracy: 0.9895\n","Epoch 45: val_loss improved from 0.23060 to 0.22454, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.2286 - accuracy: 0.9895 - val_loss: 0.2245 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 46/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2257 - accuracy: 0.9906\n","Epoch 46: val_loss improved from 0.22454 to 0.21779, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.2257 - accuracy: 0.9906 - val_loss: 0.2178 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 47/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9902\n","Epoch 47: val_loss improved from 0.21779 to 0.21659, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.2228 - accuracy: 0.9902 - val_loss: 0.2166 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 48/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2233 - accuracy: 0.9909\n","Epoch 48: val_loss improved from 0.21659 to 0.20635, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.2233 - accuracy: 0.9909 - val_loss: 0.2063 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 49/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9883\n","Epoch 49: val_loss improved from 0.20635 to 0.20530, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.2254 - accuracy: 0.9883 - val_loss: 0.2053 - val_accuracy: 0.9863 - lr: 1.0000e-04\n","Epoch 50/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.9889\n","Epoch 50: val_loss did not improve from 0.20530\n","112/112 [==============================] - 44s 390ms/step - loss: 0.2217 - accuracy: 0.9889 - val_loss: 0.2287 - val_accuracy: 0.9810 - lr: 1.0000e-04\n","Epoch 51/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2163 - accuracy: 0.9885\n","Epoch 51: val_loss did not improve from 0.20530\n","112/112 [==============================] - 46s 407ms/step - loss: 0.2163 - accuracy: 0.9885 - val_loss: 0.2133 - val_accuracy: 0.9891 - lr: 1.0000e-04\n","Epoch 52/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2153 - accuracy: 0.9897\n","Epoch 52: val_loss did not improve from 0.20530\n","112/112 [==============================] - 44s 390ms/step - loss: 0.2153 - accuracy: 0.9897 - val_loss: 0.2114 - val_accuracy: 0.9891 - lr: 1.0000e-04\n","Epoch 53/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2100 - accuracy: 0.9916\n","Epoch 53: val_loss did not improve from 0.20530\n","112/112 [==============================] - 46s 407ms/step - loss: 0.2100 - accuracy: 0.9916 - val_loss: 0.2068 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 54/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2073 - accuracy: 0.9917\n","Epoch 54: val_loss improved from 0.20530 to 0.20128, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.2073 - accuracy: 0.9917 - val_loss: 0.2013 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 55/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2071 - accuracy: 0.9911\n","Epoch 55: val_loss did not improve from 0.20128\n","112/112 [==============================] - 44s 390ms/step - loss: 0.2071 - accuracy: 0.9911 - val_loss: 0.2051 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 56/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9917\n","Epoch 56: val_loss improved from 0.20128 to 0.19715, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.2050 - accuracy: 0.9917 - val_loss: 0.1972 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 57/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.2067 - accuracy: 0.9893\n","Epoch 57: val_loss did not improve from 0.19715\n","112/112 [==============================] - 46s 408ms/step - loss: 0.2067 - accuracy: 0.9893 - val_loss: 0.1983 - val_accuracy: 0.9872 - lr: 1.0000e-04\n","Epoch 58/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.9914\n","Epoch 58: val_loss improved from 0.19715 to 0.19373, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1980 - accuracy: 0.9914 - val_loss: 0.1937 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 59/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.9914\n","Epoch 59: val_loss improved from 0.19373 to 0.19073, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.1996 - accuracy: 0.9914 - val_loss: 0.1907 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 60/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1984 - accuracy: 0.9900\n","Epoch 60: val_loss did not improve from 0.19073\n","112/112 [==============================] - 44s 390ms/step - loss: 0.1984 - accuracy: 0.9900 - val_loss: 0.1947 - val_accuracy: 0.9876 - lr: 1.0000e-04\n","Epoch 61/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1975 - accuracy: 0.9899\n","Epoch 61: val_loss improved from 0.19073 to 0.18968, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 394ms/step - loss: 0.1975 - accuracy: 0.9899 - val_loss: 0.1897 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 62/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1944 - accuracy: 0.9902\n","Epoch 62: val_loss improved from 0.18968 to 0.18734, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1944 - accuracy: 0.9902 - val_loss: 0.1873 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 63/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9915\n","Epoch 63: val_loss improved from 0.18734 to 0.18361, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.1931 - accuracy: 0.9915 - val_loss: 0.1836 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 64/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1882 - accuracy: 0.9904\n","Epoch 64: val_loss did not improve from 0.18361\n","112/112 [==============================] - 46s 407ms/step - loss: 0.1882 - accuracy: 0.9904 - val_loss: 0.1846 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 65/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1910 - accuracy: 0.9885\n","Epoch 65: val_loss improved from 0.18361 to 0.17332, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.1910 - accuracy: 0.9885 - val_loss: 0.1733 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 66/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1866 - accuracy: 0.9906\n","Epoch 66: val_loss did not improve from 0.17332\n","112/112 [==============================] - 44s 390ms/step - loss: 0.1866 - accuracy: 0.9906 - val_loss: 0.1764 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 67/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.9924\n","Epoch 67: val_loss did not improve from 0.17332\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1804 - accuracy: 0.9924 - val_loss: 0.1762 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 68/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.9916\n","Epoch 68: val_loss did not improve from 0.17332\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1804 - accuracy: 0.9916 - val_loss: 0.1761 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 69/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.9922\n","Epoch 69: val_loss improved from 0.17332 to 0.17161, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.1788 - accuracy: 0.9922 - val_loss: 0.1716 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 70/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9928\n","Epoch 70: val_loss did not improve from 0.17161\n","112/112 [==============================] - 44s 390ms/step - loss: 0.1777 - accuracy: 0.9928 - val_loss: 0.1723 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 71/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.9930\n","Epoch 71: val_loss improved from 0.17161 to 0.16743, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.1716 - accuracy: 0.9930 - val_loss: 0.1674 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 72/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9924\n","Epoch 72: val_loss improved from 0.16743 to 0.15797, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.1724 - accuracy: 0.9924 - val_loss: 0.1580 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 73/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9861\n","Epoch 73: val_loss improved from 0.15797 to 0.15075, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.1820 - accuracy: 0.9861 - val_loss: 0.1508 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 74/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9916\n","Epoch 74: val_loss did not improve from 0.15075\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1723 - accuracy: 0.9916 - val_loss: 0.1618 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 75/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.9918\n","Epoch 75: val_loss did not improve from 0.15075\n","112/112 [==============================] - 44s 391ms/step - loss: 0.1708 - accuracy: 0.9918 - val_loss: 0.1597 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 76/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9922\n","Epoch 76: val_loss did not improve from 0.15075\n","112/112 [==============================] - 46s 407ms/step - loss: 0.1707 - accuracy: 0.9922 - val_loss: 0.1606 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 77/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 0.9922\n","Epoch 77: val_loss did not improve from 0.15075\n","112/112 [==============================] - 46s 407ms/step - loss: 0.1688 - accuracy: 0.9922 - val_loss: 0.1623 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 78/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.9924\n","Epoch 78: val_loss did not improve from 0.15075\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1650 - accuracy: 0.9924 - val_loss: 0.1565 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 79/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9911\n","Epoch 79: val_loss did not improve from 0.15075\n","112/112 [==============================] - 46s 407ms/step - loss: 0.1624 - accuracy: 0.9911 - val_loss: 0.1518 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 80/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.9906\n","Epoch 80: val_loss did not improve from 0.15075\n","112/112 [==============================] - 46s 407ms/step - loss: 0.1650 - accuracy: 0.9906 - val_loss: 0.1549 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 81/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9929\n","Epoch 81: val_loss did not improve from 0.15075\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1572 - accuracy: 0.9929 - val_loss: 0.1582 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 82/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1592 - accuracy: 0.9924\n","Epoch 82: val_loss improved from 0.15075 to 0.14935, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1592 - accuracy: 0.9924 - val_loss: 0.1493 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 83/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1568 - accuracy: 0.9921\n","Epoch 83: val_loss did not improve from 0.14935\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1568 - accuracy: 0.9921 - val_loss: 0.1500 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 84/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1597 - accuracy: 0.9915\n","Epoch 84: val_loss did not improve from 0.14935\n","112/112 [==============================] - 46s 407ms/step - loss: 0.1597 - accuracy: 0.9915 - val_loss: 0.1502 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 85/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.9932\n","Epoch 85: val_loss improved from 0.14935 to 0.14693, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.1517 - accuracy: 0.9932 - val_loss: 0.1469 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 86/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.9934\n","Epoch 86: val_loss improved from 0.14693 to 0.14580, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.1513 - accuracy: 0.9934 - val_loss: 0.1458 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 87/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9935\n","Epoch 87: val_loss improved from 0.14580 to 0.14289, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1503 - accuracy: 0.9935 - val_loss: 0.1429 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 88/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.9927\n","Epoch 88: val_loss improved from 0.14289 to 0.14051, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1500 - accuracy: 0.9927 - val_loss: 0.1405 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 89/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.9927\n","Epoch 89: val_loss improved from 0.14051 to 0.13733, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.1444 - accuracy: 0.9927 - val_loss: 0.1373 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 90/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9932\n","Epoch 90: val_loss improved from 0.13733 to 0.13581, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.1484 - accuracy: 0.9932 - val_loss: 0.1358 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 91/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9934\n","Epoch 91: val_loss improved from 0.13581 to 0.13442, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.1453 - accuracy: 0.9934 - val_loss: 0.1344 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 92/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.9936\n","Epoch 92: val_loss did not improve from 0.13442\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1431 - accuracy: 0.9936 - val_loss: 0.1395 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 93/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1395 - accuracy: 0.9936\n","Epoch 93: val_loss did not improve from 0.13442\n","112/112 [==============================] - 44s 391ms/step - loss: 0.1395 - accuracy: 0.9936 - val_loss: 0.1355 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 94/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1368 - accuracy: 0.9940\n","Epoch 94: val_loss improved from 0.13442 to 0.13194, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1368 - accuracy: 0.9940 - val_loss: 0.1319 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 95/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9938\n","Epoch 95: val_loss improved from 0.13194 to 0.13052, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1402 - accuracy: 0.9938 - val_loss: 0.1305 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 96/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9920\n","Epoch 96: val_loss improved from 0.13052 to 0.12955, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1399 - accuracy: 0.9920 - val_loss: 0.1296 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 97/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9936\n","Epoch 97: val_loss did not improve from 0.12955\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1353 - accuracy: 0.9936 - val_loss: 0.1338 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 98/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9905\n","Epoch 98: val_loss improved from 0.12955 to 0.12817, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1405 - accuracy: 0.9905 - val_loss: 0.1282 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 99/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1350 - accuracy: 0.9918\n","Epoch 99: val_loss improved from 0.12817 to 0.12555, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1350 - accuracy: 0.9918 - val_loss: 0.1255 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 100/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1350 - accuracy: 0.9924\n","Epoch 100: val_loss did not improve from 0.12555\n","112/112 [==============================] - 46s 407ms/step - loss: 0.1350 - accuracy: 0.9924 - val_loss: 0.1279 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 101/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1294 - accuracy: 0.9935\n","Epoch 101: val_loss did not improve from 0.12555\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1294 - accuracy: 0.9935 - val_loss: 0.1261 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 102/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.9931\n","Epoch 102: val_loss improved from 0.12555 to 0.12406, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1295 - accuracy: 0.9931 - val_loss: 0.1241 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 103/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9939\n","Epoch 103: val_loss improved from 0.12406 to 0.12314, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.1268 - accuracy: 0.9939 - val_loss: 0.1231 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 104/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9939\n","Epoch 104: val_loss improved from 0.12314 to 0.12049, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.1271 - accuracy: 0.9939 - val_loss: 0.1205 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 105/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.9940\n","Epoch 105: val_loss improved from 0.12049 to 0.11791, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1254 - accuracy: 0.9940 - val_loss: 0.1179 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 106/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1290 - accuracy: 0.9925\n","Epoch 106: val_loss did not improve from 0.11791\n","112/112 [==============================] - 44s 390ms/step - loss: 0.1290 - accuracy: 0.9925 - val_loss: 0.1181 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 107/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1222 - accuracy: 0.9937\n","Epoch 107: val_loss did not improve from 0.11791\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1222 - accuracy: 0.9937 - val_loss: 0.1185 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 108/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1248 - accuracy: 0.9935\n","Epoch 108: val_loss improved from 0.11791 to 0.11394, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1248 - accuracy: 0.9935 - val_loss: 0.1139 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 109/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9941\n","Epoch 109: val_loss improved from 0.11394 to 0.11231, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.1172 - accuracy: 0.9941 - val_loss: 0.1123 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 110/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1237 - accuracy: 0.9937\n","Epoch 110: val_loss did not improve from 0.11231\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1237 - accuracy: 0.9937 - val_loss: 0.1178 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 111/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9929\n","Epoch 111: val_loss improved from 0.11231 to 0.11086, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1208 - accuracy: 0.9929 - val_loss: 0.1109 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 112/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.9935\n","Epoch 112: val_loss improved from 0.11086 to 0.10844, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.1177 - accuracy: 0.9935 - val_loss: 0.1084 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 113/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1186 - accuracy: 0.9941\n","Epoch 113: val_loss did not improve from 0.10844\n","112/112 [==============================] - 46s 407ms/step - loss: 0.1186 - accuracy: 0.9941 - val_loss: 0.1099 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 114/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 0.9943\n","Epoch 114: val_loss did not improve from 0.10844\n","112/112 [==============================] - 46s 407ms/step - loss: 0.1182 - accuracy: 0.9943 - val_loss: 0.1095 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 115/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9944\n","Epoch 115: val_loss did not improve from 0.10844\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1183 - accuracy: 0.9944 - val_loss: 0.1154 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 116/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9939\n","Epoch 116: val_loss improved from 0.10844 to 0.10732, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.1183 - accuracy: 0.9939 - val_loss: 0.1073 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 117/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9946\n","Epoch 117: val_loss improved from 0.10732 to 0.10604, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.1128 - accuracy: 0.9946 - val_loss: 0.1060 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 118/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1125 - accuracy: 0.9946\n","Epoch 118: val_loss did not improve from 0.10604\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1125 - accuracy: 0.9946 - val_loss: 0.1064 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 119/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9947\n","Epoch 119: val_loss improved from 0.10604 to 0.10558, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.1123 - accuracy: 0.9947 - val_loss: 0.1056 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 120/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1111 - accuracy: 0.9948\n","Epoch 120: val_loss improved from 0.10558 to 0.10289, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.1111 - accuracy: 0.9948 - val_loss: 0.1029 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 121/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9948\n","Epoch 121: val_loss did not improve from 0.10289\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1096 - accuracy: 0.9948 - val_loss: 0.1031 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 122/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9929\n","Epoch 122: val_loss improved from 0.10289 to 0.09738, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.1100 - accuracy: 0.9929 - val_loss: 0.0974 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 123/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1084 - accuracy: 0.9933\n","Epoch 123: val_loss did not improve from 0.09738\n","112/112 [==============================] - 44s 390ms/step - loss: 0.1084 - accuracy: 0.9933 - val_loss: 0.1006 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 124/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9946\n","Epoch 124: val_loss did not improve from 0.09738\n","112/112 [==============================] - 44s 390ms/step - loss: 0.1059 - accuracy: 0.9946 - val_loss: 0.1002 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 125/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 0.9947\n","Epoch 125: val_loss did not improve from 0.09738\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1064 - accuracy: 0.9947 - val_loss: 0.0998 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 126/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9944\n","Epoch 126: val_loss improved from 0.09738 to 0.09379, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.1048 - accuracy: 0.9944 - val_loss: 0.0938 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 127/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.9935\n","Epoch 127: val_loss did not improve from 0.09379\n","112/112 [==============================] - 46s 407ms/step - loss: 0.1055 - accuracy: 0.9935 - val_loss: 0.0951 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 128/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9944\n","Epoch 128: val_loss did not improve from 0.09379\n","112/112 [==============================] - 44s 391ms/step - loss: 0.1037 - accuracy: 0.9944 - val_loss: 0.0978 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 129/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9947\n","Epoch 129: val_loss did not improve from 0.09379\n","112/112 [==============================] - 46s 407ms/step - loss: 0.1033 - accuracy: 0.9947 - val_loss: 0.0968 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 130/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9943\n","Epoch 130: val_loss improved from 0.09379 to 0.09261, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.1042 - accuracy: 0.9943 - val_loss: 0.0926 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 131/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9949\n","Epoch 131: val_loss did not improve from 0.09261\n","112/112 [==============================] - 44s 390ms/step - loss: 0.1014 - accuracy: 0.9949 - val_loss: 0.0941 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 132/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9949\n","Epoch 132: val_loss improved from 0.09261 to 0.09145, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0989 - accuracy: 0.9949 - val_loss: 0.0915 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 133/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9935\n","Epoch 133: val_loss did not improve from 0.09145\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1016 - accuracy: 0.9935 - val_loss: 0.0930 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 134/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.9938\n","Epoch 134: val_loss did not improve from 0.09145\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0974 - accuracy: 0.9938 - val_loss: 0.0951 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 135/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9942\n","Epoch 135: val_loss did not improve from 0.09145\n","112/112 [==============================] - 46s 408ms/step - loss: 0.1002 - accuracy: 0.9942 - val_loss: 0.0923 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 136/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9946\n","Epoch 136: val_loss improved from 0.09145 to 0.08873, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0955 - accuracy: 0.9946 - val_loss: 0.0887 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 137/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9948\n","Epoch 137: val_loss improved from 0.08873 to 0.08776, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.1014 - accuracy: 0.9948 - val_loss: 0.0878 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 138/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0968 - accuracy: 0.9951\n","Epoch 138: val_loss did not improve from 0.08776\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0968 - accuracy: 0.9951 - val_loss: 0.0881 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 139/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9943\n","Epoch 139: val_loss did not improve from 0.08776\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0953 - accuracy: 0.9943 - val_loss: 0.0887 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 140/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9936\n","Epoch 140: val_loss did not improve from 0.08776\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0933 - accuracy: 0.9936 - val_loss: 0.0899 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 141/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.9940\n","Epoch 141: val_loss improved from 0.08776 to 0.08355, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0949 - accuracy: 0.9940 - val_loss: 0.0836 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 142/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9948\n","Epoch 142: val_loss did not improve from 0.08355\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0929 - accuracy: 0.9948 - val_loss: 0.0869 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 143/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9952\n","Epoch 143: val_loss did not improve from 0.08355\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0919 - accuracy: 0.9952 - val_loss: 0.0847 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 144/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0899 - accuracy: 0.9952\n","Epoch 144: val_loss improved from 0.08355 to 0.08038, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0899 - accuracy: 0.9952 - val_loss: 0.0804 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 145/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9953\n","Epoch 145: val_loss did not improve from 0.08038\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0881 - accuracy: 0.9953 - val_loss: 0.0809 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 146/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9956\n","Epoch 146: val_loss did not improve from 0.08038\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0860 - accuracy: 0.9956 - val_loss: 0.0807 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 147/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9947\n","Epoch 147: val_loss did not improve from 0.08038\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0881 - accuracy: 0.9947 - val_loss: 0.0848 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 148/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9946\n","Epoch 148: val_loss did not improve from 0.08038\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0901 - accuracy: 0.9946 - val_loss: 0.0819 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 149/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9938\n","Epoch 149: val_loss improved from 0.08038 to 0.08032, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0915 - accuracy: 0.9938 - val_loss: 0.0803 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 150/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9944\n","Epoch 150: val_loss did not improve from 0.08032\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0910 - accuracy: 0.9944 - val_loss: 0.0821 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 151/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9933\n","Epoch 151: val_loss improved from 0.08032 to 0.07848, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0898 - accuracy: 0.9933 - val_loss: 0.0785 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 152/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9950\n","Epoch 152: val_loss did not improve from 0.07848\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0852 - accuracy: 0.9950 - val_loss: 0.0818 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 153/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9939\n","Epoch 153: val_loss improved from 0.07848 to 0.07789, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0857 - accuracy: 0.9939 - val_loss: 0.0779 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 154/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9925\n","Epoch 154: val_loss improved from 0.07789 to 0.07367, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.0883 - accuracy: 0.9925 - val_loss: 0.0737 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 155/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9944\n","Epoch 155: val_loss did not improve from 0.07367\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0859 - accuracy: 0.9944 - val_loss: 0.0783 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 156/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9946\n","Epoch 156: val_loss did not improve from 0.07367\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0847 - accuracy: 0.9946 - val_loss: 0.0739 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 157/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9952\n","Epoch 157: val_loss did not improve from 0.07367\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0827 - accuracy: 0.9952 - val_loss: 0.0739 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 158/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9955\n","Epoch 158: val_loss improved from 0.07367 to 0.07325, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0794 - accuracy: 0.9955 - val_loss: 0.0733 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 159/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9953\n","Epoch 159: val_loss improved from 0.07325 to 0.07188, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0826 - accuracy: 0.9953 - val_loss: 0.0719 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 160/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9955\n","Epoch 160: val_loss improved from 0.07188 to 0.07164, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0782 - accuracy: 0.9955 - val_loss: 0.0716 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 161/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9957\n","Epoch 161: val_loss did not improve from 0.07164\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0805 - accuracy: 0.9957 - val_loss: 0.0734 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 162/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9954\n","Epoch 162: val_loss improved from 0.07164 to 0.07053, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0747 - accuracy: 0.9954 - val_loss: 0.0705 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 163/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9956\n","Epoch 163: val_loss improved from 0.07053 to 0.07010, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0794 - accuracy: 0.9956 - val_loss: 0.0701 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 164/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9958\n","Epoch 164: val_loss improved from 0.07010 to 0.06913, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0759 - accuracy: 0.9958 - val_loss: 0.0691 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 165/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9959\n","Epoch 165: val_loss improved from 0.06913 to 0.06734, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0725 - accuracy: 0.9959 - val_loss: 0.0673 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 166/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9959\n","Epoch 166: val_loss did not improve from 0.06734\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0762 - accuracy: 0.9959 - val_loss: 0.0687 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 167/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9927\n","Epoch 167: val_loss did not improve from 0.06734\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0795 - accuracy: 0.9927 - val_loss: 0.0684 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 168/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9953\n","Epoch 168: val_loss did not improve from 0.06734\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0730 - accuracy: 0.9953 - val_loss: 0.0685 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 169/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9956\n","Epoch 169: val_loss improved from 0.06734 to 0.06685, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0718 - accuracy: 0.9956 - val_loss: 0.0668 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 170/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9957\n","Epoch 170: val_loss improved from 0.06685 to 0.06660, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0719 - accuracy: 0.9957 - val_loss: 0.0666 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 171/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9959\n","Epoch 171: val_loss improved from 0.06660 to 0.06582, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0722 - accuracy: 0.9959 - val_loss: 0.0658 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 172/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9959\n","Epoch 172: val_loss improved from 0.06582 to 0.06441, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0693 - accuracy: 0.9959 - val_loss: 0.0644 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 173/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9958\n","Epoch 173: val_loss improved from 0.06441 to 0.06254, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 394ms/step - loss: 0.0704 - accuracy: 0.9958 - val_loss: 0.0625 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 174/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9961\n","Epoch 174: val_loss did not improve from 0.06254\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0669 - accuracy: 0.9961 - val_loss: 0.0641 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 175/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9960\n","Epoch 175: val_loss did not improve from 0.06254\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0652 - accuracy: 0.9960 - val_loss: 0.0655 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 176/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9959\n","Epoch 176: val_loss did not improve from 0.06254\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0698 - accuracy: 0.9959 - val_loss: 0.0647 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 177/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9960\n","Epoch 177: val_loss improved from 0.06254 to 0.06060, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0682 - accuracy: 0.9960 - val_loss: 0.0606 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 178/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9960\n","Epoch 178: val_loss did not improve from 0.06060\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0674 - accuracy: 0.9960 - val_loss: 0.0614 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 179/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9962\n","Epoch 179: val_loss did not improve from 0.06060\n","112/112 [==============================] - 44s 389ms/step - loss: 0.0657 - accuracy: 0.9962 - val_loss: 0.0608 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 180/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9960\n","Epoch 180: val_loss did not improve from 0.06060\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0685 - accuracy: 0.9960 - val_loss: 0.0651 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 181/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9876\n","Epoch 181: val_loss improved from 0.06060 to 0.05947, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0815 - accuracy: 0.9876 - val_loss: 0.0595 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 182/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9946\n","Epoch 182: val_loss did not improve from 0.05947\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0691 - accuracy: 0.9946 - val_loss: 0.0625 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 183/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9951\n","Epoch 183: val_loss did not improve from 0.05947\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0665 - accuracy: 0.9951 - val_loss: 0.0596 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 184/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9955\n","Epoch 184: val_loss improved from 0.05947 to 0.05848, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0652 - accuracy: 0.9955 - val_loss: 0.0585 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 185/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9946\n","Epoch 185: val_loss did not improve from 0.05848\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0674 - accuracy: 0.9946 - val_loss: 0.0604 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 186/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9935\n","Epoch 186: val_loss did not improve from 0.05848\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0689 - accuracy: 0.9935 - val_loss: 0.0604 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 187/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9952\n","Epoch 187: val_loss improved from 0.05848 to 0.05759, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0640 - accuracy: 0.9952 - val_loss: 0.0576 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 188/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9958\n","Epoch 188: val_loss improved from 0.05759 to 0.05708, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0618 - accuracy: 0.9958 - val_loss: 0.0571 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 189/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9949\n","Epoch 189: val_loss improved from 0.05708 to 0.05694, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0657 - accuracy: 0.9949 - val_loss: 0.0569 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 190/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9958\n","Epoch 190: val_loss improved from 0.05694 to 0.05507, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0595 - accuracy: 0.9958 - val_loss: 0.0551 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 191/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9960\n","Epoch 191: val_loss did not improve from 0.05507\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0618 - accuracy: 0.9960 - val_loss: 0.0567 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 192/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9959\n","Epoch 192: val_loss improved from 0.05507 to 0.05464, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0637 - accuracy: 0.9959 - val_loss: 0.0546 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 193/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9960\n","Epoch 193: val_loss did not improve from 0.05464\n","112/112 [==============================] - 45s 407ms/step - loss: 0.0623 - accuracy: 0.9960 - val_loss: 0.0553 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 194/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9958\n","Epoch 194: val_loss did not improve from 0.05464\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0599 - accuracy: 0.9958 - val_loss: 0.0548 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 195/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9961\n","Epoch 195: val_loss improved from 0.05464 to 0.05298, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0608 - accuracy: 0.9961 - val_loss: 0.0530 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 196/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9958\n","Epoch 196: val_loss did not improve from 0.05298\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0595 - accuracy: 0.9958 - val_loss: 0.0552 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 197/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9961\n","Epoch 197: val_loss did not improve from 0.05298\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0573 - accuracy: 0.9961 - val_loss: 0.0537 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 198/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9964\n","Epoch 198: val_loss improved from 0.05298 to 0.05297, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0582 - accuracy: 0.9964 - val_loss: 0.0530 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 199/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9951\n","Epoch 199: val_loss did not improve from 0.05297\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0601 - accuracy: 0.9951 - val_loss: 0.0569 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 200/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0585 - accuracy: 0.9958\n","Epoch 200: val_loss did not improve from 0.05297\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0585 - accuracy: 0.9958 - val_loss: 0.0540 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 201/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9962\n","Epoch 201: val_loss improved from 0.05297 to 0.05143, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0576 - accuracy: 0.9962 - val_loss: 0.0514 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 202/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9963\n","Epoch 202: val_loss improved from 0.05143 to 0.05063, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.0558 - accuracy: 0.9963 - val_loss: 0.0506 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 203/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9963\n","Epoch 203: val_loss did not improve from 0.05063\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0574 - accuracy: 0.9963 - val_loss: 0.0512 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 204/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9962\n","Epoch 204: val_loss improved from 0.05063 to 0.04982, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0537 - accuracy: 0.9962 - val_loss: 0.0498 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 205/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9965\n","Epoch 205: val_loss improved from 0.04982 to 0.04961, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0549 - accuracy: 0.9965 - val_loss: 0.0496 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 206/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9961\n","Epoch 206: val_loss improved from 0.04961 to 0.04905, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0566 - accuracy: 0.9961 - val_loss: 0.0491 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 207/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9965\n","Epoch 207: val_loss did not improve from 0.04905\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0537 - accuracy: 0.9965 - val_loss: 0.0492 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 208/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9965\n","Epoch 208: val_loss did not improve from 0.04905\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0522 - accuracy: 0.9965 - val_loss: 0.0507 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 209/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9964\n","Epoch 209: val_loss improved from 0.04905 to 0.04822, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0547 - accuracy: 0.9964 - val_loss: 0.0482 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 210/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9965\n","Epoch 210: val_loss did not improve from 0.04822\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0519 - accuracy: 0.9965 - val_loss: 0.0484 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 211/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9965\n","Epoch 211: val_loss improved from 0.04822 to 0.04585, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0504 - accuracy: 0.9965 - val_loss: 0.0458 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 212/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9967\n","Epoch 212: val_loss did not improve from 0.04585\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0489 - accuracy: 0.9967 - val_loss: 0.0478 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 213/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9967\n","Epoch 213: val_loss improved from 0.04585 to 0.04473, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0491 - accuracy: 0.9967 - val_loss: 0.0447 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 214/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9965\n","Epoch 214: val_loss did not improve from 0.04473\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0526 - accuracy: 0.9965 - val_loss: 0.0462 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 215/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9966\n","Epoch 215: val_loss did not improve from 0.04473\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0520 - accuracy: 0.9966 - val_loss: 0.0472 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 216/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9967\n","Epoch 216: val_loss did not improve from 0.04473\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0470 - accuracy: 0.9967 - val_loss: 0.0470 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 217/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9961\n","Epoch 217: val_loss did not improve from 0.04473\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0517 - accuracy: 0.9961 - val_loss: 0.0472 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 218/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9965\n","Epoch 218: val_loss did not improve from 0.04473\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0500 - accuracy: 0.9965 - val_loss: 0.0459 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 219/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9952\n","Epoch 219: val_loss did not improve from 0.04473\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0524 - accuracy: 0.9952 - val_loss: 0.0475 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 220/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9962\n","Epoch 220: val_loss did not improve from 0.04473\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0506 - accuracy: 0.9962 - val_loss: 0.0469 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 221/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9966\n","Epoch 221: val_loss improved from 0.04473 to 0.04359, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0470 - accuracy: 0.9966 - val_loss: 0.0436 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 222/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9966\n","Epoch 222: val_loss improved from 0.04359 to 0.04326, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0493 - accuracy: 0.9966 - val_loss: 0.0433 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 223/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9967\n","Epoch 223: val_loss improved from 0.04326 to 0.04281, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0460 - accuracy: 0.9967 - val_loss: 0.0428 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 224/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9965\n","Epoch 224: val_loss improved from 0.04281 to 0.04199, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0478 - accuracy: 0.9965 - val_loss: 0.0420 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 225/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9965\n","Epoch 225: val_loss did not improve from 0.04199\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0501 - accuracy: 0.9965 - val_loss: 0.0448 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 226/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9935\n","Epoch 226: val_loss did not improve from 0.04199\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0533 - accuracy: 0.9935 - val_loss: 0.0466 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 227/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9957\n","Epoch 227: val_loss did not improve from 0.04199\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0473 - accuracy: 0.9957 - val_loss: 0.0429 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 228/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9952\n","Epoch 228: val_loss improved from 0.04199 to 0.03992, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0452 - accuracy: 0.9952 - val_loss: 0.0399 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 229/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9963\n","Epoch 229: val_loss did not improve from 0.03992\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0476 - accuracy: 0.9963 - val_loss: 0.0410 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 230/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9967\n","Epoch 230: val_loss did not improve from 0.03992\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0465 - accuracy: 0.9967 - val_loss: 0.0408 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 231/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9967\n","Epoch 231: val_loss did not improve from 0.03992\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0434 - accuracy: 0.9967 - val_loss: 0.0414 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 232/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9968\n","Epoch 232: val_loss did not improve from 0.03992\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0422 - accuracy: 0.9968 - val_loss: 0.0406 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 233/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9967\n","Epoch 233: val_loss did not improve from 0.03992\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0467 - accuracy: 0.9967 - val_loss: 0.0676 - val_accuracy: 0.9852 - lr: 1.0000e-04\n","Epoch 234/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9965\n","Epoch 234: val_loss did not improve from 0.03992\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0454 - accuracy: 0.9965 - val_loss: 0.0403 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 235/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9968\n","Epoch 235: val_loss improved from 0.03992 to 0.03930, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0437 - accuracy: 0.9968 - val_loss: 0.0393 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 236/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0420 - accuracy: 0.9969\n","Epoch 236: val_loss did not improve from 0.03930\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0420 - accuracy: 0.9969 - val_loss: 0.0407 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 237/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9969\n","Epoch 237: val_loss improved from 0.03930 to 0.03884, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0432 - accuracy: 0.9969 - val_loss: 0.0388 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 238/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9967\n","Epoch 238: val_loss did not improve from 0.03884\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0432 - accuracy: 0.9967 - val_loss: 0.0406 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 239/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9938\n","Epoch 239: val_loss did not improve from 0.03884\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0503 - accuracy: 0.9938 - val_loss: 0.0402 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 240/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9956\n","Epoch 240: val_loss did not improve from 0.03884\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0458 - accuracy: 0.9956 - val_loss: 0.0395 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 241/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9966\n","Epoch 241: val_loss improved from 0.03884 to 0.03817, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0410 - accuracy: 0.9966 - val_loss: 0.0382 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 242/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9965\n","Epoch 242: val_loss did not improve from 0.03817\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0439 - accuracy: 0.9965 - val_loss: 0.0395 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 243/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9969\n","Epoch 243: val_loss improved from 0.03817 to 0.03813, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0413 - accuracy: 0.9969 - val_loss: 0.0381 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 244/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9969\n","Epoch 244: val_loss improved from 0.03813 to 0.03787, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0419 - accuracy: 0.9969 - val_loss: 0.0379 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 245/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9970\n","Epoch 245: val_loss improved from 0.03787 to 0.03685, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0414 - accuracy: 0.9970 - val_loss: 0.0369 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 246/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9969\n","Epoch 246: val_loss improved from 0.03685 to 0.03685, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0395 - accuracy: 0.9969 - val_loss: 0.0369 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 247/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9970\n","Epoch 247: val_loss did not improve from 0.03685\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0398 - accuracy: 0.9970 - val_loss: 0.0376 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 248/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9971\n","Epoch 248: val_loss did not improve from 0.03685\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0406 - accuracy: 0.9971 - val_loss: 0.0369 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 249/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9968\n","Epoch 249: val_loss improved from 0.03685 to 0.03598, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 47s 416ms/step - loss: 0.0415 - accuracy: 0.9968 - val_loss: 0.0360 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 250/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9971\n","Epoch 250: val_loss improved from 0.03598 to 0.03527, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 45s 399ms/step - loss: 0.0369 - accuracy: 0.9971 - val_loss: 0.0353 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 251/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9969\n","Epoch 251: val_loss did not improve from 0.03527\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0400 - accuracy: 0.9969 - val_loss: 0.0363 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 252/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9961\n","Epoch 252: val_loss did not improve from 0.03527\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0406 - accuracy: 0.9961 - val_loss: 0.0361 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 253/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9970\n","Epoch 253: val_loss did not improve from 0.03527\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0385 - accuracy: 0.9970 - val_loss: 0.0355 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 254/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9971\n","Epoch 254: val_loss did not improve from 0.03527\n","112/112 [==============================] - 44s 393ms/step - loss: 0.0381 - accuracy: 0.9971 - val_loss: 0.0383 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 255/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9963\n","Epoch 255: val_loss did not improve from 0.03527\n","112/112 [==============================] - 44s 392ms/step - loss: 0.0392 - accuracy: 0.9963 - val_loss: 0.0376 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 256/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9921\n","Epoch 256: val_loss did not improve from 0.03527\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0511 - accuracy: 0.9921 - val_loss: 0.0357 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 257/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9958\n","Epoch 257: val_loss improved from 0.03527 to 0.03500, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0426 - accuracy: 0.9958 - val_loss: 0.0350 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 258/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9963\n","Epoch 258: val_loss did not improve from 0.03500\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0415 - accuracy: 0.9963 - val_loss: 0.0360 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 259/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9956\n","Epoch 259: val_loss improved from 0.03500 to 0.03446, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 397ms/step - loss: 0.0406 - accuracy: 0.9956 - val_loss: 0.0345 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 260/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9966\n","Epoch 260: val_loss did not improve from 0.03446\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0406 - accuracy: 0.9966 - val_loss: 0.0345 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 261/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9969\n","Epoch 261: val_loss improved from 0.03446 to 0.03322, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0357 - accuracy: 0.9969 - val_loss: 0.0332 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 262/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9970\n","Epoch 262: val_loss did not improve from 0.03322\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0374 - accuracy: 0.9970 - val_loss: 0.0336 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 263/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9970\n","Epoch 263: val_loss did not improve from 0.03322\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0365 - accuracy: 0.9970 - val_loss: 0.0341 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 264/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9971\n","Epoch 264: val_loss improved from 0.03322 to 0.03207, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0347 - accuracy: 0.9971 - val_loss: 0.0321 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 265/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9966\n","Epoch 265: val_loss did not improve from 0.03207\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0360 - accuracy: 0.9966 - val_loss: 0.0323 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 266/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9963\n","Epoch 266: val_loss did not improve from 0.03207\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0376 - accuracy: 0.9963 - val_loss: 0.0330 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 267/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9970\n","Epoch 267: val_loss did not improve from 0.03207\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0343 - accuracy: 0.9970 - val_loss: 0.0326 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 268/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9970\n","Epoch 268: val_loss did not improve from 0.03207\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0375 - accuracy: 0.9970 - val_loss: 0.0322 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 269/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9970\n","Epoch 269: val_loss did not improve from 0.03207\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0366 - accuracy: 0.9970 - val_loss: 0.0327 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 270/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9971\n","Epoch 270: val_loss did not improve from 0.03207\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0349 - accuracy: 0.9971 - val_loss: 0.0333 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 271/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9970\n","Epoch 271: val_loss improved from 0.03207 to 0.03159, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0349 - accuracy: 0.9970 - val_loss: 0.0316 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 272/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9951\n","Epoch 272: val_loss did not improve from 0.03159\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0367 - accuracy: 0.9951 - val_loss: 0.0343 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 273/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9957\n","Epoch 273: val_loss did not improve from 0.03159\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0365 - accuracy: 0.9957 - val_loss: 0.0321 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 274/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9967\n","Epoch 274: val_loss improved from 0.03159 to 0.03069, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0352 - accuracy: 0.9967 - val_loss: 0.0307 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 275/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9971\n","Epoch 275: val_loss did not improve from 0.03069\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0325 - accuracy: 0.9971 - val_loss: 0.0312 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 276/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9972\n","Epoch 276: val_loss improved from 0.03069 to 0.03014, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0305 - accuracy: 0.9972 - val_loss: 0.0301 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 277/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9972\n","Epoch 277: val_loss did not improve from 0.03014\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0310 - accuracy: 0.9972 - val_loss: 0.0302 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 278/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9973\n","Epoch 278: val_loss improved from 0.03014 to 0.03008, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0313 - accuracy: 0.9973 - val_loss: 0.0301 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 279/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9973\n","Epoch 279: val_loss improved from 0.03008 to 0.02927, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0308 - accuracy: 0.9973 - val_loss: 0.0293 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 280/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9973\n","Epoch 280: val_loss did not improve from 0.02927\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0307 - accuracy: 0.9973 - val_loss: 0.0296 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 281/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9973\n","Epoch 281: val_loss did not improve from 0.02927\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0326 - accuracy: 0.9973 - val_loss: 0.0298 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 282/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9974\n","Epoch 282: val_loss improved from 0.02927 to 0.02926, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0318 - accuracy: 0.9974 - val_loss: 0.0293 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 283/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9966\n","Epoch 283: val_loss improved from 0.02926 to 0.02834, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 414ms/step - loss: 0.0308 - accuracy: 0.9966 - val_loss: 0.0283 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 284/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9968\n","Epoch 284: val_loss did not improve from 0.02834\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0311 - accuracy: 0.9968 - val_loss: 0.0334 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 285/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9951\n","Epoch 285: val_loss did not improve from 0.02834\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0354 - accuracy: 0.9951 - val_loss: 0.0314 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 286/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9941\n","Epoch 286: val_loss did not improve from 0.02834\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0381 - accuracy: 0.9941 - val_loss: 0.0314 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 287/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9966\n","Epoch 287: val_loss did not improve from 0.02834\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0332 - accuracy: 0.9966 - val_loss: 0.0296 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 288/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9968\n","Epoch 288: val_loss did not improve from 0.02834\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0326 - accuracy: 0.9968 - val_loss: 0.0291 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 289/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9970\n","Epoch 289: val_loss did not improve from 0.02834\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0309 - accuracy: 0.9970 - val_loss: 0.0287 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 290/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9946\n","Epoch 290: val_loss did not improve from 0.02834\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0389 - accuracy: 0.9946 - val_loss: 0.0287 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 291/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9969\n","Epoch 291: val_loss did not improve from 0.02834\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0312 - accuracy: 0.9969 - val_loss: 0.0297 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 292/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9964\n","Epoch 292: val_loss did not improve from 0.02834\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0325 - accuracy: 0.9964 - val_loss: 0.0290 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 293/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9971\n","Epoch 293: val_loss improved from 0.02834 to 0.02739, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0279 - accuracy: 0.9971 - val_loss: 0.0274 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 294/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9971\n","Epoch 294: val_loss did not improve from 0.02739\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0289 - accuracy: 0.9971 - val_loss: 0.0276 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 295/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9973\n","Epoch 295: val_loss improved from 0.02739 to 0.02699, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0296 - accuracy: 0.9973 - val_loss: 0.0270 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 296/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9973\n","Epoch 296: val_loss did not improve from 0.02699\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0298 - accuracy: 0.9973 - val_loss: 0.0279 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 297/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9970\n","Epoch 297: val_loss improved from 0.02699 to 0.02695, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0301 - accuracy: 0.9970 - val_loss: 0.0269 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 298/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9974\n","Epoch 298: val_loss improved from 0.02695 to 0.02608, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0277 - accuracy: 0.9974 - val_loss: 0.0261 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 299/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9975\n","Epoch 299: val_loss improved from 0.02608 to 0.02583, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0273 - accuracy: 0.9975 - val_loss: 0.0258 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 300/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9975\n","Epoch 300: val_loss did not improve from 0.02583\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0292 - accuracy: 0.9975 - val_loss: 0.0264 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 301/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9973\n","Epoch 301: val_loss improved from 0.02583 to 0.02538, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0269 - accuracy: 0.9973 - val_loss: 0.0254 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 302/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9974\n","Epoch 302: val_loss did not improve from 0.02538\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0271 - accuracy: 0.9974 - val_loss: 0.0270 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 303/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9976\n","Epoch 303: val_loss did not improve from 0.02538\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0262 - accuracy: 0.9976 - val_loss: 0.0257 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 304/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9973\n","Epoch 304: val_loss improved from 0.02538 to 0.02517, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0258 - accuracy: 0.9973 - val_loss: 0.0252 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 305/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9974\n","Epoch 305: val_loss did not improve from 0.02517\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0276 - accuracy: 0.9974 - val_loss: 0.0259 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 306/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9976\n","Epoch 306: val_loss did not improve from 0.02517\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0262 - accuracy: 0.9976 - val_loss: 0.0256 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 307/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9976\n","Epoch 307: val_loss did not improve from 0.02517\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0268 - accuracy: 0.9976 - val_loss: 0.0255 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 308/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9976\n","Epoch 308: val_loss did not improve from 0.02517\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0257 - accuracy: 0.9976 - val_loss: 0.0259 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 309/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9970\n","Epoch 309: val_loss did not improve from 0.02517\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0272 - accuracy: 0.9970 - val_loss: 0.0259 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 310/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9961\n","Epoch 310: val_loss improved from 0.02517 to 0.02513, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0278 - accuracy: 0.9961 - val_loss: 0.0251 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 311/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9972\n","Epoch 311: val_loss improved from 0.02513 to 0.02471, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0273 - accuracy: 0.9972 - val_loss: 0.0247 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 312/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9974\n","Epoch 312: val_loss did not improve from 0.02471\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0273 - accuracy: 0.9974 - val_loss: 0.0248 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 313/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9972\n","Epoch 313: val_loss did not improve from 0.02471\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0262 - accuracy: 0.9972 - val_loss: 0.0284 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 314/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9955\n","Epoch 314: val_loss did not improve from 0.02471\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0315 - accuracy: 0.9955 - val_loss: 0.0258 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 315/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9972\n","Epoch 315: val_loss improved from 0.02471 to 0.02430, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0254 - accuracy: 0.9972 - val_loss: 0.0243 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 316/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9975\n","Epoch 316: val_loss improved from 0.02430 to 0.02321, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 396ms/step - loss: 0.0251 - accuracy: 0.9975 - val_loss: 0.0232 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 317/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9975\n","Epoch 317: val_loss did not improve from 0.02321\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0247 - accuracy: 0.9975 - val_loss: 0.0241 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 318/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9973\n","Epoch 318: val_loss did not improve from 0.02321\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0270 - accuracy: 0.9973 - val_loss: 0.0241 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 319/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9975\n","Epoch 319: val_loss did not improve from 0.02321\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0238 - accuracy: 0.9975 - val_loss: 0.0237 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 320/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9976\n","Epoch 320: val_loss improved from 0.02321 to 0.02298, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0246 - accuracy: 0.9976 - val_loss: 0.0230 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 321/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9977\n","Epoch 321: val_loss improved from 0.02298 to 0.02297, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0233 - accuracy: 0.9977 - val_loss: 0.0230 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 322/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9972\n","Epoch 322: val_loss improved from 0.02297 to 0.02227, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0265 - accuracy: 0.9972 - val_loss: 0.0223 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 323/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9976\n","Epoch 323: val_loss did not improve from 0.02227\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0239 - accuracy: 0.9976 - val_loss: 0.0231 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 324/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9945\n","Epoch 324: val_loss did not improve from 0.02227\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0337 - accuracy: 0.9945 - val_loss: 0.0251 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 325/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9971\n","Epoch 325: val_loss did not improve from 0.02227\n","112/112 [==============================] - 44s 389ms/step - loss: 0.0246 - accuracy: 0.9971 - val_loss: 0.0244 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 326/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9974\n","Epoch 326: val_loss did not improve from 0.02227\n","112/112 [==============================] - 45s 407ms/step - loss: 0.0238 - accuracy: 0.9974 - val_loss: 0.0231 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 327/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9975\n","Epoch 327: val_loss did not improve from 0.02227\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0221 - accuracy: 0.9975 - val_loss: 0.0229 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 328/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9976\n","Epoch 328: val_loss improved from 0.02227 to 0.02221, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0223 - accuracy: 0.9976 - val_loss: 0.0222 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 329/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9977\n","Epoch 329: val_loss improved from 0.02221 to 0.02212, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 394ms/step - loss: 0.0222 - accuracy: 0.9977 - val_loss: 0.0221 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 330/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9977\n","Epoch 330: val_loss improved from 0.02212 to 0.02175, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 44s 395ms/step - loss: 0.0239 - accuracy: 0.9977 - val_loss: 0.0217 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 331/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9976\n","Epoch 331: val_loss did not improve from 0.02175\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0235 - accuracy: 0.9976 - val_loss: 0.0221 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 332/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9978\n","Epoch 332: val_loss did not improve from 0.02175\n","112/112 [==============================] - 45s 406ms/step - loss: 0.0222 - accuracy: 0.9978 - val_loss: 0.0227 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 333/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9965\n","Epoch 333: val_loss did not improve from 0.02175\n","112/112 [==============================] - 45s 407ms/step - loss: 0.0253 - accuracy: 0.9965 - val_loss: 0.0226 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 334/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9973\n","Epoch 334: val_loss did not improve from 0.02175\n","112/112 [==============================] - 43s 388ms/step - loss: 0.0241 - accuracy: 0.9973 - val_loss: 0.0241 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 335/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9975\n","Epoch 335: val_loss did not improve from 0.02175\n","112/112 [==============================] - 45s 406ms/step - loss: 0.0253 - accuracy: 0.9975 - val_loss: 0.0242 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 336/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9970\n","Epoch 336: val_loss did not improve from 0.02175\n","112/112 [==============================] - 45s 407ms/step - loss: 0.0245 - accuracy: 0.9970 - val_loss: 0.0227 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 337/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9975\n","Epoch 337: val_loss did not improve from 0.02175\n","112/112 [==============================] - 43s 389ms/step - loss: 0.0240 - accuracy: 0.9975 - val_loss: 0.0221 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 338/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9978\n","Epoch 338: val_loss improved from 0.02175 to 0.02159, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0229 - accuracy: 0.9978 - val_loss: 0.0216 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 339/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9976\n","Epoch 339: val_loss improved from 0.02159 to 0.02142, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 412ms/step - loss: 0.0232 - accuracy: 0.9976 - val_loss: 0.0214 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 340/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9977\n","Epoch 340: val_loss did not improve from 0.02142\n","112/112 [==============================] - 45s 406ms/step - loss: 0.0226 - accuracy: 0.9977 - val_loss: 0.0222 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 341/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9978\n","Epoch 341: val_loss improved from 0.02142 to 0.02109, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 415ms/step - loss: 0.0228 - accuracy: 0.9978 - val_loss: 0.0211 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 342/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9979\n","Epoch 342: val_loss improved from 0.02109 to 0.02061, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0202 - accuracy: 0.9979 - val_loss: 0.0206 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 343/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9978\n","Epoch 343: val_loss did not improve from 0.02061\n","112/112 [==============================] - 44s 389ms/step - loss: 0.0212 - accuracy: 0.9978 - val_loss: 0.0208 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 344/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9979\n","Epoch 344: val_loss did not improve from 0.02061\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0228 - accuracy: 0.9979 - val_loss: 0.0210 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 345/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9979\n","Epoch 345: val_loss did not improve from 0.02061\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0204 - accuracy: 0.9979 - val_loss: 0.0209 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 346/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9980\n","Epoch 346: val_loss did not improve from 0.02061\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0220 - accuracy: 0.9980 - val_loss: 0.0210 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 347/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9975\n","Epoch 347: val_loss did not improve from 0.02061\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0234 - accuracy: 0.9975 - val_loss: 0.0233 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 348/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9968\n","Epoch 348: val_loss did not improve from 0.02061\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0243 - accuracy: 0.9968 - val_loss: 0.0231 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 349/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9973\n","Epoch 349: val_loss did not improve from 0.02061\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0220 - accuracy: 0.9973 - val_loss: 0.0207 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 350/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9967\n","Epoch 350: val_loss improved from 0.02061 to 0.01934, saving model to unet3_bce.hdf5\n","112/112 [==============================] - 46s 413ms/step - loss: 0.0207 - accuracy: 0.9967 - val_loss: 0.0193 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 351/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9977\n","Epoch 351: val_loss did not improve from 0.01934\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0217 - accuracy: 0.9977 - val_loss: 0.0200 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 352/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9979\n","Epoch 352: val_loss did not improve from 0.01934\n","112/112 [==============================] - 44s 389ms/step - loss: 0.0197 - accuracy: 0.9979 - val_loss: 0.0217 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 353/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9974\n","Epoch 353: val_loss did not improve from 0.01934\n","112/112 [==============================] - 44s 389ms/step - loss: 0.0225 - accuracy: 0.9974 - val_loss: 0.0202 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 354/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9975\n","Epoch 354: val_loss did not improve from 0.01934\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0210 - accuracy: 0.9975 - val_loss: 0.0301 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 355/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9929\n","Epoch 355: val_loss did not improve from 0.01934\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0355 - accuracy: 0.9929 - val_loss: 0.0272 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 356/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9968\n","Epoch 356: val_loss did not improve from 0.01934\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0233 - accuracy: 0.9968 - val_loss: 0.0219 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 357/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9972\n","Epoch 357: val_loss did not improve from 0.01934\n","112/112 [==============================] - 44s 389ms/step - loss: 0.0215 - accuracy: 0.9972 - val_loss: 0.0214 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 358/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9975\n","Epoch 358: val_loss did not improve from 0.01934\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0201 - accuracy: 0.9975 - val_loss: 0.0209 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 359/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9974\n","Epoch 359: val_loss did not improve from 0.01934\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0206 - accuracy: 0.9974 - val_loss: 0.0198 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 360/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9969\n","Epoch 360: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n","\n","Epoch 360: val_loss did not improve from 0.01934\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0224 - accuracy: 0.9969 - val_loss: 0.0206 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 361/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9975\n","Epoch 361: val_loss did not improve from 0.01934\n","112/112 [==============================] - 46s 408ms/step - loss: 0.0207 - accuracy: 0.9975 - val_loss: 0.0198 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 362/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9976\n","Epoch 362: val_loss did not improve from 0.01934\n","112/112 [==============================] - 46s 407ms/step - loss: 0.0213 - accuracy: 0.9976 - val_loss: 0.0198 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 363/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9972\n","Epoch 363: val_loss did not improve from 0.01934\n","112/112 [==============================] - 44s 391ms/step - loss: 0.0216 - accuracy: 0.9972 - val_loss: 0.0197 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 364/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9977\n","Epoch 364: val_loss did not improve from 0.01934\n","112/112 [==============================] - 46s 409ms/step - loss: 0.0204 - accuracy: 0.9977 - val_loss: 0.0198 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 365/1000\n","112/112 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9973\n","Epoch 365: val_loss did not improve from 0.01934\n","112/112 [==============================] - 44s 390ms/step - loss: 0.0202 - accuracy: 0.9973 - val_loss: 0.0196 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 365: early stopping\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f7230c16b50>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from keras.callbacks import ReduceLROnPlateau\n","reduce_lr=ReduceLROnPlateau(monitor='val_loss',\n","                         factor=0.1,\n","                         patience=10,\n","                         verbose=1,\n","                         mode='auto',\n","                         min_delta=0.00003,\n","                         cooldown=0,\n","                         min_lr=0)\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15,verbose=1,mode='min')\n","save_model= ModelCheckpoint('unet3_bce.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","unet.fit(images, masks, validation_data=(val_images,val_masks), batch_size=8, epochs=1000,verbose=1,shuffle=True,callbacks=[reduce_lr,save_model,early_stop])"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T20:30:29.177131Z","iopub.status.busy":"2023-04-03T20:30:29.176127Z","iopub.status.idle":"2023-04-03T20:30:29.185980Z","shell.execute_reply":"2023-04-03T20:30:29.184879Z","shell.execute_reply.started":"2023-04-03T20:30:29.177092Z"},"trusted":true},"outputs":[],"source":["np.save('unet3_bce-history.npy',unet.history.history)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T20:30:32.275162Z","iopub.status.busy":"2023-04-03T20:30:32.274072Z","iopub.status.idle":"2023-04-03T20:30:32.281440Z","shell.execute_reply":"2023-04-03T20:30:32.280107Z","shell.execute_reply.started":"2023-04-03T20:30:32.275119Z"},"trusted":true},"outputs":[],"source":["model_history = np.load('unet3_bce-history.npy', allow_pickle='TRUE').item()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T20:30:46.168032Z","iopub.status.busy":"2023-04-03T20:30:46.167661Z","iopub.status.idle":"2023-04-03T20:30:46.692879Z","shell.execute_reply":"2023-04-03T20:30:46.691869Z","shell.execute_reply.started":"2023-04-03T20:30:46.167996Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<matplotlib.legend.Legend at 0x7f72308e3c50>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#@title Plot accuracy and loss \n","from matplotlib import pyplot as plt\n","## Accuracy\n","plt.plot(model_history['accuracy'])\n","plt.plot(model_history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')\n","plt.show()\n","\n","## Loss\n","plt.plot(model_history['loss'])\n","plt.plot(model_history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T20:31:05.845766Z","iopub.status.busy":"2023-04-03T20:31:05.845257Z","iopub.status.idle":"2023-04-03T20:31:29.832454Z","shell.execute_reply":"2023-04-03T20:31:29.831355Z","shell.execute_reply.started":"2023-04-03T20:31:05.845728Z"},"trusted":true},"outputs":[],"source":["## Test images\n","test_dir=r'/content/gdrive/MyDrive/mudtest/'\n","test_images_list = os.listdir(r\"{}images/\".format(base_dir))\n","test_masks_list = []\n","test_images = []\n","for n in test_images_list:\n","    test_masks_list.append(n)\n","    a = (np.array(rxr.open_rasterio(r\"{}/images/{}\".format(base_dir,n))))\n","    a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","    test_images.append(a)\n","            \n","## Test masks\n","test_masks = []\n","for n in test_masks_list:\n","    a = (np.array(rxr.open_rasterio(r\"{}/labels/{}\".format(base_dir,n))))\n","    test_masks.append(a)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T20:31:39.304232Z","iopub.status.busy":"2023-04-03T20:31:39.303498Z","iopub.status.idle":"2023-04-03T20:31:39.457292Z","shell.execute_reply":"2023-04-03T20:31:39.456223Z","shell.execute_reply.started":"2023-04-03T20:31:39.304183Z"},"trusted":true},"outputs":[],"source":["for i in range(len(test_images)):\n","  test_images[i] = test_images[i].astype('float32')\n","  test_images[i] = test_images[i].T\n","\n","for i in range(len(test_masks)):\n","  test_masks[i] = test_masks[i].reshape(1,256,256,1)\n","  test_masks[i] = test_masks[i].T\n","\n","for i in range(len(test_images)):\n","  test_images[i] = test_images[i].reshape(-1,256,256,10)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T20:31:42.112552Z","iopub.status.busy":"2023-04-03T20:31:42.112158Z","iopub.status.idle":"2023-04-03T20:31:42.122639Z","shell.execute_reply":"2023-04-03T20:31:42.119736Z","shell.execute_reply.started":"2023-04-03T20:31:42.112512Z"},"trusted":true},"outputs":[],"source":["#@title Returns an image or array plot of mask prediction\n","\n","def reconstruct_image(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","  # Standardise between 0-1\n","  reconstruction = reconstruction/np.max(reconstruction)\n","\n","  # Round to 0-1, binary pixel-by-pixel classification \n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  # Plot reconstructed mask (prediction)\n","  plt.imshow(reconstruction) \n","'''\n","  Returns array of mask prediction, given model and image\n","'''\n","def reconstruct_array(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","\n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  return reconstruction # Returns array"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T20:31:44.974811Z","iopub.status.busy":"2023-04-03T20:31:44.974386Z","iopub.status.idle":"2023-04-03T20:31:44.996966Z","shell.execute_reply":"2023-04-03T20:31:44.995897Z","shell.execute_reply.started":"2023-04-03T20:31:44.974775Z"},"trusted":true},"outputs":[],"source":["#@title Metric functions for evaluation\n","\n","def score_eval(model, image, mask): # Gives score of mask vs prediction\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","    return accuracy_score(mask.flatten(), reconstruction)\n","\n","  else: # If a list of images input, find accuracy for each\n","    scores = []\n","    for i in range(len(image)):\n","      reconstruction = model.predict(image[i].reshape(1, 256, 256, 10))\n","      reconstruction = np.round(reconstruction).flatten()\n","      scores.append(accuracy_score(mask[i].flatten(), reconstruction))\n","\n","    return scores\n","\n","def recall_eval(model, image, mask): # Find recall score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return recall_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    recall = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        recall.append(recall_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return recall\n","\n","def precision_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return precision_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    precision = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        precision.append(precision_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return precision\n","\n","def iou_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return jaccard_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    iou = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        iou.append(jaccard_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return iou\n","\n","def f1_score_eval(model, image, mask): # Find F1-score\n","    prec = np.mean(precision_eval(model, image, mask))\n","    rec = np.mean(recall_eval(model, image, mask))\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def f1_score_eval_basic(precision, recall):\n","    prec = np.mean(precision)\n","    rec = np.mean(recall)\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def produce_mask(image): # Outputs rounded image (binary)\n","  return np.round(image)\n"," \n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T20:31:49.579882Z","iopub.status.busy":"2023-04-03T20:31:49.579196Z","iopub.status.idle":"2023-04-03T20:33:31.228818Z","shell.execute_reply":"2023-04-03T20:33:31.227684Z","shell.execute_reply.started":"2023-04-03T20:31:49.579844Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 698ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 47ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 47ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 47ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 49ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n"]}],"source":["\n","score = (score_eval(unet, test_images, test_masks))\n","\n","precision = (precision_eval(unet, test_images, test_masks))\n","\n","recall = (recall_eval(unet, test_images, test_masks))\n","\n","iou = (iou_eval(unet, test_images, test_masks))"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T20:33:39.774122Z","iopub.status.busy":"2023-04-03T20:33:39.773722Z","iopub.status.idle":"2023-04-03T20:33:39.782371Z","shell.execute_reply":"2023-04-03T20:33:39.781296Z","shell.execute_reply.started":"2023-04-03T20:33:39.774085Z"},"trusted":true},"outputs":[],"source":["f1_score = (f1_score_eval_basic(precision, recall))"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T20:33:42.789093Z","iopub.status.busy":"2023-04-03T20:33:42.788280Z","iopub.status.idle":"2023-04-03T20:33:42.797122Z","shell.execute_reply":"2023-04-03T20:33:42.795879Z","shell.execute_reply.started":"2023-04-03T20:33:42.789053Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model accuracy:  0.9789817064328301 0.02963055910785305\n","model precision:  0.937027834892734 0.11205362934407555\n","model recall:  0.9796571116314311 0.0325912818435102\n","model F1-score:  0.9578684112002502\n","model iou:  0.9200591639747209 0.11395350073927422\n"]}],"source":["print('model accuracy: ', np.mean(score), np.std(score))\n","print('model precision: ', np.mean(precision), np.std(precision))\n","print('model recall: ', np.mean(recall), np.std(recall))\n","print('model F1-score: ', np.mean(f1_score))\n","print('model iou: ', np.mean(iou), np.std(iou))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
