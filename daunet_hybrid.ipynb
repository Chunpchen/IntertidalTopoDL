{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-04T19:46:11.328286Z","iopub.status.busy":"2023-04-04T19:46:11.327478Z","iopub.status.idle":"2023-04-04T19:46:50.633755Z","shell.execute_reply":"2023-04-04T19:46:50.632663Z","shell.execute_reply.started":"2023-04-04T19:46:11.328233Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/davej23/image-segmentation-keras.git\n","  Cloning https://github.com/davej23/image-segmentation-keras.git to /tmp/pip-req-build-fvnydjm1\n","  Running command git clone --filter=blob:none --quiet https://github.com/davej23/image-segmentation-keras.git /tmp/pip-req-build-fvnydjm1\n","  Resolved https://github.com/davej23/image-segmentation-keras.git to commit e01b0a8d5859854cd9d259a618829889166439f5\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting rarfile\n","  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n","Collecting segmentation-models\n","  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n","Collecting rioxarray\n","  Downloading rioxarray-0.9.1.tar.gz (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting image-classifiers==1.0.0\n","  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n","Collecting keras-applications<=1.0.8,>=1.0.7\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting efficientnet==1.0.0\n","  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n","Collecting h5py<=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: Keras>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (2.11.0)\n","Collecting imageio==2.5.0\n","  Downloading imageio-2.5.0-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0mm\n","\u001b[?25hRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (0.4.0)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.5.4.60)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.64.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (1.21.6)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (9.4.0)\n","Requirement already satisfied: xarray>=0.17 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (0.20.2)\n","Requirement already satisfied: rasterio in /opt/conda/lib/python3.7/site-packages (from rioxarray) (1.2.10)\n","Requirement already satisfied: pyproj>=2.2 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (3.1.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from rioxarray) (23.0)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py<=2.10.0->keras-segmentation==0.3.0) (1.16.0)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (3.5.3)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.7.3)\n","Requirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.8.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from pyproj>=2.2->rioxarray) (2022.12.7)\n","Requirement already satisfied: pandas>=1.1 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (1.3.5)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.11.4)\n","Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.4.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (59.8.0)\n","Requirement already satisfied: click-plugins in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.1.1)\n","Requirement already satisfied: affine in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (2.4.0)\n","Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (22.2.0)\n","Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (0.7.2)\n","Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.4.7)\n","Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (8.1.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2023.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.0)\n","Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.11.2)\n","Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio->rioxarray) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->xarray>=0.17->rioxarray) (3.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (4.38.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (1.4.4)\n","Building wheels for collected packages: keras-segmentation, rioxarray\n","  Building wheel for keras-segmentation (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for keras-segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34377 sha256=7a1fb633bddbc4d1e8370373dd2fbb652069212cac72bee2c278cf656bfaf0a1\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-_6h2rvhk/wheels/f4/fb/07/8f81ceb3d9fe936f5e4dcd1a64cbc489e42e6e7f9c2f166785\n","  Building wheel for rioxarray (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rioxarray: filename=rioxarray-0.9.1-py3-none-any.whl size=54590 sha256=455ddfa20c9912a6d0838736983f790da7ace71a28b2bec74d4956d6bb74eafe\n","  Stored in directory: /root/.cache/pip/wheels/03/b2/26/2e2cc1797ac99cc070d2cae87c340bd3429bbb583c90b1c780\n","Successfully built keras-segmentation rioxarray\n","Installing collected packages: rarfile, imageio, h5py, keras-applications, image-classifiers, efficientnet, segmentation-models, keras-segmentation, rioxarray\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.25.0\n","    Uninstalling imageio-2.25.0:\n","      Successfully uninstalled imageio-2.25.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n","tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed efficientnet-1.0.0 h5py-2.10.0 image-classifiers-1.0.0 imageio-2.5.0 keras-applications-1.0.8 keras-segmentation-0.3.0 rarfile-4.0 rioxarray-0.9.1 segmentation-models-1.0.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#@title import packages\n","import keras\n","import numpy as np\n","import os\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.losses import *\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint\n","import sys\n","\n","!pip install rarfile segmentation-models git+https://github.com/davej23/image-segmentation-keras.git rioxarray\n","from rarfile import RarFile\n","from sklearn.metrics import *\n","import rioxarray as rxr"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:46:50.636390Z","iopub.status.busy":"2023-04-04T19:46:50.635996Z","iopub.status.idle":"2023-04-04T19:48:34.604489Z","shell.execute_reply":"2023-04-04T19:48:34.603442Z","shell.execute_reply.started":"2023-04-04T19:46:50.636347Z"},"trusted":true},"outputs":[],"source":["base_dir = r\"/content/gdrive/MyDrive/mudtrain/\"\n","#@title Read training images and normalise\n","training_images_list = os.listdir(r\"{}train/images/\".format(base_dir))\n","training_masks_list = []\n","training_images = []\n","for n in training_images_list:\n","  training_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}train/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  training_images.append(a)\n","\n","## Training masks\n","training_masks = []\n","for n in training_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}train/labels/{}\".format(base_dir,n))))\n","  training_masks.append(a)\n","\n","\n","## Validation images\n","validation_images_list = os.listdir(r\"{}val/images/\".format(base_dir))\n","validation_masks_list = []\n","validation_images = []\n","for n in validation_images_list:\n","  validation_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}val/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  validation_images.append(a)\n","\n","## Validation masks\n","validation_masks = []\n","for n in validation_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}val/labels/{}\".format(base_dir,n))))\n","  validation_masks.append(a)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:11.104726Z","iopub.status.busy":"2023-04-04T19:50:11.103781Z","iopub.status.idle":"2023-04-04T19:50:11.715132Z","shell.execute_reply":"2023-04-04T19:50:11.714083Z","shell.execute_reply.started":"2023-04-04T19:50:11.104671Z"},"trusted":true},"outputs":[],"source":["#@title Pre-process data, reshaping and transposing\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].astype('float32')\n","  training_images[i] = training_images[i].T\n","\n","for i in range(len(training_masks)):\n","  training_masks[i] = training_masks[i].reshape(1,256,256)\n","  training_masks[i] = training_masks[i].T\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].astype('float32')\n","  validation_images[i] = validation_images[i].T\n","\n","for i in range(len(validation_masks)):\n","  validation_masks[i] = validation_masks[i].reshape(1,256,256)\n","  validation_masks[i] = validation_masks[i].T\n","\n","\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].reshape(256,256,10)\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].reshape(256,256,10)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:15.301001Z","iopub.status.busy":"2023-04-04T19:50:15.300408Z","iopub.status.idle":"2023-04-04T19:50:20.495474Z","shell.execute_reply":"2023-04-04T19:50:20.494310Z","shell.execute_reply.started":"2023-04-04T19:50:15.300963Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 10)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["images=np.vstack([training_images])\n","val_images=np.vstack([validation_images])\n","images.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:20.497974Z","iopub.status.busy":"2023-04-04T19:50:20.497588Z","iopub.status.idle":"2023-04-04T19:50:20.685052Z","shell.execute_reply":"2023-04-04T19:50:20.683663Z","shell.execute_reply.started":"2023-04-04T19:50:20.497935Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 1)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["masks=np.vstack([training_masks])\n","val_masks=np.vstack([validation_masks])\n","val_masks.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:23.849966Z","iopub.status.busy":"2023-04-04T19:50:23.849060Z","iopub.status.idle":"2023-04-04T19:50:24.078253Z","shell.execute_reply":"2023-04-04T19:50:24.077315Z","shell.execute_reply.started":"2023-04-04T19:50:23.849912Z"},"trusted":true},"outputs":[{"data":{"text/plain":["904"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del training_images,validation_images,training_masks,validation_masks,training_images_list,validation_images_list,\n","training_masks_list,validation_masks_list\n","gc.collect()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T21:38:22.808770Z","iopub.status.busy":"2023-04-04T21:38:22.808063Z","iopub.status.idle":"2023-04-04T21:38:22.823610Z","shell.execute_reply":"2023-04-04T21:38:22.822484Z","shell.execute_reply.started":"2023-04-04T21:38:22.808728Z"},"trusted":true},"outputs":[],"source":["del images,masks,val_images,val_masks"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:26.642341Z","iopub.status.busy":"2023-04-04T19:50:26.641074Z","iopub.status.idle":"2023-04-04T19:50:26.653425Z","shell.execute_reply":"2023-04-04T19:50:26.652093Z","shell.execute_reply.started":"2023-04-04T19:50:26.642291Z"},"trusted":true},"outputs":[],"source":["#@title boundary loss\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import models\n","\n","#Shape of semantic segmentation mask\n","OUTPUT_SHAPE = (256, 256, 1)\n","def boundary_loss(y_true, y_pred):\n","\n","    \"\"\"\n","    Paper Implemented : https://arxiv.org/abs/1905.07852\n","    Using Binary Segmentation mask, generates boundary mask on fly and claculates boundary loss.\n","    :param y_true:\n","    :param y_pred:\n","    :return:\n","    \"\"\"\n","    y_true=tf.cast(y_true,tf.float32)\n","    y_pred=tf.cast(y_pred,tf.float32)\n","    \n","    y_pred_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd = y_pred_bd - (1 - y_pred)\n","    y_true_bd = y_true_bd - (1 - y_true)\n","\n","    y_pred_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd_ext = y_pred_bd_ext - (1 - y_pred)\n","    y_true_bd_ext = y_true_bd_ext - (1 - y_true)\n","\n","    P = K.sum(y_pred_bd * y_true_bd_ext) / K.sum(y_pred_bd) + 1e-7\n","    R = K.sum(y_true_bd * y_pred_bd_ext) / K.sum(y_true_bd) + 1e-7\n","    F1_Score = 2 * P * R / (P + R + 1e-7)\n","    loss = K.mean(1 - F1_Score)\n","    \n","    return loss"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:29.939912Z","iopub.status.busy":"2023-04-04T19:50:29.939199Z","iopub.status.idle":"2023-04-04T19:50:32.206391Z","shell.execute_reply":"2023-04-04T19:50:32.205381Z","shell.execute_reply.started":"2023-04-04T19:50:29.939872Z"},"trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, Callback\n","class AlphaScheduler(Callback):\n","  def init(self, alpha, update_fn):\n","    self.alpha = alpha\n","    self.update_fn = update_fn\n","  def on_epoch_end(self, epoch, logs=None):\n","    updated_alpha = self.update_fn(K.get_value(self.alpha))\n","\n","alpha = K.variable(1, dtype='float32')\n","\n","def update_alpha(value):\n","  return np.clip(value - 0.005, 0.005, 1)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:32.349836Z","iopub.status.busy":"2023-04-04T19:50:32.349500Z","iopub.status.idle":"2023-04-04T19:50:32.356389Z","shell.execute_reply":"2023-04-04T19:50:32.354437Z","shell.execute_reply.started":"2023-04-04T19:50:32.349804Z"},"trusted":true},"outputs":[],"source":["def gl_sl_wrapper(alpha):\n","    def gl_sl(y_true, y_pred):\n","        return alpha*keras.losses.binary_crossentropy(y_true, y_pred) +  (1-alpha)* boundary_loss(y_true, y_pred)\n","    return gl_sl"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:35.400196Z","iopub.status.busy":"2023-04-04T19:50:35.399628Z","iopub.status.idle":"2023-04-04T19:50:35.428070Z","shell.execute_reply":"2023-04-04T19:50:35.426726Z","shell.execute_reply.started":"2023-04-04T19:50:35.400155Z"},"trusted":true},"outputs":[],"source":[" \n","def spatial_pool(x, mode, ratio=4):\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n"," \n","    if channel_axis == -1:\n","        batch, height, width, channels = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels//2\n","        input_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        input_x = Reshape((width*height, channel))(input_x)\n"," \n","        context_mask = Conv2D(1, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        context_mask = Reshape((width*height, 1))(context_mask)\n","        context_mask = Softmax(axis=1)(context_mask)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1], transpose_a=True))([input_x, context_mask])\n","        context = Permute((2, 1))(context)\n","        context = Reshape((1, 1, channel))(context)\n"," \n","    else:\n","        batch, channels, height, width = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels // 2\n","        input_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False,\n","                         kernel_initializer='he_normal')(x)\n","        input_x = Reshape((channel, width * height))(input_x)\n"," \n","        context_mask = Conv2D(1, kernel_size=1, strides=1, padding='same', use_bias=False,\n","                              kernel_initializer='he_normal')(x)\n","        context_mask = Reshape((width * height, 1))(context_mask)\n","        context_mask = Softmax(axis=1)(context_mask)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1]))([input_x, context_mask])\n","        context = Reshape((channel, 1, 1))(context)\n"," \n","    if mode == 'p':\n","        context = Conv2D(channels, kernel_size=1, strides=1, padding='same')(context)\n","    else:\n","        context = Conv2D(channel // ratio, kernel_size=1, strides=1, padding='same')(context)\n","        context = LayerNormalization()(context) # pip install keras-layer-normalization\n","        context = Conv2D(channels, kernel_size=1, strides=1, padding='same')(context)\n"," \n","    mask_ch = Activation('sigmoid')(context)\n","    return Multiply()([x, mask_ch])\n"," \n","def channel_pool(x):\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n","    if channel_axis == -1:\n","        batch, height, width, channels = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels//2\n","        g_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        avg_x = GlobalAvgPool2D()(g_x)\n","        avg_x = Softmax()(avg_x)\n","        avg_x = Reshape((channel, 1))(avg_x)\n"," \n","        theta_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        theta_x = Reshape((height*width, channel))(theta_x)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1]))([theta_x, avg_x])\n","        context = Reshape((height*width,))(context)\n","        mask_sp = Activation('sigmoid')(context)\n","        mask_sp = Reshape((height, width, 1))(mask_sp)\n","    else:\n","        batch, channels, height, width = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels//2\n","        g_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        avg_x = GlobalAvgPool2D()(g_x)\n","        avg_x = Softmax()(avg_x)\n","        avg_x = Reshape((1, channel))(avg_x)\n"," \n","        theta_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        theta_x = Reshape((channel, height*width))(theta_x)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1]))([avg_x, theta_x])\n","        context = Reshape((height*width,))(context)\n","        mask_sp = Activation('sigmoid')(context)\n","        mask_sp = Reshape((1, height, width))(mask_sp)\n"," \n","    return Multiply()([x, mask_sp])\n","\n","def PSA(x, mode='p'):\n","    context_channel = spatial_pool(x, mode)\n","    if mode == 'p':\n","        context_spatial = channel_pool(x)\n","        out = Add()([context_spatial, context_channel])\n","    elif mode == 's':\n","        out = channel_pool(context_channel)\n","    else:\n","        out = x\n","    return out"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:40.667519Z","iopub.status.busy":"2023-04-04T19:50:40.666731Z","iopub.status.idle":"2023-04-04T19:50:40.909745Z","shell.execute_reply":"2023-04-04T19:50:40.908704Z","shell.execute_reply.started":"2023-04-04T19:50:40.667481Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import models, layers, regularizers\n","from tensorflow.keras import backend as K\n","\n","#convolutional block\n","def conv_block(x, kernelsize, filters, dropout, batchnorm=True): \n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    if dropout > 0:\n","        conv = layers.Dropout(dropout)(conv)\n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(conv)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    return conv\n","\n","def daunet(input_shape, dropout=0, batchnorm=True):    \n","    \n","    filters = [32,64, 128, 256,512]\n","    kernelsize = 3\n","    upsample_size = 2\n","    \n","    inputs = layers.Input(input_shape)    \n","\n","    # Downsampling layers\n","    dn_1 = conv_block(inputs, kernelsize, filters[0], dropout, batchnorm)\n","    c1=PSA(dn_1)\n","    pool_1 = layers.MaxPooling2D(pool_size=(2,2))(dn_1)\n","    \n","    dn_2 = conv_block(pool_1, kernelsize, filters[1], dropout, batchnorm)\n","    c2=PSA(dn_2)\n","    pool_2 = layers.MaxPooling2D(pool_size=(2,2))(dn_2)\n","    \n","    dn_3 = conv_block(pool_2, kernelsize, filters[2], dropout, batchnorm)\n","    c3=PSA(dn_3)\n","    pool_3 = layers.MaxPooling2D(pool_size=(2,2))(dn_3)\n","    \n","    dn_4 = conv_block(pool_3, kernelsize, filters[3], dropout, batchnorm)\n","    c4=PSA(dn_4)\n","    pool_4 = layers.MaxPooling2D(pool_size=(2,2))(dn_4)\n","    \n","    dn_5 = conv_block(pool_4, kernelsize, filters[4], dropout, batchnorm)\n","\n","    # Upsampling layers   \n","    up_5 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(dn_5)\n","    up_5 = layers.concatenate([up_5, c4], axis=3)\n","    up_conv_5 = conv_block(up_5, kernelsize, filters[3], dropout, batchnorm)\n","    \n","    up_4 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_5)\n","    up_4 = layers.concatenate([up_4, c3], axis=3)\n","    up_conv_4 = conv_block(up_4, kernelsize, filters[2], dropout, batchnorm)\n","       \n","    up_3 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_4)\n","    up_3 = layers.concatenate([up_3, c2], axis=3)\n","    up_conv_3 = conv_block(up_3, kernelsize, filters[1], dropout, batchnorm)\n","    \n","    up_2 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_3)\n","    up_2 = layers.concatenate([up_2, c1], axis=3)\n","    up_conv_2 = conv_block(up_2, kernelsize, filters[0], dropout, batchnorm)    \n","   \n","    conv_final = layers.Conv2D(1, kernel_size=(1,1))(up_conv_2)\n","    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n","    outputs = layers.Activation('sigmoid')(conv_final)  \n","\n","    model = models.Model(inputs=[inputs], outputs=[outputs])     \n","    return model\n","                                     "]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:50.350016Z","iopub.status.busy":"2023-04-04T19:50:50.349443Z","iopub.status.idle":"2023-04-04T19:50:51.697550Z","shell.execute_reply":"2023-04-04T19:50:51.696707Z","shell.execute_reply.started":"2023-04-04T19:50:50.349980Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 256, 256, 1  0           []                               \n","                                0)]                                                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 256, 256, 32  2912        ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 256, 256, 32  128        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," activation (Activation)        (None, 256, 256, 32  0           ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 256, 256, 32  9248        ['activation[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 256, 256, 32  128        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_1 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 128, 128, 32  0           ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 128, 128, 64  18496       ['max_pooling2d[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 128, 128, 64  256        ['conv2d_7[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_4 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_2[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 128, 128, 64  36928       ['activation_4[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 128, 128, 64  256        ['conv2d_8[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_5 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_3[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)  0           ['activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 64, 64, 128)  73856       ['max_pooling2d_1[0][0]']        \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_14[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_8 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_8[0][0]']           \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_15[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_9 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0          ['activation_9[0][0]']           \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 32, 32, 256)  295168      ['max_pooling2d_2[0][0]']        \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_21[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_12 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_12[0][0]']          \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_22[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_13 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," conv2d_26 (Conv2D)             (None, 32, 32, 128)  32768       ['activation_13[0][0]']          \n","                                                                                                  \n"," conv2d_24 (Conv2D)             (None, 32, 32, 1)    256         ['activation_13[0][0]']          \n","                                                                                                  \n"," global_average_pooling2d_3 (Gl  (None, 128)         0           ['conv2d_26[0][0]']              \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 32, 32, 128)  32768       ['activation_13[0][0]']          \n","                                                                                                  \n"," reshape_22 (Reshape)           (None, 1024, 1)      0           ['conv2d_24[0][0]']              \n","                                                                                                  \n"," max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0          ['activation_13[0][0]']          \n","                                                                                                  \n"," conv2d_27 (Conv2D)             (None, 32, 32, 128)  32768       ['activation_13[0][0]']          \n","                                                                                                  \n"," softmax_7 (Softmax)            (None, 128)          0           ['global_average_pooling2d_3[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," reshape_21 (Reshape)           (None, 1024, 128)    0           ['conv2d_23[0][0]']              \n","                                                                                                  \n"," softmax_6 (Softmax)            (None, 1024, 1)      0           ['reshape_22[0][0]']             \n","                                                                                                  \n"," conv2d_28 (Conv2D)             (None, 16, 16, 512)  1180160     ['max_pooling2d_3[0][0]']        \n","                                                                                                  \n"," reshape_25 (Reshape)           (None, 1024, 128)    0           ['conv2d_27[0][0]']              \n","                                                                                                  \n"," reshape_24 (Reshape)           (None, 128, 1)       0           ['softmax_7[0][0]']              \n","                                                                                                  \n"," lambda_6 (Lambda)              (None, 128, 1)       0           ['reshape_21[0][0]',             \n","                                                                  'softmax_6[0][0]']              \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_28[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," lambda_7 (Lambda)              (None, 1024, 1)      0           ['reshape_25[0][0]',             \n","                                                                  'reshape_24[0][0]']             \n","                                                                                                  \n"," permute_3 (Permute)            (None, 1, 128)       0           ['lambda_6[0][0]']               \n","                                                                                                  \n"," activation_16 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," reshape_26 (Reshape)           (None, 1024)         0           ['lambda_7[0][0]']               \n","                                                                                                  \n"," reshape_23 (Reshape)           (None, 1, 1, 128)    0           ['permute_3[0][0]']              \n","                                                                                                  \n"," conv2d_29 (Conv2D)             (None, 16, 16, 512)  2359808     ['activation_16[0][0]']          \n","                                                                                                  \n"," activation_15 (Activation)     (None, 1024)         0           ['reshape_26[0][0]']             \n","                                                                                                  \n"," conv2d_25 (Conv2D)             (None, 1, 1, 256)    33024       ['reshape_23[0][0]']             \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_29[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," reshape_27 (Reshape)           (None, 32, 32, 1)    0           ['activation_15[0][0]']          \n","                                                                                                  \n"," activation_14 (Activation)     (None, 1, 1, 256)    0           ['conv2d_25[0][0]']              \n","                                                                                                  \n"," activation_17 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," multiply_7 (Multiply)          (None, 32, 32, 256)  0           ['activation_13[0][0]',          \n","                                                                  'reshape_27[0][0]']             \n","                                                                                                  \n"," multiply_6 (Multiply)          (None, 32, 32, 256)  0           ['activation_13[0][0]',          \n","                                                                  'activation_14[0][0]']          \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 64, 64, 64)   8192        ['activation_9[0][0]']           \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 64, 64, 1)    128         ['activation_9[0][0]']           \n","                                                                                                  \n"," up_sampling2d (UpSampling2D)   (None, 32, 32, 512)  0           ['activation_17[0][0]']          \n","                                                                                                  \n"," add_3 (Add)                    (None, 32, 32, 256)  0           ['multiply_7[0][0]',             \n","                                                                  'multiply_6[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d_2 (Gl  (None, 64)          0           ['conv2d_19[0][0]']              \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 64, 64, 64)   8192        ['activation_9[0][0]']           \n","                                                                                                  \n"," reshape_15 (Reshape)           (None, 4096, 1)      0           ['conv2d_17[0][0]']              \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 32, 32, 768)  0           ['up_sampling2d[0][0]',          \n","                                                                  'add_3[0][0]']                  \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 64, 64, 64)   8192        ['activation_9[0][0]']           \n","                                                                                                  \n"," softmax_5 (Softmax)            (None, 64)           0           ['global_average_pooling2d_2[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," reshape_14 (Reshape)           (None, 4096, 64)     0           ['conv2d_16[0][0]']              \n","                                                                                                  \n"," softmax_4 (Softmax)            (None, 4096, 1)      0           ['reshape_15[0][0]']             \n","                                                                                                  \n"," conv2d_30 (Conv2D)             (None, 32, 32, 256)  1769728     ['concatenate[0][0]']            \n","                                                                                                  \n"," reshape_18 (Reshape)           (None, 4096, 64)     0           ['conv2d_20[0][0]']              \n","                                                                                                  \n"," reshape_17 (Reshape)           (None, 64, 1)        0           ['softmax_5[0][0]']              \n","                                                                                                  \n"," lambda_4 (Lambda)              (None, 64, 1)        0           ['reshape_14[0][0]',             \n","                                                                  'softmax_4[0][0]']              \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_30[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," lambda_5 (Lambda)              (None, 4096, 1)      0           ['reshape_18[0][0]',             \n","                                                                  'reshape_17[0][0]']             \n","                                                                                                  \n"," permute_2 (Permute)            (None, 1, 64)        0           ['lambda_4[0][0]']               \n","                                                                                                  \n"," activation_18 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_10[0][0]'] \n","                                                                                                  \n"," reshape_19 (Reshape)           (None, 4096)         0           ['lambda_5[0][0]']               \n","                                                                                                  \n"," reshape_16 (Reshape)           (None, 1, 1, 64)     0           ['permute_2[0][0]']              \n","                                                                                                  \n"," conv2d_31 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_18[0][0]']          \n","                                                                                                  \n"," activation_11 (Activation)     (None, 4096)         0           ['reshape_19[0][0]']             \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 1, 1, 128)    8320        ['reshape_16[0][0]']             \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_31[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," reshape_20 (Reshape)           (None, 64, 64, 1)    0           ['activation_11[0][0]']          \n","                                                                                                  \n"," activation_10 (Activation)     (None, 1, 1, 128)    0           ['conv2d_18[0][0]']              \n","                                                                                                  \n"," activation_19 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," multiply_5 (Multiply)          (None, 64, 64, 128)  0           ['activation_9[0][0]',           \n","                                                                  'reshape_20[0][0]']             \n","                                                                                                  \n"," multiply_4 (Multiply)          (None, 64, 64, 128)  0           ['activation_9[0][0]',           \n","                                                                  'activation_10[0][0]']          \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 128, 128, 32  2048        ['activation_5[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 128, 128, 1)  64          ['activation_5[0][0]']           \n","                                                                                                  \n"," up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0          ['activation_19[0][0]']          \n","                                                                                                  \n"," add_2 (Add)                    (None, 64, 64, 128)  0           ['multiply_5[0][0]',             \n","                                                                  'multiply_4[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d_1 (Gl  (None, 32)          0           ['conv2d_12[0][0]']              \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 128, 128, 32  2048        ['activation_5[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," reshape_8 (Reshape)            (None, 16384, 1)     0           ['conv2d_10[0][0]']              \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 64, 64, 384)  0           ['up_sampling2d_1[0][0]',        \n","                                                                  'add_2[0][0]']                  \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 128, 128, 32  2048        ['activation_5[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," softmax_3 (Softmax)            (None, 32)           0           ['global_average_pooling2d_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," reshape_7 (Reshape)            (None, 16384, 32)    0           ['conv2d_9[0][0]']               \n","                                                                                                  \n"," softmax_2 (Softmax)            (None, 16384, 1)     0           ['reshape_8[0][0]']              \n","                                                                                                  \n"," conv2d_32 (Conv2D)             (None, 64, 64, 128)  442496      ['concatenate_1[0][0]']          \n","                                                                                                  \n"," reshape_11 (Reshape)           (None, 16384, 32)    0           ['conv2d_13[0][0]']              \n","                                                                                                  \n"," reshape_10 (Reshape)           (None, 32, 1)        0           ['softmax_3[0][0]']              \n","                                                                                                  \n"," lambda_2 (Lambda)              (None, 32, 1)        0           ['reshape_7[0][0]',              \n","                                                                  'softmax_2[0][0]']              \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 64, 64, 128)  512        ['conv2d_32[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," lambda_3 (Lambda)              (None, 16384, 1)     0           ['reshape_11[0][0]',             \n","                                                                  'reshape_10[0][0]']             \n","                                                                                                  \n"," permute_1 (Permute)            (None, 1, 32)        0           ['lambda_2[0][0]']               \n","                                                                                                  \n"," activation_20 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," reshape_12 (Reshape)           (None, 16384)        0           ['lambda_3[0][0]']               \n","                                                                                                  \n"," reshape_9 (Reshape)            (None, 1, 1, 32)     0           ['permute_1[0][0]']              \n","                                                                                                  \n"," conv2d_33 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_20[0][0]']          \n","                                                                                                  \n"," activation_7 (Activation)      (None, 16384)        0           ['reshape_12[0][0]']             \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 1, 1, 64)     2112        ['reshape_9[0][0]']              \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 64, 64, 128)  512        ['conv2d_33[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," reshape_13 (Reshape)           (None, 128, 128, 1)  0           ['activation_7[0][0]']           \n","                                                                                                  \n"," activation_6 (Activation)      (None, 1, 1, 64)     0           ['conv2d_11[0][0]']              \n","                                                                                                  \n"," activation_21 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," multiply_3 (Multiply)          (None, 128, 128, 64  0           ['activation_5[0][0]',           \n","                                )                                 'reshape_13[0][0]']             \n","                                                                                                  \n"," multiply_2 (Multiply)          (None, 128, 128, 64  0           ['activation_5[0][0]',           \n","                                )                                 'activation_6[0][0]']           \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 256, 256, 16  512         ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 256, 256, 1)  32          ['activation_1[0][0]']           \n","                                                                                                  \n"," up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 12  0          ['activation_21[0][0]']          \n","                                8)                                                                \n","                                                                                                  \n"," add_1 (Add)                    (None, 128, 128, 64  0           ['multiply_3[0][0]',             \n","                                )                                 'multiply_2[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d (Glob  (None, 16)          0           ['conv2d_5[0][0]']               \n"," alAveragePooling2D)                                                                              \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 256, 256, 16  512         ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," reshape_1 (Reshape)            (None, 65536, 1)     0           ['conv2d_3[0][0]']               \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 128, 128, 19  0           ['up_sampling2d_2[0][0]',        \n","                                2)                                'add_1[0][0]']                  \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 256, 256, 16  512         ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," softmax_1 (Softmax)            (None, 16)           0           ['global_average_pooling2d[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," reshape (Reshape)              (None, 65536, 16)    0           ['conv2d_2[0][0]']               \n","                                                                                                  \n"," softmax (Softmax)              (None, 65536, 1)     0           ['reshape_1[0][0]']              \n","                                                                                                  \n"," conv2d_34 (Conv2D)             (None, 128, 128, 64  110656      ['concatenate_2[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," reshape_4 (Reshape)            (None, 65536, 16)    0           ['conv2d_6[0][0]']               \n","                                                                                                  \n"," reshape_3 (Reshape)            (None, 16, 1)        0           ['softmax_1[0][0]']              \n","                                                                                                  \n"," lambda (Lambda)                (None, 16, 1)        0           ['reshape[0][0]',                \n","                                                                  'softmax[0][0]']                \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 128, 128, 64  256        ['conv2d_34[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," lambda_1 (Lambda)              (None, 65536, 1)     0           ['reshape_4[0][0]',              \n","                                                                  'reshape_3[0][0]']              \n","                                                                                                  \n"," permute (Permute)              (None, 1, 16)        0           ['lambda[0][0]']                 \n","                                                                                                  \n"," activation_22 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_14[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," reshape_5 (Reshape)            (None, 65536)        0           ['lambda_1[0][0]']               \n","                                                                                                  \n"," reshape_2 (Reshape)            (None, 1, 1, 16)     0           ['permute[0][0]']                \n","                                                                                                  \n"," conv2d_35 (Conv2D)             (None, 128, 128, 64  36928       ['activation_22[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_3 (Activation)      (None, 65536)        0           ['reshape_5[0][0]']              \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 1, 1, 32)     544         ['reshape_2[0][0]']              \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 128, 128, 64  256        ['conv2d_35[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," reshape_6 (Reshape)            (None, 256, 256, 1)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," activation_2 (Activation)      (None, 1, 1, 32)     0           ['conv2d_4[0][0]']               \n","                                                                                                  \n"," activation_23 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_15[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," multiply_1 (Multiply)          (None, 256, 256, 32  0           ['activation_1[0][0]',           \n","                                )                                 'reshape_6[0][0]']              \n","                                                                                                  \n"," multiply (Multiply)            (None, 256, 256, 32  0           ['activation_1[0][0]',           \n","                                )                                 'activation_2[0][0]']           \n","                                                                                                  \n"," up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64  0          ['activation_23[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," add (Add)                      (None, 256, 256, 32  0           ['multiply_1[0][0]',             \n","                                )                                 'multiply[0][0]']               \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 256, 256, 96  0           ['up_sampling2d_3[0][0]',        \n","                                )                                 'add[0][0]']                    \n","                                                                                                  \n"," conv2d_36 (Conv2D)             (None, 256, 256, 32  27680       ['concatenate_3[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 256, 256, 32  128        ['conv2d_36[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_24 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_16[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_37 (Conv2D)             (None, 256, 256, 32  9248        ['activation_24[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 256, 256, 32  128        ['conv2d_37[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_25 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_17[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_38 (Conv2D)             (None, 256, 256, 1)  33          ['activation_25[0][0]']          \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 256, 256, 1)  4          ['conv2d_38[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_26 (Activation)     (None, 256, 256, 1)  0           ['batch_normalization_18[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 8,035,493\n","Trainable params: 8,029,603\n","Non-trainable params: 5,890\n","__________________________________________________________________________________________________\n"]}],"source":["from keras import metrics\n","unet2= daunet(input_shape=(256,256,10))#binary_crossentropy\n","unet2.compile(optimizer = adam_v2.Adam(learning_rate = 1e-4), loss =gl_sl_wrapper(alpha), metrics = ['accuracy'])\n","unet2.summary()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T19:50:59.073493Z","iopub.status.busy":"2023-04-04T19:50:59.072888Z","iopub.status.idle":"2023-04-04T21:37:33.721231Z","shell.execute_reply":"2023-04-04T21:37:33.720287Z","shell.execute_reply.started":"2023-04-04T19:50:59.073454Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.4435 - accuracy: 0.8918\n","Epoch 1: val_loss improved from inf to 0.61002, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 33s 379ms/step - loss: 0.4435 - accuracy: 0.8918 - val_loss: 0.6100 - val_accuracy: 0.7747 - lr: 1.0000e-04\n","Epoch 2/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3898 - accuracy: 0.9515\n","Epoch 2: val_loss improved from 0.61002 to 0.58973, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.3898 - accuracy: 0.9515 - val_loss: 0.5897 - val_accuracy: 0.7037 - lr: 1.0000e-04\n","Epoch 3/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3887 - accuracy: 0.9528\n","Epoch 3: val_loss improved from 0.58973 to 0.50496, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.3887 - accuracy: 0.9528 - val_loss: 0.5050 - val_accuracy: 0.8031 - lr: 1.0000e-04\n","Epoch 4/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3809 - accuracy: 0.9605\n","Epoch 4: val_loss improved from 0.50496 to 0.47522, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.3809 - accuracy: 0.9605 - val_loss: 0.4752 - val_accuracy: 0.8436 - lr: 1.0000e-04\n","Epoch 5/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3727 - accuracy: 0.9675\n","Epoch 5: val_loss improved from 0.47522 to 0.43957, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.3727 - accuracy: 0.9675 - val_loss: 0.4396 - val_accuracy: 0.8713 - lr: 1.0000e-04\n","Epoch 6/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3684 - accuracy: 0.9683\n","Epoch 6: val_loss improved from 0.43957 to 0.39630, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.3684 - accuracy: 0.9683 - val_loss: 0.3963 - val_accuracy: 0.9583 - lr: 1.0000e-04\n","Epoch 7/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.9672\n","Epoch 7: val_loss improved from 0.39630 to 0.38023, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.3679 - accuracy: 0.9672 - val_loss: 0.3802 - val_accuracy: 0.9622 - lr: 1.0000e-04\n","Epoch 8/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3611 - accuracy: 0.9724\n","Epoch 8: val_loss improved from 0.38023 to 0.36943, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.3611 - accuracy: 0.9724 - val_loss: 0.3694 - val_accuracy: 0.9644 - lr: 1.0000e-04\n","Epoch 9/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3600 - accuracy: 0.9726\n","Epoch 9: val_loss improved from 0.36943 to 0.35784, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.3600 - accuracy: 0.9726 - val_loss: 0.3578 - val_accuracy: 0.9790 - lr: 1.0000e-04\n","Epoch 10/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3528 - accuracy: 0.9775\n","Epoch 10: val_loss improved from 0.35784 to 0.35187, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.3528 - accuracy: 0.9775 - val_loss: 0.3519 - val_accuracy: 0.9831 - lr: 1.0000e-04\n","Epoch 11/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3537 - accuracy: 0.9750\n","Epoch 11: val_loss did not improve from 0.35187\n","56/56 [==============================] - 16s 287ms/step - loss: 0.3537 - accuracy: 0.9750 - val_loss: 0.3532 - val_accuracy: 0.9781 - lr: 1.0000e-04\n","Epoch 12/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3515 - accuracy: 0.9745\n","Epoch 12: val_loss did not improve from 0.35187\n","56/56 [==============================] - 16s 286ms/step - loss: 0.3515 - accuracy: 0.9745 - val_loss: 0.3528 - val_accuracy: 0.9811 - lr: 1.0000e-04\n","Epoch 13/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3496 - accuracy: 0.9770\n","Epoch 13: val_loss improved from 0.35187 to 0.34234, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.3496 - accuracy: 0.9770 - val_loss: 0.3423 - val_accuracy: 0.9796 - lr: 1.0000e-04\n","Epoch 14/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3459 - accuracy: 0.9772\n","Epoch 14: val_loss improved from 0.34234 to 0.34149, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.3459 - accuracy: 0.9772 - val_loss: 0.3415 - val_accuracy: 0.9808 - lr: 1.0000e-04\n","Epoch 15/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3414 - accuracy: 0.9818\n","Epoch 15: val_loss improved from 0.34149 to 0.34067, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.3414 - accuracy: 0.9818 - val_loss: 0.3407 - val_accuracy: 0.9844 - lr: 1.0000e-04\n","Epoch 16/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3388 - accuracy: 0.9827\n","Epoch 16: val_loss improved from 0.34067 to 0.32940, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.3388 - accuracy: 0.9827 - val_loss: 0.3294 - val_accuracy: 0.9849 - lr: 1.0000e-04\n","Epoch 17/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3358 - accuracy: 0.9837\n","Epoch 17: val_loss did not improve from 0.32940\n","56/56 [==============================] - 16s 287ms/step - loss: 0.3358 - accuracy: 0.9837 - val_loss: 0.3359 - val_accuracy: 0.9835 - lr: 1.0000e-04\n","Epoch 18/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3360 - accuracy: 0.9805\n","Epoch 18: val_loss improved from 0.32940 to 0.32685, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.3360 - accuracy: 0.9805 - val_loss: 0.3268 - val_accuracy: 0.9842 - lr: 1.0000e-04\n","Epoch 19/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3310 - accuracy: 0.9839\n","Epoch 19: val_loss did not improve from 0.32685\n","56/56 [==============================] - 16s 288ms/step - loss: 0.3310 - accuracy: 0.9839 - val_loss: 0.3346 - val_accuracy: 0.9862 - lr: 1.0000e-04\n","Epoch 20/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3277 - accuracy: 0.9863\n","Epoch 20: val_loss did not improve from 0.32685\n","56/56 [==============================] - 16s 286ms/step - loss: 0.3277 - accuracy: 0.9863 - val_loss: 0.3312 - val_accuracy: 0.9849 - lr: 1.0000e-04\n","Epoch 21/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.9868\n","Epoch 21: val_loss improved from 0.32685 to 0.32621, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.3237 - accuracy: 0.9868 - val_loss: 0.3262 - val_accuracy: 0.9856 - lr: 1.0000e-04\n","Epoch 22/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3219 - accuracy: 0.9854\n","Epoch 22: val_loss did not improve from 0.32621\n","56/56 [==============================] - 16s 286ms/step - loss: 0.3219 - accuracy: 0.9854 - val_loss: 0.3283 - val_accuracy: 0.9858 - lr: 1.0000e-04\n","Epoch 23/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.9865\n","Epoch 23: val_loss improved from 0.32621 to 0.32316, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.3195 - accuracy: 0.9865 - val_loss: 0.3232 - val_accuracy: 0.9863 - lr: 1.0000e-04\n","Epoch 24/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3197 - accuracy: 0.9849\n","Epoch 24: val_loss improved from 0.32316 to 0.31913, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.3197 - accuracy: 0.9849 - val_loss: 0.3191 - val_accuracy: 0.9828 - lr: 1.0000e-04\n","Epoch 25/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.9837\n","Epoch 25: val_loss improved from 0.31913 to 0.31499, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.3206 - accuracy: 0.9837 - val_loss: 0.3150 - val_accuracy: 0.9848 - lr: 1.0000e-04\n","Epoch 26/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3181 - accuracy: 0.9850\n","Epoch 26: val_loss improved from 0.31499 to 0.31388, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.3181 - accuracy: 0.9850 - val_loss: 0.3139 - val_accuracy: 0.9863 - lr: 1.0000e-04\n","Epoch 27/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3145 - accuracy: 0.9858\n","Epoch 27: val_loss did not improve from 0.31388\n","56/56 [==============================] - 16s 288ms/step - loss: 0.3145 - accuracy: 0.9858 - val_loss: 0.3140 - val_accuracy: 0.9868 - lr: 1.0000e-04\n","Epoch 28/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3147 - accuracy: 0.9840\n","Epoch 28: val_loss did not improve from 0.31388\n","56/56 [==============================] - 16s 287ms/step - loss: 0.3147 - accuracy: 0.9840 - val_loss: 0.3191 - val_accuracy: 0.9831 - lr: 1.0000e-04\n","Epoch 29/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3074 - accuracy: 0.9876\n","Epoch 29: val_loss improved from 0.31388 to 0.31030, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.3074 - accuracy: 0.9876 - val_loss: 0.3103 - val_accuracy: 0.9880 - lr: 1.0000e-04\n","Epoch 30/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.9881\n","Epoch 30: val_loss improved from 0.31030 to 0.30801, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.3075 - accuracy: 0.9881 - val_loss: 0.3080 - val_accuracy: 0.9861 - lr: 1.0000e-04\n","Epoch 31/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.9873\n","Epoch 31: val_loss improved from 0.30801 to 0.30466, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.3049 - accuracy: 0.9873 - val_loss: 0.3047 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 32/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3028 - accuracy: 0.9884\n","Epoch 32: val_loss did not improve from 0.30466\n","56/56 [==============================] - 16s 287ms/step - loss: 0.3028 - accuracy: 0.9884 - val_loss: 0.3066 - val_accuracy: 0.9883 - lr: 1.0000e-04\n","Epoch 33/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2991 - accuracy: 0.9900\n","Epoch 33: val_loss improved from 0.30466 to 0.30197, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2991 - accuracy: 0.9900 - val_loss: 0.3020 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 34/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3000 - accuracy: 0.9883\n","Epoch 34: val_loss did not improve from 0.30197\n","56/56 [==============================] - 16s 286ms/step - loss: 0.3000 - accuracy: 0.9883 - val_loss: 0.3059 - val_accuracy: 0.9831 - lr: 1.0000e-04\n","Epoch 35/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.9873\n","Epoch 35: val_loss did not improve from 0.30197\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2975 - accuracy: 0.9873 - val_loss: 0.3026 - val_accuracy: 0.9841 - lr: 1.0000e-04\n","Epoch 36/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2959 - accuracy: 0.9880\n","Epoch 36: val_loss improved from 0.30197 to 0.29984, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2959 - accuracy: 0.9880 - val_loss: 0.2998 - val_accuracy: 0.9875 - lr: 1.0000e-04\n","Epoch 37/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2979 - accuracy: 0.9849\n","Epoch 37: val_loss did not improve from 0.29984\n","56/56 [==============================] - 16s 288ms/step - loss: 0.2979 - accuracy: 0.9849 - val_loss: 0.3030 - val_accuracy: 0.9846 - lr: 1.0000e-04\n","Epoch 38/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2979 - accuracy: 0.9835\n","Epoch 38: val_loss improved from 0.29984 to 0.29840, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.2979 - accuracy: 0.9835 - val_loss: 0.2984 - val_accuracy: 0.9859 - lr: 1.0000e-04\n","Epoch 39/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2907 - accuracy: 0.9890\n","Epoch 39: val_loss improved from 0.29840 to 0.29420, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2907 - accuracy: 0.9890 - val_loss: 0.2942 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 40/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.9876\n","Epoch 40: val_loss improved from 0.29420 to 0.29105, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 18s 318ms/step - loss: 0.2914 - accuracy: 0.9876 - val_loss: 0.2910 - val_accuracy: 0.9883 - lr: 1.0000e-04\n","Epoch 41/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9875\n","Epoch 41: val_loss improved from 0.29105 to 0.28327, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.2903 - accuracy: 0.9875 - val_loss: 0.2833 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 42/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2832 - accuracy: 0.9894\n","Epoch 42: val_loss did not improve from 0.28327\n","56/56 [==============================] - 16s 288ms/step - loss: 0.2832 - accuracy: 0.9894 - val_loss: 0.2856 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 43/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.9897\n","Epoch 43: val_loss did not improve from 0.28327\n","56/56 [==============================] - 16s 285ms/step - loss: 0.2839 - accuracy: 0.9897 - val_loss: 0.2868 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 44/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.9901\n","Epoch 44: val_loss did not improve from 0.28327\n","56/56 [==============================] - 16s 288ms/step - loss: 0.2811 - accuracy: 0.9901 - val_loss: 0.2845 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 45/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.9876\n","Epoch 45: val_loss improved from 0.28327 to 0.28221, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.2820 - accuracy: 0.9876 - val_loss: 0.2822 - val_accuracy: 0.9817 - lr: 1.0000e-04\n","Epoch 46/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2802 - accuracy: 0.9896\n","Epoch 46: val_loss improved from 0.28221 to 0.28155, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.2802 - accuracy: 0.9896 - val_loss: 0.2816 - val_accuracy: 0.9878 - lr: 1.0000e-04\n","Epoch 47/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2787 - accuracy: 0.9893\n","Epoch 47: val_loss improved from 0.28155 to 0.28033, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2787 - accuracy: 0.9893 - val_loss: 0.2803 - val_accuracy: 0.9883 - lr: 1.0000e-04\n","Epoch 48/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2798 - accuracy: 0.9889\n","Epoch 48: val_loss improved from 0.28033 to 0.27518, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.2798 - accuracy: 0.9889 - val_loss: 0.2752 - val_accuracy: 0.9886 - lr: 1.0000e-04\n","Epoch 49/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.9886\n","Epoch 49: val_loss did not improve from 0.27518\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2723 - accuracy: 0.9886 - val_loss: 0.2781 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 50/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.9902\n","Epoch 50: val_loss improved from 0.27518 to 0.27400, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.2731 - accuracy: 0.9902 - val_loss: 0.2740 - val_accuracy: 0.9882 - lr: 1.0000e-04\n","Epoch 51/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.9906\n","Epoch 51: val_loss improved from 0.27400 to 0.26883, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.2703 - accuracy: 0.9906 - val_loss: 0.2688 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 52/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2686 - accuracy: 0.9910\n","Epoch 52: val_loss improved from 0.26883 to 0.26668, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.2686 - accuracy: 0.9910 - val_loss: 0.2667 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 53/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9912\n","Epoch 53: val_loss did not improve from 0.26668\n","56/56 [==============================] - 16s 285ms/step - loss: 0.2648 - accuracy: 0.9912 - val_loss: 0.2677 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 54/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.9912\n","Epoch 54: val_loss did not improve from 0.26668\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2651 - accuracy: 0.9912 - val_loss: 0.2669 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 55/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.9915\n","Epoch 55: val_loss improved from 0.26668 to 0.26552, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.2624 - accuracy: 0.9915 - val_loss: 0.2655 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 56/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9916\n","Epoch 56: val_loss improved from 0.26552 to 0.26332, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.2625 - accuracy: 0.9916 - val_loss: 0.2633 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 57/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2593 - accuracy: 0.9918\n","Epoch 57: val_loss improved from 0.26332 to 0.26149, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2593 - accuracy: 0.9918 - val_loss: 0.2615 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 58/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2582 - accuracy: 0.9919\n","Epoch 58: val_loss improved from 0.26149 to 0.25998, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.2582 - accuracy: 0.9919 - val_loss: 0.2600 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 59/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9922\n","Epoch 59: val_loss improved from 0.25998 to 0.25448, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.2569 - accuracy: 0.9922 - val_loss: 0.2545 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 60/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.9923\n","Epoch 60: val_loss did not improve from 0.25448\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2547 - accuracy: 0.9923 - val_loss: 0.2568 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 61/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.9921\n","Epoch 61: val_loss did not improve from 0.25448\n","56/56 [==============================] - 16s 285ms/step - loss: 0.2525 - accuracy: 0.9921 - val_loss: 0.2551 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 62/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9917\n","Epoch 62: val_loss improved from 0.25448 to 0.25431, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2506 - accuracy: 0.9917 - val_loss: 0.2543 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 63/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.9918\n","Epoch 63: val_loss improved from 0.25431 to 0.25053, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2501 - accuracy: 0.9918 - val_loss: 0.2505 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 64/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.9926\n","Epoch 64: val_loss did not improve from 0.25053\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2489 - accuracy: 0.9926 - val_loss: 0.2513 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 65/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.9926\n","Epoch 65: val_loss improved from 0.25053 to 0.24999, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.2478 - accuracy: 0.9926 - val_loss: 0.2500 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 66/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.9921\n","Epoch 66: val_loss improved from 0.24999 to 0.24642, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2471 - accuracy: 0.9921 - val_loss: 0.2464 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 67/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.9921\n","Epoch 67: val_loss did not improve from 0.24642\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2464 - accuracy: 0.9921 - val_loss: 0.2470 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 68/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.9924\n","Epoch 68: val_loss improved from 0.24642 to 0.24376, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.2419 - accuracy: 0.9924 - val_loss: 0.2438 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 69/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.9921\n","Epoch 69: val_loss improved from 0.24376 to 0.24140, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.2443 - accuracy: 0.9921 - val_loss: 0.2414 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 70/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2423 - accuracy: 0.9916\n","Epoch 70: val_loss did not improve from 0.24140\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2423 - accuracy: 0.9916 - val_loss: 0.2452 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 71/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2395 - accuracy: 0.9907\n","Epoch 71: val_loss improved from 0.24140 to 0.24095, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.2395 - accuracy: 0.9907 - val_loss: 0.2409 - val_accuracy: 0.9878 - lr: 1.0000e-04\n","Epoch 72/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2402 - accuracy: 0.9907\n","Epoch 72: val_loss improved from 0.24095 to 0.23929, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.2402 - accuracy: 0.9907 - val_loss: 0.2393 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 73/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9917\n","Epoch 73: val_loss improved from 0.23929 to 0.23651, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 18s 319ms/step - loss: 0.2405 - accuracy: 0.9917 - val_loss: 0.2365 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 74/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2386 - accuracy: 0.9914\n","Epoch 74: val_loss did not improve from 0.23651\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2386 - accuracy: 0.9914 - val_loss: 0.2366 - val_accuracy: 0.9872 - lr: 1.0000e-04\n","Epoch 75/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.9924\n","Epoch 75: val_loss improved from 0.23651 to 0.23488, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2343 - accuracy: 0.9924 - val_loss: 0.2349 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 76/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2323 - accuracy: 0.9925\n","Epoch 76: val_loss improved from 0.23488 to 0.23247, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.2323 - accuracy: 0.9925 - val_loss: 0.2325 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 77/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2304 - accuracy: 0.9927\n","Epoch 77: val_loss improved from 0.23247 to 0.23096, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2304 - accuracy: 0.9927 - val_loss: 0.2310 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 78/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2348 - accuracy: 0.9900\n","Epoch 78: val_loss improved from 0.23096 to 0.22672, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2348 - accuracy: 0.9900 - val_loss: 0.2267 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 79/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2298 - accuracy: 0.9913\n","Epoch 79: val_loss did not improve from 0.22672\n","56/56 [==============================] - 16s 288ms/step - loss: 0.2298 - accuracy: 0.9913 - val_loss: 0.2331 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 80/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 0.9869\n","Epoch 80: val_loss did not improve from 0.22672\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2337 - accuracy: 0.9869 - val_loss: 0.2337 - val_accuracy: 0.9859 - lr: 1.0000e-04\n","Epoch 81/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.9901\n","Epoch 81: val_loss did not improve from 0.22672\n","56/56 [==============================] - 16s 288ms/step - loss: 0.2284 - accuracy: 0.9901 - val_loss: 0.2271 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 82/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.9921\n","Epoch 82: val_loss did not improve from 0.22672\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2255 - accuracy: 0.9921 - val_loss: 0.2279 - val_accuracy: 0.9876 - lr: 1.0000e-04\n","Epoch 83/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2237 - accuracy: 0.9921\n","Epoch 83: val_loss improved from 0.22672 to 0.22399, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2237 - accuracy: 0.9921 - val_loss: 0.2240 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 84/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2259 - accuracy: 0.9904\n","Epoch 84: val_loss improved from 0.22399 to 0.22306, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2259 - accuracy: 0.9904 - val_loss: 0.2231 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 85/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.9920\n","Epoch 85: val_loss improved from 0.22306 to 0.22138, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2211 - accuracy: 0.9920 - val_loss: 0.2214 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 86/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.9930\n","Epoch 86: val_loss improved from 0.22138 to 0.22106, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.2185 - accuracy: 0.9930 - val_loss: 0.2211 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 87/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2180 - accuracy: 0.9929\n","Epoch 87: val_loss improved from 0.22106 to 0.21912, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.2180 - accuracy: 0.9929 - val_loss: 0.2191 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 88/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9928\n","Epoch 88: val_loss improved from 0.21912 to 0.21787, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2189 - accuracy: 0.9928 - val_loss: 0.2179 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 89/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.9930\n","Epoch 89: val_loss improved from 0.21787 to 0.21755, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.2174 - accuracy: 0.9930 - val_loss: 0.2176 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 90/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2170 - accuracy: 0.9924\n","Epoch 90: val_loss did not improve from 0.21755\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2170 - accuracy: 0.9924 - val_loss: 0.2206 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 91/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2202 - accuracy: 0.9872\n","Epoch 91: val_loss improved from 0.21755 to 0.21456, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.2202 - accuracy: 0.9872 - val_loss: 0.2146 - val_accuracy: 0.9873 - lr: 1.0000e-04\n","Epoch 92/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 0.9911\n","Epoch 92: val_loss did not improve from 0.21456\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2161 - accuracy: 0.9911 - val_loss: 0.2151 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 93/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9927\n","Epoch 93: val_loss improved from 0.21456 to 0.21396, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 18s 320ms/step - loss: 0.2109 - accuracy: 0.9927 - val_loss: 0.2140 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 94/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.9928\n","Epoch 94: val_loss improved from 0.21396 to 0.21083, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.2115 - accuracy: 0.9928 - val_loss: 0.2108 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 95/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2085 - accuracy: 0.9933\n","Epoch 95: val_loss improved from 0.21083 to 0.21023, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.2085 - accuracy: 0.9933 - val_loss: 0.2102 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 96/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9933\n","Epoch 96: val_loss improved from 0.21023 to 0.20895, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.2053 - accuracy: 0.9933 - val_loss: 0.2089 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 97/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.9913\n","Epoch 97: val_loss improved from 0.20895 to 0.20186, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.2086 - accuracy: 0.9913 - val_loss: 0.2019 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 98/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9929\n","Epoch 98: val_loss did not improve from 0.20186\n","56/56 [==============================] - 17s 311ms/step - loss: 0.2064 - accuracy: 0.9929 - val_loss: 0.2055 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 99/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2021 - accuracy: 0.9935\n","Epoch 99: val_loss did not improve from 0.20186\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2021 - accuracy: 0.9935 - val_loss: 0.2065 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 100/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2017 - accuracy: 0.9937\n","Epoch 100: val_loss did not improve from 0.20186\n","56/56 [==============================] - 16s 288ms/step - loss: 0.2017 - accuracy: 0.9937 - val_loss: 0.2032 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 101/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9863\n","Epoch 101: val_loss did not improve from 0.20186\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2111 - accuracy: 0.9863 - val_loss: 0.2042 - val_accuracy: 0.9864 - lr: 1.0000e-04\n","Epoch 102/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9924\n","Epoch 102: val_loss did not improve from 0.20186\n","56/56 [==============================] - 16s 288ms/step - loss: 0.2053 - accuracy: 0.9924 - val_loss: 0.2043 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 103/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.9923\n","Epoch 103: val_loss did not improve from 0.20186\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2015 - accuracy: 0.9923 - val_loss: 0.2037 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 104/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9922\n","Epoch 104: val_loss did not improve from 0.20186\n","56/56 [==============================] - 16s 288ms/step - loss: 0.2005 - accuracy: 0.9922 - val_loss: 0.2025 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 105/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.9912\n","Epoch 105: val_loss improved from 0.20186 to 0.19989, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.2000 - accuracy: 0.9912 - val_loss: 0.1999 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 106/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1962 - accuracy: 0.9934\n","Epoch 106: val_loss improved from 0.19989 to 0.19771, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.1962 - accuracy: 0.9934 - val_loss: 0.1977 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 107/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9933\n","Epoch 107: val_loss improved from 0.19771 to 0.19468, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.1938 - accuracy: 0.9933 - val_loss: 0.1947 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 108/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9928\n","Epoch 108: val_loss did not improve from 0.19468\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1942 - accuracy: 0.9928 - val_loss: 0.1977 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 109/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9925\n","Epoch 109: val_loss did not improve from 0.19468\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1950 - accuracy: 0.9925 - val_loss: 0.1997 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 110/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1911 - accuracy: 0.9933\n","Epoch 110: val_loss improved from 0.19468 to 0.19283, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1911 - accuracy: 0.9933 - val_loss: 0.1928 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 111/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1913 - accuracy: 0.9938\n","Epoch 111: val_loss improved from 0.19283 to 0.18824, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1913 - accuracy: 0.9938 - val_loss: 0.1882 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 112/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1892 - accuracy: 0.9940\n","Epoch 112: val_loss did not improve from 0.18824\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1892 - accuracy: 0.9940 - val_loss: 0.1891 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 113/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.9942\n","Epoch 113: val_loss did not improve from 0.18824\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1869 - accuracy: 0.9942 - val_loss: 0.1886 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 114/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1867 - accuracy: 0.9940\n","Epoch 114: val_loss improved from 0.18824 to 0.18650, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1867 - accuracy: 0.9940 - val_loss: 0.1865 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 115/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1847 - accuracy: 0.9942\n","Epoch 115: val_loss improved from 0.18650 to 0.18429, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.1847 - accuracy: 0.9942 - val_loss: 0.1843 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 116/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1857 - accuracy: 0.9943\n","Epoch 116: val_loss improved from 0.18429 to 0.18416, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1857 - accuracy: 0.9943 - val_loss: 0.1842 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 117/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1857 - accuracy: 0.9942\n","Epoch 117: val_loss improved from 0.18416 to 0.18293, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.1857 - accuracy: 0.9942 - val_loss: 0.1829 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 118/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1827 - accuracy: 0.9946\n","Epoch 118: val_loss improved from 0.18293 to 0.18123, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1827 - accuracy: 0.9946 - val_loss: 0.1812 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 119/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9943\n","Epoch 119: val_loss improved from 0.18123 to 0.18021, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1807 - accuracy: 0.9943 - val_loss: 0.1802 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 120/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1830 - accuracy: 0.9943\n","Epoch 120: val_loss improved from 0.18021 to 0.18009, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1830 - accuracy: 0.9943 - val_loss: 0.1801 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 121/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9945\n","Epoch 121: val_loss improved from 0.18009 to 0.17864, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.1800 - accuracy: 0.9945 - val_loss: 0.1786 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 122/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9946\n","Epoch 122: val_loss improved from 0.17864 to 0.17620, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1774 - accuracy: 0.9946 - val_loss: 0.1762 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 123/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1761 - accuracy: 0.9946\n","Epoch 123: val_loss improved from 0.17620 to 0.17612, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.1761 - accuracy: 0.9946 - val_loss: 0.1761 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 124/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1793 - accuracy: 0.9946\n","Epoch 124: val_loss improved from 0.17612 to 0.17360, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1793 - accuracy: 0.9946 - val_loss: 0.1736 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 125/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9946\n","Epoch 125: val_loss did not improve from 0.17360\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1770 - accuracy: 0.9946 - val_loss: 0.1749 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 126/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9947\n","Epoch 126: val_loss improved from 0.17360 to 0.17338, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.1765 - accuracy: 0.9947 - val_loss: 0.1734 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 127/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9946\n","Epoch 127: val_loss improved from 0.17338 to 0.17316, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.1734 - accuracy: 0.9946 - val_loss: 0.1732 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 128/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9945\n","Epoch 128: val_loss improved from 0.17316 to 0.16911, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1731 - accuracy: 0.9945 - val_loss: 0.1691 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 129/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9946\n","Epoch 129: val_loss did not improve from 0.16911\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1712 - accuracy: 0.9946 - val_loss: 0.1708 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 130/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9948\n","Epoch 130: val_loss improved from 0.16911 to 0.16857, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1701 - accuracy: 0.9948 - val_loss: 0.1686 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 131/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9947\n","Epoch 131: val_loss did not improve from 0.16857\n","56/56 [==============================] - 16s 289ms/step - loss: 0.1711 - accuracy: 0.9947 - val_loss: 0.1687 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 132/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9892\n","Epoch 132: val_loss did not improve from 0.16857\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1800 - accuracy: 0.9892 - val_loss: 0.1748 - val_accuracy: 0.9792 - lr: 1.0000e-04\n","Epoch 133/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9898\n","Epoch 133: val_loss improved from 0.16857 to 0.16104, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.1729 - accuracy: 0.9898 - val_loss: 0.1610 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 134/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.9907\n","Epoch 134: val_loss did not improve from 0.16104\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1706 - accuracy: 0.9907 - val_loss: 0.1729 - val_accuracy: 0.9868 - lr: 1.0000e-04\n","Epoch 135/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9921\n","Epoch 135: val_loss did not improve from 0.16104\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1700 - accuracy: 0.9921 - val_loss: 0.1663 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 136/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9931\n","Epoch 136: val_loss did not improve from 0.16104\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1682 - accuracy: 0.9931 - val_loss: 0.1666 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 137/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9938\n","Epoch 137: val_loss did not improve from 0.16104\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1678 - accuracy: 0.9938 - val_loss: 0.1726 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 138/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9941\n","Epoch 138: val_loss did not improve from 0.16104\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1641 - accuracy: 0.9941 - val_loss: 0.1658 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 139/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9933\n","Epoch 139: val_loss improved from 0.16104 to 0.15940, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1657 - accuracy: 0.9933 - val_loss: 0.1594 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 140/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1617 - accuracy: 0.9943\n","Epoch 140: val_loss did not improve from 0.15940\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1617 - accuracy: 0.9943 - val_loss: 0.1618 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 141/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9944\n","Epoch 141: val_loss did not improve from 0.15940\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1624 - accuracy: 0.9944 - val_loss: 0.1600 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 142/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9943\n","Epoch 142: val_loss did not improve from 0.15940\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1602 - accuracy: 0.9943 - val_loss: 0.1597 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 143/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9946\n","Epoch 143: val_loss improved from 0.15940 to 0.15787, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1581 - accuracy: 0.9946 - val_loss: 0.1579 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 144/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9948\n","Epoch 144: val_loss did not improve from 0.15787\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1573 - accuracy: 0.9948 - val_loss: 0.1579 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 145/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1582 - accuracy: 0.9949\n","Epoch 145: val_loss improved from 0.15787 to 0.15659, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1582 - accuracy: 0.9949 - val_loss: 0.1566 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 146/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9948\n","Epoch 146: val_loss improved from 0.15659 to 0.15389, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 18s 319ms/step - loss: 0.1572 - accuracy: 0.9948 - val_loss: 0.1539 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 147/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1562 - accuracy: 0.9948\n","Epoch 147: val_loss did not improve from 0.15389\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1562 - accuracy: 0.9948 - val_loss: 0.1544 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 148/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1552 - accuracy: 0.9951\n","Epoch 148: val_loss improved from 0.15389 to 0.15337, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1552 - accuracy: 0.9951 - val_loss: 0.1534 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 149/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1556 - accuracy: 0.9948\n","Epoch 149: val_loss improved from 0.15337 to 0.15249, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.1556 - accuracy: 0.9948 - val_loss: 0.1525 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 150/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1549 - accuracy: 0.9952\n","Epoch 150: val_loss improved from 0.15249 to 0.15190, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.1549 - accuracy: 0.9952 - val_loss: 0.1519 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 151/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9951\n","Epoch 151: val_loss improved from 0.15190 to 0.14961, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1540 - accuracy: 0.9951 - val_loss: 0.1496 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 152/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.9953\n","Epoch 152: val_loss improved from 0.14961 to 0.14907, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.1499 - accuracy: 0.9953 - val_loss: 0.1491 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 153/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.9954\n","Epoch 153: val_loss improved from 0.14907 to 0.14883, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1509 - accuracy: 0.9954 - val_loss: 0.1488 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 154/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1511 - accuracy: 0.9953\n","Epoch 154: val_loss improved from 0.14883 to 0.14878, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1511 - accuracy: 0.9953 - val_loss: 0.1488 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 155/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1492 - accuracy: 0.9949\n","Epoch 155: val_loss improved from 0.14878 to 0.14575, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1492 - accuracy: 0.9949 - val_loss: 0.1458 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 156/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1458 - accuracy: 0.9953\n","Epoch 156: val_loss did not improve from 0.14575\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1458 - accuracy: 0.9953 - val_loss: 0.1464 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 157/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.9954\n","Epoch 157: val_loss improved from 0.14575 to 0.14445, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1479 - accuracy: 0.9954 - val_loss: 0.1444 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 158/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1459 - accuracy: 0.9955\n","Epoch 158: val_loss did not improve from 0.14445\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1459 - accuracy: 0.9955 - val_loss: 0.1449 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 159/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1449 - accuracy: 0.9957\n","Epoch 159: val_loss did not improve from 0.14445\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1449 - accuracy: 0.9957 - val_loss: 0.1447 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 160/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9957\n","Epoch 160: val_loss improved from 0.14445 to 0.14306, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1446 - accuracy: 0.9957 - val_loss: 0.1431 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 161/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1438 - accuracy: 0.9955\n","Epoch 161: val_loss improved from 0.14306 to 0.14016, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.1438 - accuracy: 0.9955 - val_loss: 0.1402 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 162/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.9957\n","Epoch 162: val_loss did not improve from 0.14016\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1410 - accuracy: 0.9957 - val_loss: 0.1405 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 163/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9955\n","Epoch 163: val_loss improved from 0.14016 to 0.13991, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1440 - accuracy: 0.9955 - val_loss: 0.1399 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 164/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9955\n","Epoch 164: val_loss did not improve from 0.13991\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1414 - accuracy: 0.9955 - val_loss: 0.1399 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 165/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1429 - accuracy: 0.9956\n","Epoch 165: val_loss did not improve from 0.13991\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1429 - accuracy: 0.9956 - val_loss: 0.1399 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 166/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9954\n","Epoch 166: val_loss improved from 0.13991 to 0.13759, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 299ms/step - loss: 0.1423 - accuracy: 0.9954 - val_loss: 0.1376 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 167/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1395 - accuracy: 0.9955\n","Epoch 167: val_loss did not improve from 0.13759\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1395 - accuracy: 0.9955 - val_loss: 0.1380 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 168/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1441 - accuracy: 0.9946\n","Epoch 168: val_loss did not improve from 0.13759\n","56/56 [==============================] - 17s 312ms/step - loss: 0.1441 - accuracy: 0.9946 - val_loss: 0.1413 - val_accuracy: 0.9891 - lr: 1.0000e-04\n","Epoch 169/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9952\n","Epoch 169: val_loss did not improve from 0.13759\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1379 - accuracy: 0.9952 - val_loss: 0.1388 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 170/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9951\n","Epoch 170: val_loss did not improve from 0.13759\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1379 - accuracy: 0.9951 - val_loss: 0.1386 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 171/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9956\n","Epoch 171: val_loss improved from 0.13759 to 0.13547, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1362 - accuracy: 0.9956 - val_loss: 0.1355 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 172/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9956\n","Epoch 172: val_loss improved from 0.13547 to 0.13535, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1363 - accuracy: 0.9956 - val_loss: 0.1354 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 173/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9949\n","Epoch 173: val_loss did not improve from 0.13535\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1362 - accuracy: 0.9949 - val_loss: 0.1375 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 174/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9950\n","Epoch 174: val_loss improved from 0.13535 to 0.13208, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1353 - accuracy: 0.9950 - val_loss: 0.1321 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 175/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1338 - accuracy: 0.9953\n","Epoch 175: val_loss did not improve from 0.13208\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1338 - accuracy: 0.9953 - val_loss: 0.1330 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 176/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9904\n","Epoch 176: val_loss did not improve from 0.13208\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1412 - accuracy: 0.9904 - val_loss: 0.1333 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 177/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9913\n","Epoch 177: val_loss did not improve from 0.13208\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1403 - accuracy: 0.9913 - val_loss: 0.1422 - val_accuracy: 0.9848 - lr: 1.0000e-04\n","Epoch 178/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9937\n","Epoch 178: val_loss did not improve from 0.13208\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1331 - accuracy: 0.9937 - val_loss: 0.1323 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 179/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1345 - accuracy: 0.9943\n","Epoch 179: val_loss did not improve from 0.13208\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1345 - accuracy: 0.9943 - val_loss: 0.1335 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 180/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9925\n","Epoch 180: val_loss did not improve from 0.13208\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1335 - accuracy: 0.9925 - val_loss: 0.1350 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 181/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1320 - accuracy: 0.9941\n","Epoch 181: val_loss did not improve from 0.13208\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1320 - accuracy: 0.9941 - val_loss: 0.1341 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 182/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9946\n","Epoch 182: val_loss improved from 0.13208 to 0.13007, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1317 - accuracy: 0.9946 - val_loss: 0.1301 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 183/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1310 - accuracy: 0.9951\n","Epoch 183: val_loss did not improve from 0.13007\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1310 - accuracy: 0.9951 - val_loss: 0.1309 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 184/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9954\n","Epoch 184: val_loss improved from 0.13007 to 0.12849, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1281 - accuracy: 0.9954 - val_loss: 0.1285 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 185/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.9954\n","Epoch 185: val_loss improved from 0.12849 to 0.12458, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1273 - accuracy: 0.9954 - val_loss: 0.1246 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 186/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9955\n","Epoch 186: val_loss did not improve from 0.12458\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1280 - accuracy: 0.9955 - val_loss: 0.1247 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 187/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9956\n","Epoch 187: val_loss improved from 0.12458 to 0.12418, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1267 - accuracy: 0.9956 - val_loss: 0.1242 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 188/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9957\n","Epoch 188: val_loss improved from 0.12418 to 0.12416, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1252 - accuracy: 0.9957 - val_loss: 0.1242 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 189/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9959\n","Epoch 189: val_loss did not improve from 0.12416\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1252 - accuracy: 0.9959 - val_loss: 0.1243 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 190/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.9957\n","Epoch 190: val_loss improved from 0.12416 to 0.12197, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1243 - accuracy: 0.9957 - val_loss: 0.1220 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 191/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9959\n","Epoch 191: val_loss improved from 0.12197 to 0.12168, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1239 - accuracy: 0.9959 - val_loss: 0.1217 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 192/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9957\n","Epoch 192: val_loss improved from 0.12168 to 0.11908, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1235 - accuracy: 0.9957 - val_loss: 0.1191 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 193/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1218 - accuracy: 0.9960\n","Epoch 193: val_loss did not improve from 0.11908\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1218 - accuracy: 0.9960 - val_loss: 0.1195 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 194/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9961\n","Epoch 194: val_loss improved from 0.11908 to 0.11906, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1211 - accuracy: 0.9961 - val_loss: 0.1191 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 195/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9960\n","Epoch 195: val_loss improved from 0.11906 to 0.11785, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.1200 - accuracy: 0.9960 - val_loss: 0.1178 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 196/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9960\n","Epoch 196: val_loss improved from 0.11785 to 0.11684, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1219 - accuracy: 0.9960 - val_loss: 0.1168 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 197/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1171 - accuracy: 0.9959\n","Epoch 197: val_loss improved from 0.11684 to 0.11662, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1171 - accuracy: 0.9959 - val_loss: 0.1166 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 198/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9961\n","Epoch 198: val_loss improved from 0.11662 to 0.11575, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1156 - accuracy: 0.9961 - val_loss: 0.1157 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 199/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9962\n","Epoch 199: val_loss improved from 0.11575 to 0.11543, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 299ms/step - loss: 0.1160 - accuracy: 0.9962 - val_loss: 0.1154 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 200/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9962\n","Epoch 200: val_loss improved from 0.11543 to 0.11453, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1166 - accuracy: 0.9962 - val_loss: 0.1145 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 201/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9962\n","Epoch 201: val_loss improved from 0.11453 to 0.11406, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.1163 - accuracy: 0.9962 - val_loss: 0.1141 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 202/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9963\n","Epoch 202: val_loss improved from 0.11406 to 0.11369, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1146 - accuracy: 0.9963 - val_loss: 0.1137 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 203/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9960\n","Epoch 203: val_loss improved from 0.11369 to 0.11331, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.1169 - accuracy: 0.9960 - val_loss: 0.1133 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 204/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9961\n","Epoch 204: val_loss improved from 0.11331 to 0.11185, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.1172 - accuracy: 0.9961 - val_loss: 0.1118 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 205/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9963\n","Epoch 205: val_loss did not improve from 0.11185\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1140 - accuracy: 0.9963 - val_loss: 0.1124 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 206/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.9962\n","Epoch 206: val_loss improved from 0.11185 to 0.11055, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1144 - accuracy: 0.9962 - val_loss: 0.1105 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 207/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.9963\n","Epoch 207: val_loss improved from 0.11055 to 0.11046, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1159 - accuracy: 0.9963 - val_loss: 0.1105 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 208/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9964\n","Epoch 208: val_loss improved from 0.11046 to 0.11024, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.1123 - accuracy: 0.9964 - val_loss: 0.1102 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 209/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9963\n","Epoch 209: val_loss improved from 0.11024 to 0.10666, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1112 - accuracy: 0.9963 - val_loss: 0.1067 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 210/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9960\n","Epoch 210: val_loss did not improve from 0.10666\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1147 - accuracy: 0.9960 - val_loss: 0.1186 - val_accuracy: 0.9893 - lr: 1.0000e-04\n","Epoch 211/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9931\n","Epoch 211: val_loss did not improve from 0.10666\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1155 - accuracy: 0.9931 - val_loss: 0.1189 - val_accuracy: 0.9891 - lr: 1.0000e-04\n","Epoch 212/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.9898\n","Epoch 212: val_loss did not improve from 0.10666\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1196 - accuracy: 0.9898 - val_loss: 0.1353 - val_accuracy: 0.9747 - lr: 1.0000e-04\n","Epoch 213/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9918\n","Epoch 213: val_loss did not improve from 0.10666\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1155 - accuracy: 0.9918 - val_loss: 0.1074 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 214/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9944\n","Epoch 214: val_loss did not improve from 0.10666\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1127 - accuracy: 0.9944 - val_loss: 0.1108 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 215/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9948\n","Epoch 215: val_loss did not improve from 0.10666\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1077 - accuracy: 0.9948 - val_loss: 0.1100 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 216/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9950\n","Epoch 216: val_loss did not improve from 0.10666\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1108 - accuracy: 0.9950 - val_loss: 0.1121 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 217/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 0.9953\n","Epoch 217: val_loss did not improve from 0.10666\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1088 - accuracy: 0.9953 - val_loss: 0.1079 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 218/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9958\n","Epoch 218: val_loss improved from 0.10666 to 0.10646, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.1070 - accuracy: 0.9958 - val_loss: 0.1065 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 219/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9959\n","Epoch 219: val_loss improved from 0.10646 to 0.10530, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.1061 - accuracy: 0.9959 - val_loss: 0.1053 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 220/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1090 - accuracy: 0.9960\n","Epoch 220: val_loss did not improve from 0.10530\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1090 - accuracy: 0.9960 - val_loss: 0.1061 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 221/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9960\n","Epoch 221: val_loss improved from 0.10530 to 0.10401, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1070 - accuracy: 0.9960 - val_loss: 0.1040 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 222/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1067 - accuracy: 0.9962\n","Epoch 222: val_loss improved from 0.10401 to 0.10373, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.1067 - accuracy: 0.9962 - val_loss: 0.1037 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 223/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9963\n","Epoch 223: val_loss improved from 0.10373 to 0.10282, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1061 - accuracy: 0.9963 - val_loss: 0.1028 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 224/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9964\n","Epoch 224: val_loss improved from 0.10282 to 0.10204, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1043 - accuracy: 0.9964 - val_loss: 0.1020 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 225/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9964\n","Epoch 225: val_loss improved from 0.10204 to 0.10055, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.1019 - accuracy: 0.9964 - val_loss: 0.1005 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 226/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9964\n","Epoch 226: val_loss did not improve from 0.10055\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1027 - accuracy: 0.9964 - val_loss: 0.1006 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 227/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.9960\n","Epoch 227: val_loss did not improve from 0.10055\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1032 - accuracy: 0.9960 - val_loss: 0.1051 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 228/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9945\n","Epoch 228: val_loss did not improve from 0.10055\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1035 - accuracy: 0.9945 - val_loss: 0.1026 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 229/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9957\n","Epoch 229: val_loss improved from 0.10055 to 0.09857, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1039 - accuracy: 0.9957 - val_loss: 0.0986 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 230/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9941\n","Epoch 230: val_loss did not improve from 0.09857\n","56/56 [==============================] - 16s 288ms/step - loss: 0.1042 - accuracy: 0.9941 - val_loss: 0.1062 - val_accuracy: 0.9887 - lr: 1.0000e-04\n","Epoch 231/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9950\n","Epoch 231: val_loss did not improve from 0.09857\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1019 - accuracy: 0.9950 - val_loss: 0.0991 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 232/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9952\n","Epoch 232: val_loss improved from 0.09857 to 0.09854, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.1013 - accuracy: 0.9952 - val_loss: 0.0985 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 233/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9960\n","Epoch 233: val_loss improved from 0.09854 to 0.09839, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0978 - accuracy: 0.9960 - val_loss: 0.0984 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 234/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0982 - accuracy: 0.9961\n","Epoch 234: val_loss improved from 0.09839 to 0.09558, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 300ms/step - loss: 0.0982 - accuracy: 0.9961 - val_loss: 0.0956 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 235/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9962\n","Epoch 235: val_loss did not improve from 0.09558\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0977 - accuracy: 0.9962 - val_loss: 0.0960 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 236/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0995 - accuracy: 0.9962\n","Epoch 236: val_loss improved from 0.09558 to 0.09481, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 18s 319ms/step - loss: 0.0995 - accuracy: 0.9962 - val_loss: 0.0948 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 237/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9964\n","Epoch 237: val_loss did not improve from 0.09481\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0988 - accuracy: 0.9964 - val_loss: 0.0961 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 238/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9963\n","Epoch 238: val_loss improved from 0.09481 to 0.09320, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0991 - accuracy: 0.9963 - val_loss: 0.0932 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 239/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9963\n","Epoch 239: val_loss did not improve from 0.09320\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1003 - accuracy: 0.9963 - val_loss: 0.0939 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 240/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9964\n","Epoch 240: val_loss did not improve from 0.09320\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0973 - accuracy: 0.9964 - val_loss: 0.0937 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 241/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.9965\n","Epoch 241: val_loss improved from 0.09320 to 0.09221, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0941 - accuracy: 0.9965 - val_loss: 0.0922 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 242/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.9960\n","Epoch 242: val_loss improved from 0.09221 to 0.09194, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0974 - accuracy: 0.9960 - val_loss: 0.0919 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 243/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0945 - accuracy: 0.9965\n","Epoch 243: val_loss improved from 0.09194 to 0.09067, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0945 - accuracy: 0.9965 - val_loss: 0.0907 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 244/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9967\n","Epoch 244: val_loss did not improve from 0.09067\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0931 - accuracy: 0.9967 - val_loss: 0.0918 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 245/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0945 - accuracy: 0.9966\n","Epoch 245: val_loss did not improve from 0.09067\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0945 - accuracy: 0.9966 - val_loss: 0.0918 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 246/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9964\n","Epoch 246: val_loss improved from 0.09067 to 0.09053, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.0959 - accuracy: 0.9964 - val_loss: 0.0905 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 247/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9966\n","Epoch 247: val_loss improved from 0.09053 to 0.09033, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.0922 - accuracy: 0.9966 - val_loss: 0.0903 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 248/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.9896\n","Epoch 248: val_loss improved from 0.09033 to 0.09022, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.1034 - accuracy: 0.9896 - val_loss: 0.0902 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 249/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9929\n","Epoch 249: val_loss improved from 0.09022 to 0.08918, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0970 - accuracy: 0.9929 - val_loss: 0.0892 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 250/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9913\n","Epoch 250: val_loss improved from 0.08918 to 0.08656, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.1017 - accuracy: 0.9913 - val_loss: 0.0866 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 251/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9939\n","Epoch 251: val_loss did not improve from 0.08656\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0943 - accuracy: 0.9939 - val_loss: 0.0962 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 252/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9939\n","Epoch 252: val_loss did not improve from 0.08656\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0951 - accuracy: 0.9939 - val_loss: 0.0930 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 253/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0924 - accuracy: 0.9954\n","Epoch 253: val_loss did not improve from 0.08656\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0924 - accuracy: 0.9954 - val_loss: 0.0919 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 254/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0939 - accuracy: 0.9956\n","Epoch 254: val_loss did not improve from 0.08656\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0939 - accuracy: 0.9956 - val_loss: 0.0896 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 255/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9956\n","Epoch 255: val_loss did not improve from 0.08656\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0898 - accuracy: 0.9956 - val_loss: 0.0890 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 256/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9955\n","Epoch 256: val_loss did not improve from 0.08656\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0898 - accuracy: 0.9955 - val_loss: 0.0918 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 257/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9961\n","Epoch 257: val_loss did not improve from 0.08656\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0876 - accuracy: 0.9961 - val_loss: 0.0873 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 258/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9947\n","Epoch 258: val_loss did not improve from 0.08656\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0908 - accuracy: 0.9947 - val_loss: 0.0874 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 259/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9960\n","Epoch 259: val_loss did not improve from 0.08656\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0880 - accuracy: 0.9960 - val_loss: 0.0868 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 260/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9963\n","Epoch 260: val_loss improved from 0.08656 to 0.08465, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0861 - accuracy: 0.9963 - val_loss: 0.0847 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 261/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9964\n","Epoch 261: val_loss improved from 0.08465 to 0.08383, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.0875 - accuracy: 0.9964 - val_loss: 0.0838 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 262/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9964\n","Epoch 262: val_loss did not improve from 0.08383\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0878 - accuracy: 0.9964 - val_loss: 0.0850 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 263/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9966\n","Epoch 263: val_loss improved from 0.08383 to 0.08372, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0857 - accuracy: 0.9966 - val_loss: 0.0837 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 264/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9966\n","Epoch 264: val_loss improved from 0.08372 to 0.08280, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0849 - accuracy: 0.9966 - val_loss: 0.0828 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 265/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9966\n","Epoch 265: val_loss did not improve from 0.08280\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0862 - accuracy: 0.9966 - val_loss: 0.0834 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 266/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9967\n","Epoch 266: val_loss improved from 0.08280 to 0.08201, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.0834 - accuracy: 0.9967 - val_loss: 0.0820 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 267/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9966\n","Epoch 267: val_loss improved from 0.08201 to 0.08082, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0855 - accuracy: 0.9966 - val_loss: 0.0808 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 268/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9967\n","Epoch 268: val_loss improved from 0.08082 to 0.08054, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0823 - accuracy: 0.9967 - val_loss: 0.0805 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 269/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9968\n","Epoch 269: val_loss did not improve from 0.08054\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0812 - accuracy: 0.9968 - val_loss: 0.0811 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 270/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9958\n","Epoch 270: val_loss did not improve from 0.08054\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0869 - accuracy: 0.9958 - val_loss: 0.0813 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 271/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9963\n","Epoch 271: val_loss improved from 0.08054 to 0.08042, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 298ms/step - loss: 0.0829 - accuracy: 0.9963 - val_loss: 0.0804 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 272/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9966\n","Epoch 272: val_loss improved from 0.08042 to 0.08001, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.0814 - accuracy: 0.9966 - val_loss: 0.0800 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 273/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9965\n","Epoch 273: val_loss did not improve from 0.08001\n","56/56 [==============================] - 17s 309ms/step - loss: 0.0830 - accuracy: 0.9965 - val_loss: 0.0809 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 274/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9966\n","Epoch 274: val_loss improved from 0.08001 to 0.07981, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.0816 - accuracy: 0.9966 - val_loss: 0.0798 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 275/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9967\n","Epoch 275: val_loss did not improve from 0.07981\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0846 - accuracy: 0.9967 - val_loss: 0.0802 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 276/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9966\n","Epoch 276: val_loss improved from 0.07981 to 0.07773, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0827 - accuracy: 0.9966 - val_loss: 0.0777 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 277/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9968\n","Epoch 277: val_loss did not improve from 0.07773\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0800 - accuracy: 0.9968 - val_loss: 0.0791 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 278/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9968\n","Epoch 278: val_loss did not improve from 0.07773\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0809 - accuracy: 0.9968 - val_loss: 0.0779 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 279/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9969\n","Epoch 279: val_loss did not improve from 0.07773\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0799 - accuracy: 0.9969 - val_loss: 0.0781 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 280/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9969\n","Epoch 280: val_loss improved from 0.07773 to 0.07712, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.0769 - accuracy: 0.9969 - val_loss: 0.0771 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 281/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9970\n","Epoch 281: val_loss improved from 0.07712 to 0.07569, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.0757 - accuracy: 0.9970 - val_loss: 0.0757 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 282/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9970\n","Epoch 282: val_loss did not improve from 0.07569\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0777 - accuracy: 0.9970 - val_loss: 0.0770 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 283/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9970\n","Epoch 283: val_loss did not improve from 0.07569\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0759 - accuracy: 0.9970 - val_loss: 0.0760 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 284/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9968\n","Epoch 284: val_loss improved from 0.07569 to 0.07480, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0772 - accuracy: 0.9968 - val_loss: 0.0748 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 285/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9970\n","Epoch 285: val_loss improved from 0.07480 to 0.07474, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0748 - accuracy: 0.9970 - val_loss: 0.0747 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 286/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9969\n","Epoch 286: val_loss improved from 0.07474 to 0.07467, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0774 - accuracy: 0.9969 - val_loss: 0.0747 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 287/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9970\n","Epoch 287: val_loss improved from 0.07467 to 0.07386, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0755 - accuracy: 0.9970 - val_loss: 0.0739 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 288/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9970\n","Epoch 288: val_loss improved from 0.07386 to 0.07361, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0767 - accuracy: 0.9970 - val_loss: 0.0736 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 289/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9970\n","Epoch 289: val_loss did not improve from 0.07361\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0763 - accuracy: 0.9970 - val_loss: 0.0744 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 290/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9970\n","Epoch 290: val_loss improved from 0.07361 to 0.07333, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.0772 - accuracy: 0.9970 - val_loss: 0.0733 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 291/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9971\n","Epoch 291: val_loss improved from 0.07333 to 0.07289, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0755 - accuracy: 0.9971 - val_loss: 0.0729 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 292/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9971\n","Epoch 292: val_loss improved from 0.07289 to 0.07150, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.0726 - accuracy: 0.9971 - val_loss: 0.0715 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 293/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9971\n","Epoch 293: val_loss did not improve from 0.07150\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0745 - accuracy: 0.9971 - val_loss: 0.0728 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 294/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9970\n","Epoch 294: val_loss did not improve from 0.07150\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0719 - accuracy: 0.9970 - val_loss: 0.0716 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 295/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9972\n","Epoch 295: val_loss improved from 0.07150 to 0.07059, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0728 - accuracy: 0.9972 - val_loss: 0.0706 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 296/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9968\n","Epoch 296: val_loss did not improve from 0.07059\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0758 - accuracy: 0.9968 - val_loss: 0.0708 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 297/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9960\n","Epoch 297: val_loss did not improve from 0.07059\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0735 - accuracy: 0.9960 - val_loss: 0.0718 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 298/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9966\n","Epoch 298: val_loss improved from 0.07059 to 0.06886, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.0761 - accuracy: 0.9966 - val_loss: 0.0689 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 299/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9957\n","Epoch 299: val_loss did not improve from 0.06886\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0754 - accuracy: 0.9957 - val_loss: 0.0839 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 300/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9948\n","Epoch 300: val_loss did not improve from 0.06886\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0755 - accuracy: 0.9948 - val_loss: 0.0715 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 301/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9965\n","Epoch 301: val_loss did not improve from 0.06886\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0739 - accuracy: 0.9965 - val_loss: 0.0721 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 302/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9968\n","Epoch 302: val_loss did not improve from 0.06886\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0705 - accuracy: 0.9968 - val_loss: 0.0705 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 303/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9970\n","Epoch 303: val_loss did not improve from 0.06886\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0714 - accuracy: 0.9970 - val_loss: 0.0695 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 304/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9970\n","Epoch 304: val_loss did not improve from 0.06886\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0726 - accuracy: 0.9970 - val_loss: 0.0692 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 305/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9971\n","Epoch 305: val_loss improved from 0.06886 to 0.06857, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0695 - accuracy: 0.9971 - val_loss: 0.0686 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 306/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9970\n","Epoch 306: val_loss improved from 0.06857 to 0.06723, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.0714 - accuracy: 0.9970 - val_loss: 0.0672 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 307/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9972\n","Epoch 307: val_loss did not improve from 0.06723\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0698 - accuracy: 0.9972 - val_loss: 0.0680 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 308/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9973\n","Epoch 308: val_loss did not improve from 0.06723\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0680 - accuracy: 0.9973 - val_loss: 0.0679 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 309/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9972\n","Epoch 309: val_loss improved from 0.06723 to 0.06599, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0667 - accuracy: 0.9972 - val_loss: 0.0660 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 310/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9972\n","Epoch 310: val_loss improved from 0.06599 to 0.06587, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 299ms/step - loss: 0.0667 - accuracy: 0.9972 - val_loss: 0.0659 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 311/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9973\n","Epoch 311: val_loss improved from 0.06587 to 0.06583, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.0670 - accuracy: 0.9973 - val_loss: 0.0658 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 312/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9965\n","Epoch 312: val_loss did not improve from 0.06583\n","56/56 [==============================] - 17s 310ms/step - loss: 0.0711 - accuracy: 0.9965 - val_loss: 0.1050 - val_accuracy: 0.9746 - lr: 1.0000e-04\n","Epoch 313/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9958\n","Epoch 313: val_loss did not improve from 0.06583\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0711 - accuracy: 0.9958 - val_loss: 0.0674 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 314/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9967\n","Epoch 314: val_loss did not improve from 0.06583\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0675 - accuracy: 0.9967 - val_loss: 0.0667 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 315/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9970\n","Epoch 315: val_loss did not improve from 0.06583\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0679 - accuracy: 0.9970 - val_loss: 0.0661 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 316/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9970\n","Epoch 316: val_loss improved from 0.06583 to 0.06375, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0685 - accuracy: 0.9970 - val_loss: 0.0637 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 317/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9972\n","Epoch 317: val_loss did not improve from 0.06375\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0650 - accuracy: 0.9972 - val_loss: 0.0647 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 318/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9972\n","Epoch 318: val_loss did not improve from 0.06375\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0655 - accuracy: 0.9972 - val_loss: 0.0651 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 319/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9972\n","Epoch 319: val_loss did not improve from 0.06375\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0646 - accuracy: 0.9972 - val_loss: 0.0641 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 320/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9973\n","Epoch 320: val_loss did not improve from 0.06375\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0648 - accuracy: 0.9973 - val_loss: 0.0639 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 321/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9973\n","Epoch 321: val_loss did not improve from 0.06375\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0668 - accuracy: 0.9973 - val_loss: 0.0639 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 322/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9969\n","Epoch 322: val_loss improved from 0.06375 to 0.06232, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.0662 - accuracy: 0.9969 - val_loss: 0.0623 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 323/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9940\n","Epoch 323: val_loss did not improve from 0.06232\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0734 - accuracy: 0.9940 - val_loss: 0.0805 - val_accuracy: 0.9857 - lr: 1.0000e-04\n","Epoch 324/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9950\n","Epoch 324: val_loss did not improve from 0.06232\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0665 - accuracy: 0.9950 - val_loss: 0.0659 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 325/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9965\n","Epoch 325: val_loss improved from 0.06232 to 0.06222, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0670 - accuracy: 0.9965 - val_loss: 0.0622 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 326/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9970\n","Epoch 326: val_loss did not improve from 0.06222\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0618 - accuracy: 0.9970 - val_loss: 0.0632 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 327/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9972\n","Epoch 327: val_loss did not improve from 0.06222\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0619 - accuracy: 0.9972 - val_loss: 0.0624 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 328/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9972\n","Epoch 328: val_loss improved from 0.06222 to 0.06143, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.0638 - accuracy: 0.9972 - val_loss: 0.0614 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 329/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9972\n","Epoch 329: val_loss did not improve from 0.06143\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0626 - accuracy: 0.9972 - val_loss: 0.0625 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 330/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9973\n","Epoch 330: val_loss improved from 0.06143 to 0.06045, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0631 - accuracy: 0.9973 - val_loss: 0.0604 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 331/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9973\n","Epoch 331: val_loss improved from 0.06045 to 0.06014, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.0600 - accuracy: 0.9973 - val_loss: 0.0601 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 332/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9974\n","Epoch 332: val_loss improved from 0.06014 to 0.05895, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0603 - accuracy: 0.9974 - val_loss: 0.0590 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 333/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9973\n","Epoch 333: val_loss did not improve from 0.05895\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0624 - accuracy: 0.9973 - val_loss: 0.0598 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 334/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9973\n","Epoch 334: val_loss did not improve from 0.05895\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0604 - accuracy: 0.9973 - val_loss: 0.0596 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 335/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9974\n","Epoch 335: val_loss did not improve from 0.05895\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0583 - accuracy: 0.9974 - val_loss: 0.0590 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 336/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9975\n","Epoch 336: val_loss improved from 0.05895 to 0.05864, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0598 - accuracy: 0.9975 - val_loss: 0.0586 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 337/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9974\n","Epoch 337: val_loss improved from 0.05864 to 0.05858, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0596 - accuracy: 0.9974 - val_loss: 0.0586 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 338/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9975\n","Epoch 338: val_loss improved from 0.05858 to 0.05845, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.0608 - accuracy: 0.9975 - val_loss: 0.0585 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 339/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9975\n","Epoch 339: val_loss improved from 0.05845 to 0.05761, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.0588 - accuracy: 0.9975 - val_loss: 0.0576 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 340/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9975\n","Epoch 340: val_loss improved from 0.05761 to 0.05731, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0604 - accuracy: 0.9975 - val_loss: 0.0573 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 341/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9974\n","Epoch 341: val_loss did not improve from 0.05731\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0591 - accuracy: 0.9974 - val_loss: 0.0577 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 342/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9976\n","Epoch 342: val_loss improved from 0.05731 to 0.05663, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0574 - accuracy: 0.9976 - val_loss: 0.0566 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 343/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9975\n","Epoch 343: val_loss improved from 0.05663 to 0.05660, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.0594 - accuracy: 0.9975 - val_loss: 0.0566 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 344/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9975\n","Epoch 344: val_loss did not improve from 0.05660\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0578 - accuracy: 0.9975 - val_loss: 0.0568 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 345/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9976\n","Epoch 345: val_loss did not improve from 0.05660\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0578 - accuracy: 0.9976 - val_loss: 0.0570 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 346/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9976\n","Epoch 346: val_loss improved from 0.05660 to 0.05597, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 296ms/step - loss: 0.0577 - accuracy: 0.9976 - val_loss: 0.0560 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 347/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9975\n","Epoch 347: val_loss did not improve from 0.05597\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0582 - accuracy: 0.9975 - val_loss: 0.0568 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 348/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9976\n","Epoch 348: val_loss improved from 0.05597 to 0.05579, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.0570 - accuracy: 0.9976 - val_loss: 0.0558 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 349/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9976\n","Epoch 349: val_loss improved from 0.05579 to 0.05389, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.0586 - accuracy: 0.9976 - val_loss: 0.0539 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 350/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9976\n","Epoch 350: val_loss did not improve from 0.05389\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0560 - accuracy: 0.9976 - val_loss: 0.0547 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 351/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9976\n","Epoch 351: val_loss did not improve from 0.05389\n","56/56 [==============================] - 17s 310ms/step - loss: 0.0586 - accuracy: 0.9976 - val_loss: 0.0550 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 352/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9977\n","Epoch 352: val_loss did not improve from 0.05389\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0543 - accuracy: 0.9977 - val_loss: 0.0547 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 353/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9976\n","Epoch 353: val_loss did not improve from 0.05389\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0554 - accuracy: 0.9976 - val_loss: 0.0539 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 354/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9976\n","Epoch 354: val_loss did not improve from 0.05389\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0555 - accuracy: 0.9976 - val_loss: 0.0552 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 355/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9976\n","Epoch 355: val_loss improved from 0.05389 to 0.05264, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.0553 - accuracy: 0.9976 - val_loss: 0.0526 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 356/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9976\n","Epoch 356: val_loss did not improve from 0.05264\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0550 - accuracy: 0.9976 - val_loss: 0.0532 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 357/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9976\n","Epoch 357: val_loss did not improve from 0.05264\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0527 - accuracy: 0.9976 - val_loss: 0.0539 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 358/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9977\n","Epoch 358: val_loss improved from 0.05264 to 0.05258, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.0547 - accuracy: 0.9977 - val_loss: 0.0526 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 359/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9977\n","Epoch 359: val_loss did not improve from 0.05258\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0543 - accuracy: 0.9977 - val_loss: 0.0531 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 360/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9977\n","Epoch 360: val_loss did not improve from 0.05258\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0530 - accuracy: 0.9977 - val_loss: 0.0530 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 361/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9976\n","Epoch 361: val_loss improved from 0.05258 to 0.05197, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 297ms/step - loss: 0.0521 - accuracy: 0.9976 - val_loss: 0.0520 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 362/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9977\n","Epoch 362: val_loss improved from 0.05197 to 0.05160, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0542 - accuracy: 0.9977 - val_loss: 0.0516 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 363/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9977\n","Epoch 363: val_loss did not improve from 0.05160\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0537 - accuracy: 0.9977 - val_loss: 0.0525 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 364/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9978\n","Epoch 364: val_loss did not improve from 0.05160\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0525 - accuracy: 0.9978 - val_loss: 0.0518 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 365/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9978\n","Epoch 365: val_loss did not improve from 0.05160\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0529 - accuracy: 0.9978 - val_loss: 0.0518 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 366/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9978\n","Epoch 366: val_loss improved from 0.05160 to 0.05096, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0536 - accuracy: 0.9978 - val_loss: 0.0510 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 367/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9977\n","Epoch 367: val_loss improved from 0.05096 to 0.05083, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 17s 295ms/step - loss: 0.0532 - accuracy: 0.9977 - val_loss: 0.0508 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 368/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9978\n","Epoch 368: val_loss improved from 0.05083 to 0.05063, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 293ms/step - loss: 0.0528 - accuracy: 0.9978 - val_loss: 0.0506 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 369/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9978\n","Epoch 369: val_loss improved from 0.05063 to 0.05062, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0512 - accuracy: 0.9978 - val_loss: 0.0506 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 370/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9978\n","Epoch 370: val_loss improved from 0.05062 to 0.04950, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.0511 - accuracy: 0.9978 - val_loss: 0.0495 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 371/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9979\n","Epoch 371: val_loss did not improve from 0.04950\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0500 - accuracy: 0.9979 - val_loss: 0.0503 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 372/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9979\n","Epoch 372: val_loss improved from 0.04950 to 0.04935, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 294ms/step - loss: 0.0496 - accuracy: 0.9979 - val_loss: 0.0494 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 373/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9979\n","Epoch 373: val_loss did not improve from 0.04935\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0503 - accuracy: 0.9979 - val_loss: 0.0498 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 374/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9979\n","Epoch 374: val_loss did not improve from 0.04935\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0505 - accuracy: 0.9979 - val_loss: 0.0501 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 375/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9979\n","Epoch 375: val_loss improved from 0.04935 to 0.04861, saving model to daunet_hybrid.hdf5\n","56/56 [==============================] - 16s 295ms/step - loss: 0.0479 - accuracy: 0.9979 - val_loss: 0.0486 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 376/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9973\n","Epoch 376: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0518 - accuracy: 0.9973 - val_loss: 0.0497 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 377/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9875\n","Epoch 377: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0704 - accuracy: 0.9875 - val_loss: 0.1802 - val_accuracy: 0.9390 - lr: 1.0000e-04\n","Epoch 378/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9918\n","Epoch 378: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0612 - accuracy: 0.9918 - val_loss: 0.0563 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 379/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.9952\n","Epoch 379: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0540 - accuracy: 0.9952 - val_loss: 0.0590 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 380/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9959\n","Epoch 380: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0522 - accuracy: 0.9959 - val_loss: 0.0565 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 381/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9965\n","Epoch 381: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0511 - accuracy: 0.9965 - val_loss: 0.0551 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 382/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9963\n","Epoch 382: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0568 - accuracy: 0.9963 - val_loss: 0.0529 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 383/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9949\n","Epoch 383: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0525 - accuracy: 0.9949 - val_loss: 0.0991 - val_accuracy: 0.9724 - lr: 1.0000e-04\n","Epoch 384/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9934\n","Epoch 384: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0564 - accuracy: 0.9934 - val_loss: 0.0562 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 385/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9944\n","Epoch 385: val_loss did not improve from 0.04861\n","\n","Epoch 385: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0565 - accuracy: 0.9944 - val_loss: 0.0515 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 386/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9963\n","Epoch 386: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0513 - accuracy: 0.9963 - val_loss: 0.0490 - val_accuracy: 0.9948 - lr: 1.0000e-05\n","Epoch 387/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9966\n","Epoch 387: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 285ms/step - loss: 0.0511 - accuracy: 0.9966 - val_loss: 0.0492 - val_accuracy: 0.9952 - lr: 1.0000e-05\n","Epoch 388/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9966\n","Epoch 388: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0516 - accuracy: 0.9966 - val_loss: 0.0493 - val_accuracy: 0.9952 - lr: 1.0000e-05\n","Epoch 389/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9967\n","Epoch 389: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 288ms/step - loss: 0.0494 - accuracy: 0.9967 - val_loss: 0.0491 - val_accuracy: 0.9953 - lr: 1.0000e-05\n","Epoch 390/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9964\n","Epoch 390: val_loss did not improve from 0.04861\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0512 - accuracy: 0.9964 - val_loss: 0.0488 - val_accuracy: 0.9954 - lr: 1.0000e-05\n","Epoch 390: early stopping\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x73c1f1156590>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Train model\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr=ReduceLROnPlateau(monitor='val_loss',\n","                         factor=0.1,\n","                         patience=10,\n","                         verbose=1,\n","                         mode='auto',\n","                         min_delta=0.00003,\n","                         cooldown=0,\n","                         min_lr=0)\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15,verbose=1,mode='min')\n","save_model= ModelCheckpoint('daunet_hybrid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","unet2.fit(images, masks, validation_data=(val_images,val_masks), batch_size=16, epochs=1000,verbose=1,shuffle=True,callbacks=[save_model,reduce_lr,early_stop])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T21:37:41.222900Z","iopub.status.busy":"2023-04-04T21:37:41.221703Z","iopub.status.idle":"2023-04-04T21:37:41.229847Z","shell.execute_reply":"2023-04-04T21:37:41.228834Z","shell.execute_reply.started":"2023-04-04T21:37:41.222851Z"},"trusted":true},"outputs":[],"source":["np.save('daunet_bybrid-history.npy',unet2.history.history)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T21:37:44.421374Z","iopub.status.busy":"2023-04-04T21:37:44.420989Z","iopub.status.idle":"2023-04-04T21:37:44.428055Z","shell.execute_reply":"2023-04-04T21:37:44.426946Z","shell.execute_reply.started":"2023-04-04T21:37:44.421339Z"},"trusted":true},"outputs":[],"source":["model_history = np.load('daunet_bybrid-history.npy', allow_pickle='TRUE').item()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T21:37:49.266787Z","iopub.status.busy":"2023-04-04T21:37:49.266085Z","iopub.status.idle":"2023-04-04T21:37:49.762330Z","shell.execute_reply":"2023-04-04T21:37:49.761219Z","shell.execute_reply.started":"2023-04-04T21:37:49.266747Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<matplotlib.legend.Legend at 0x73c1e29b38d0>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#@title Plot accuracy and loss \n","from matplotlib import pyplot as plt\n","## Accuracy\n","plt.plot(model_history['accuracy'])\n","plt.plot(model_history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')\n","plt.show()\n","\n","## Loss\n","plt.plot(model_history['loss'])\n","plt.plot(model_history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T21:38:33.010134Z","iopub.status.busy":"2023-04-04T21:38:33.009350Z","iopub.status.idle":"2023-04-04T21:38:56.702853Z","shell.execute_reply":"2023-04-04T21:38:56.701820Z","shell.execute_reply.started":"2023-04-04T21:38:33.010089Z"},"trusted":true},"outputs":[],"source":["## Test images\n","test_dir=r'/content/gdrive/MyDrive/mudtest/'\n","test_images_list = os.listdir(r\"{}/images/\".format(test_dir))\n","test_masks_list = []\n","test_images = []\n","for n in test_images_list:\n","  test_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}/images/{}\".format(test_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  test_images.append(a)\n","\n","## Test masks\n","test_masks = []\n","for n in test_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}/labels/{}\".format(test_dir,n))))\n","  test_masks.append(a)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T21:39:15.625186Z","iopub.status.busy":"2023-04-04T21:39:15.624281Z","iopub.status.idle":"2023-04-04T21:39:15.780140Z","shell.execute_reply":"2023-04-04T21:39:15.779086Z","shell.execute_reply.started":"2023-04-04T21:39:15.625148Z"},"trusted":true},"outputs":[],"source":["for i in range(len(test_images)):\n","  test_images[i] = test_images[i].astype('float32')\n","  test_images[i] = test_images[i].T\n","\n","for i in range(len(test_masks)):\n","  test_masks[i] = test_masks[i].reshape(1,256,256,1)\n","  test_masks[i] = test_masks[i].T\n","for i in range(len(test_images)):\n","  test_images[i] = test_images[i].reshape(-1,256,256,10)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T21:39:18.905725Z","iopub.status.busy":"2023-04-04T21:39:18.905356Z","iopub.status.idle":"2023-04-04T21:39:18.915420Z","shell.execute_reply":"2023-04-04T21:39:18.914408Z","shell.execute_reply.started":"2023-04-04T21:39:18.905693Z"},"trusted":true},"outputs":[],"source":["#@title Returns an image or array plot of mask prediction\n","\n","def reconstruct_image(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","  # Standardise between 0-1\n","  reconstruction = reconstruction/np.max(reconstruction)\n","\n","  # Round to 0-1, binary pixel-by-pixel classification \n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  # Plot reconstructed mask (prediction)\n","  plt.imshow(reconstruction) \n","'''\n","  Returns array of mask prediction, given model and image\n","'''\n","def reconstruct_array(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","\n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  return reconstruction # Returns array"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T21:39:22.561931Z","iopub.status.busy":"2023-04-04T21:39:22.561551Z","iopub.status.idle":"2023-04-04T21:39:22.582949Z","shell.execute_reply":"2023-04-04T21:39:22.581685Z","shell.execute_reply.started":"2023-04-04T21:39:22.561897Z"},"trusted":true},"outputs":[],"source":["#@title Metric functions for evaluation\n","def accuracy_eval(model, image, mask): # Gives score of mask vs prediction\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","    return accuracy_score(mask.flatten(), reconstruction)\n","\n","  else: # If a list of images input, find accuracy for each\n","    accuracy = []\n","    for i in range(len(image)):\n","      reconstruction = model.predict(image[i].reshape(1, 256, 256, 10))\n","      reconstruction = np.round(reconstruction).flatten()\n","      accuracy.append(accuracy_score(mask[i].flatten(), reconstruction))\n","    return accuracy\n","\n","def recall_eval(model, image, mask): # Find recall score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return recall_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    recall = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        recall.append(recall_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return recall\n","\n","def precision_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return precision_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    precision = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        precision.append(precision_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return precision\n","\n","def iou_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return jaccard_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    iou = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        iou.append(jaccard_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return iou\n","\n","def f1_score_eval_basic(precision, recall):\n","    prec = np.mean(precision)\n","    rec = np.mean(recall)\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def produce_mask(image): # Outputs rounded image (binary)\n","  return np.round(image)\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T21:39:26.297333Z","iopub.status.busy":"2023-04-04T21:39:26.296340Z","iopub.status.idle":"2023-04-04T21:40:58.957339Z","shell.execute_reply":"2023-04-04T21:40:58.956277Z","shell.execute_reply.started":"2023-04-04T21:39:26.297256Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 55ms/step\n","1/1 [==============================] - 0s 49ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 53ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 49ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 58ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 55ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 47ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 54ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 55ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 55ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n"]}],"source":["accuracy = (accuracy_eval(unet2, test_images, test_masks))\n","precision = (precision_eval(unet2, test_images, test_masks))\n","recall = (recall_eval(unet2, test_images, test_masks))\n","iou = (iou_eval(unet2, test_images, test_masks))"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T21:41:09.259931Z","iopub.status.busy":"2023-04-04T21:41:09.258995Z","iopub.status.idle":"2023-04-04T21:41:09.265094Z","shell.execute_reply":"2023-04-04T21:41:09.263911Z","shell.execute_reply.started":"2023-04-04T21:41:09.259891Z"},"trusted":true},"outputs":[],"source":["f1_score = (f1_score_eval_basic(precision, recall))"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T21:41:12.187984Z","iopub.status.busy":"2023-04-04T21:41:12.187206Z","iopub.status.idle":"2023-04-04T21:41:12.196141Z","shell.execute_reply":"2023-04-04T21:41:12.194713Z","shell.execute_reply.started":"2023-04-04T21:41:12.187943Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model accuracy:  0.9873060068689791 0.013719655415188518\n","model precision:  0.9742870434205197 0.04657738673805472\n","model recall:  0.9767508752986369 0.030008142378844548\n","model F1-score:  0.9755174036574915\n","model iou:  0.9523159753292322\n"]}],"source":["\n","# Print score eval results for each model\n","print('model accuracy: ', np.mean(accuracy), np.std(accuracy))\n","# Print precision eval results for each model\n","print('model precision: ', np.mean(precision), np.std(precision))\n","# Print recall eval results for each model\n","print('model recall: ', np.mean(recall), np.std(recall))\n","# Print f1-score eval results for each model\n","print('model F1-score: ', np.mean(f1_score))\n","print('model iou: ', np.mean(iou))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
