{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:27:23.936427Z","iopub.status.busy":"2023-04-03T09:27:23.935580Z","iopub.status.idle":"2023-04-03T09:28:01.060040Z","shell.execute_reply":"2023-04-03T09:28:01.059009Z","shell.execute_reply.started":"2023-04-03T09:27:23.936395Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/davej23/image-segmentation-keras.git\n","  Cloning https://github.com/davej23/image-segmentation-keras.git to /tmp/pip-req-build-koh3i9ay\n","  Running command git clone --filter=blob:none --quiet https://github.com/davej23/image-segmentation-keras.git /tmp/pip-req-build-koh3i9ay\n","  Resolved https://github.com/davej23/image-segmentation-keras.git to commit e01b0a8d5859854cd9d259a618829889166439f5\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting rarfile\n","  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n","Collecting segmentation-models\n","  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n","Collecting rioxarray\n","  Downloading rioxarray-0.9.1.tar.gz (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting keras-applications<=1.0.8,>=1.0.7\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting image-classifiers==1.0.0\n","  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n","Collecting efficientnet==1.0.0\n","  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n","Collecting h5py<=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: Keras>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (2.11.0)\n","Collecting imageio==2.5.0\n","  Downloading imageio-2.5.0-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (0.4.0)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.5.4.60)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.64.1)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (9.4.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (1.21.6)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from rioxarray) (23.0)\n","Requirement already satisfied: rasterio in /opt/conda/lib/python3.7/site-packages (from rioxarray) (1.2.10)\n","Requirement already satisfied: pyproj>=2.2 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (3.1.0)\n","Requirement already satisfied: xarray>=0.17 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (0.20.2)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py<=2.10.0->keras-segmentation==0.3.0) (1.16.0)\n","Requirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.8.0)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.7.3)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (3.5.3)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from pyproj>=2.2->rioxarray) (2022.12.7)\n","Requirement already satisfied: pandas>=1.1 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (1.3.5)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.11.4)\n","Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.4.0)\n","Requirement already satisfied: click-plugins in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.1.1)\n","Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (22.2.0)\n","Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.4.7)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (59.8.0)\n","Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (0.7.2)\n","Requirement already satisfied: affine in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (2.4.0)\n","Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (8.1.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2022.7.1)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.0)\n","Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.11.2)\n","Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio->rioxarray) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->xarray>=0.17->rioxarray) (3.11.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (1.4.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (4.38.0)\n","Building wheels for collected packages: keras-segmentation, rioxarray\n","  Building wheel for keras-segmentation (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for keras-segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34377 sha256=a64d6eb4d1aa1f9534455aa52d34a764bba17885569d4ab395454041fec7fd1f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3u22wfrj/wheels/f4/fb/07/8f81ceb3d9fe936f5e4dcd1a64cbc489e42e6e7f9c2f166785\n","  Building wheel for rioxarray (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rioxarray: filename=rioxarray-0.9.1-py3-none-any.whl size=54590 sha256=ffbb9de000647e6858144819cf8a9a2d45e14b4b6fad05ac980d47eabead5998\n","  Stored in directory: /root/.cache/pip/wheels/03/b2/26/2e2cc1797ac99cc070d2cae87c340bd3429bbb583c90b1c780\n","Successfully built keras-segmentation rioxarray\n","Installing collected packages: rarfile, imageio, h5py, keras-applications, image-classifiers, efficientnet, segmentation-models, keras-segmentation, rioxarray\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.25.0\n","    Uninstalling imageio-2.25.0:\n","      Successfully uninstalled imageio-2.25.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n","tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed efficientnet-1.0.0 h5py-2.10.0 image-classifiers-1.0.0 imageio-2.5.0 keras-applications-1.0.8 keras-segmentation-0.3.0 rarfile-4.0 rioxarray-0.9.1 segmentation-models-1.0.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#@title import packages\n","import keras\n","import numpy as np\n","import os\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.losses import *\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint\n","import sys\n","\n","!pip install rarfile segmentation-models git+https://github.com/davej23/image-segmentation-keras.git rioxarray\n","from rarfile import RarFile\n","from sklearn.metrics import *\n","import rioxarray as rxr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:28:48.210062Z","iopub.status.busy":"2023-04-03T09:28:48.209591Z","iopub.status.idle":"2023-04-03T09:30:21.544585Z","shell.execute_reply":"2023-04-03T09:30:21.543543Z","shell.execute_reply.started":"2023-04-03T09:28:48.210024Z"},"trusted":true},"outputs":[],"source":["base_dir = r\"/content/gdrive/MyDrive/mudtrain/\"\n","#@title Read training images and normalise\n","training_images_list = os.listdir(r\"{}train/images/\".format(base_dir))\n","training_masks_list = []\n","training_images = []\n","for n in training_images_list:\n","  training_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}train/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  training_images.append(a)\n","\n","## Training masks\n","training_masks = []\n","for n in training_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}train/labels/{}\".format(base_dir,n))))\n","  training_masks.append(a)\n","\n","## Validation images\n","validation_images_list = os.listdir(r\"{}val/images/\".format(base_dir))\n","validation_masks_list = []\n","validation_images = []\n","for n in validation_images_list:\n","  validation_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}val/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  validation_images.append(a)\n","\n","## Validation masks\n","validation_masks = []\n","for n in validation_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}val/labels/{}\".format(base_dir,n))))\n","  validation_masks.append(a)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:32:32.378906Z","iopub.status.busy":"2023-04-03T09:32:32.377787Z","iopub.status.idle":"2023-04-03T09:32:33.006393Z","shell.execute_reply":"2023-04-03T09:32:33.005323Z","shell.execute_reply.started":"2023-04-03T09:32:32.378862Z"},"trusted":true},"outputs":[],"source":["#@title Pre-process data, reshaping and transposing\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].astype('float32')\n","  training_images[i] = training_images[i].T\n","\n","for i in range(len(training_masks)):\n","  training_masks[i] = training_masks[i].reshape(1,256,256)\n","  training_masks[i] = training_masks[i].T\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].astype('float32')\n","  validation_images[i] = validation_images[i].T\n","\n","for i in range(len(validation_masks)):\n","  validation_masks[i] = validation_masks[i].reshape(1,256,256)\n","  validation_masks[i] = validation_masks[i].T\n","\n","\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].reshape(256,256,10)\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].reshape(256,256,10)\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:31:07.720904Z","iopub.status.busy":"2023-04-03T11:31:07.720514Z","iopub.status.idle":"2023-04-03T11:31:31.788258Z","shell.execute_reply":"2023-04-03T11:31:31.787199Z","shell.execute_reply.started":"2023-04-03T11:31:07.720870Z"},"trusted":true},"outputs":[],"source":["## Test images\n","test_dir=r'/content/gdrive/MyDrive/mudtest/'\n","test_images_list = os.listdir(r\"{}/images/\".format(test_dir))\n","test_masks_list = []\n","test_images = []\n","for n in test_images_list:\n","  test_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}/images/{}\".format(test_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  test_images.append(a)\n","\n","## Test masks\n","test_masks = []\n","for n in test_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}/labels/{}\".format(test_dir,n))))\n","  test_masks.append(a)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:31:35.185797Z","iopub.status.busy":"2023-04-03T11:31:35.185412Z","iopub.status.idle":"2023-04-03T11:31:35.338063Z","shell.execute_reply":"2023-04-03T11:31:35.337001Z","shell.execute_reply.started":"2023-04-03T11:31:35.185763Z"},"trusted":true},"outputs":[],"source":["for i in range(len(test_images)):\n","  test_images[i] = test_images[i].astype('float32')\n","  test_images[i] = test_images[i].T\n","\n","for i in range(len(test_masks)):\n","  test_masks[i] = test_masks[i].reshape(1,256,256,1)\n","  test_masks[i] = test_masks[i].T\n","for i in range(len(test_images)):\n","  test_images[i] = test_images[i].reshape(-1,256,256,10)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-02T12:05:52.948735Z","iopub.status.busy":"2023-04-02T12:05:52.948320Z","iopub.status.idle":"2023-04-02T12:05:52.955336Z","shell.execute_reply":"2023-04-02T12:05:52.954246Z","shell.execute_reply.started":"2023-04-02T12:05:52.948698Z"},"trusted":true},"outputs":[{"data":{"text/plain":["266"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["len(test_images)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:32:36.971884Z","iopub.status.busy":"2023-04-03T09:32:36.970806Z","iopub.status.idle":"2023-04-03T09:32:41.988093Z","shell.execute_reply":"2023-04-03T09:32:41.986818Z","shell.execute_reply.started":"2023-04-03T09:32:36.971844Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 10)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["images=np.vstack([training_images])\n","val_images=np.vstack([validation_images])\n","images.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:32:41.991030Z","iopub.status.busy":"2023-04-03T09:32:41.990497Z","iopub.status.idle":"2023-04-03T09:32:42.162098Z","shell.execute_reply":"2023-04-03T09:32:42.161058Z","shell.execute_reply.started":"2023-04-03T09:32:41.990987Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 1)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["masks=np.vstack([training_masks])\n","val_masks=np.vstack([validation_masks])\n","val_masks.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:32:43.383982Z","iopub.status.busy":"2023-04-03T09:32:43.383034Z","iopub.status.idle":"2023-04-03T09:32:43.612800Z","shell.execute_reply":"2023-04-03T09:32:43.611688Z","shell.execute_reply.started":"2023-04-03T09:32:43.383934Z"},"trusted":true},"outputs":[{"data":{"text/plain":["889"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del training_images,validation_images,training_masks,validation_masks,training_images_list,validation_images_list,\n","training_masks_list,validation_masks_list,test_masks_list,test_images_list,test_images,test_masks\n","gc.collect()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-02T20:18:29.801783Z","iopub.status.busy":"2023-04-02T20:18:29.801411Z","iopub.status.idle":"2023-04-02T20:18:29.806445Z","shell.execute_reply":"2023-04-02T20:18:29.805364Z","shell.execute_reply.started":"2023-04-02T20:18:29.801750Z"},"trusted":true},"outputs":[],"source":["del unet"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:30:56.632718Z","iopub.status.busy":"2023-04-03T11:30:56.632099Z","iopub.status.idle":"2023-04-03T11:30:56.647109Z","shell.execute_reply":"2023-04-03T11:30:56.645769Z","shell.execute_reply.started":"2023-04-03T11:30:56.632678Z"},"trusted":true},"outputs":[],"source":["del images,masks,val_images,val_masks"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-02T22:11:00.927113Z","iopub.status.busy":"2023-04-02T22:11:00.926087Z","iopub.status.idle":"2023-04-02T22:11:00.938774Z","shell.execute_reply":"2023-04-02T22:11:00.937555Z","shell.execute_reply.started":"2023-04-02T22:11:00.927075Z"},"trusted":true},"outputs":[],"source":["#@title boundary_loss\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import models\n","\n","#Shape of semantic segmentation mask\n","OUTPUT_SHAPE = (256, 256, 1)\n","def boundary_loss(y_true, y_pred):\n","\n","    \"\"\"\n","    Paper Implemented : https://arxiv.org/abs/1905.07852\n","    Using Binary Segmentation mask, generates boundary mask on fly and claculates boundary loss.\n","    :param y_true:\n","    :param y_pred:\n","    :return:\n","    \"\"\"\n","    y_true=tf.cast(y_true,tf.float32)\n","    y_pred=tf.cast(y_pred,tf.float32)\n","    \n","    y_pred_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd = y_pred_bd - (1 - y_pred)\n","    y_true_bd = y_true_bd - (1 - y_true)\n","\n","    y_pred_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd_ext = y_pred_bd_ext - (1 - y_pred)\n","    y_true_bd_ext = y_true_bd_ext - (1 - y_true)\n","\n","    P = K.sum(y_pred_bd * y_true_bd_ext) / K.sum(y_pred_bd) + 1e-7\n","    R = K.sum(y_true_bd * y_pred_bd_ext) / K.sum(y_true_bd) + 1e-7\n","    F1_Score = 2 * P * R / (P + R + 1e-7)\n","    loss = K.mean(1 - F1_Score)  \n","    return loss\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-02T22:11:04.226081Z","iopub.status.busy":"2023-04-02T22:11:04.225123Z","iopub.status.idle":"2023-04-02T22:11:06.554718Z","shell.execute_reply":"2023-04-02T22:11:06.553651Z","shell.execute_reply.started":"2023-04-02T22:11:04.226042Z"},"trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, Callback\n","\n","class AlphaScheduler(Callback):\n","  def init(self, alpha, update_fn):\n","    self.alpha = alpha\n","    self.update_fn = update_fn\n","  def on_epoch_end(self, epoch, logs=None):\n","    updated_alpha = self.update_fn(K.get_value(self.alpha))\n","\n","alpha = K.variable(1, dtype='float32')\n","\n","def update_alpha(value):\n","  return np.clip(value - 0.01, 0.01, 1)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-02T22:11:09.749828Z","iopub.status.busy":"2023-04-02T22:11:09.748857Z","iopub.status.idle":"2023-04-02T22:11:09.756577Z","shell.execute_reply":"2023-04-02T22:11:09.755200Z","shell.execute_reply.started":"2023-04-02T22:11:09.749774Z"},"trusted":true},"outputs":[],"source":["def gl_sl_wrapper(alpha):\n","    def gl_sl(y_true, y_pred):\n","        return alpha* binary_crossentropy(y_true, y_pred) +  (1-alpha)* boundary_loss(y_true, y_pred)\n","    return gl_sl"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:32:48.858827Z","iopub.status.busy":"2023-04-03T09:32:48.858430Z","iopub.status.idle":"2023-04-03T09:32:49.105107Z","shell.execute_reply":"2023-04-03T09:32:49.103966Z","shell.execute_reply.started":"2023-04-03T09:32:48.858793Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import models, layers, regularizers\n","from tensorflow.keras import backend as K\n","\n","#convolutional block\n","def conv_block(x, kernelsize, filters, dropout, batchnorm=True): \n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    if dropout > 0:\n","        conv = layers.Dropout(dropout)(conv)\n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(conv)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    return conv\n","\n","#Simple U-NET\n","def unetmodel(input_shape, dropout=0, batchnorm=True):    \n","    \n","    filters = [32,64, 128, 256,512]\n","    kernelsize = 3\n","    upsample_size = 2\n","    \n","    inputs = layers.Input(input_shape)    \n","\n","    # Downsampling layers\n","    dn_1 = conv_block(inputs, kernelsize, filters[0], dropout, batchnorm)\n","    pool_1 = layers.MaxPooling2D(pool_size=(2,2))(dn_1)\n","    \n","    dn_2 = conv_block(pool_1, kernelsize, filters[1], dropout, batchnorm)\n","    pool_2 = layers.MaxPooling2D(pool_size=(2,2))(dn_2)\n","    \n","    dn_3 = conv_block(pool_2, kernelsize, filters[2], dropout, batchnorm)\n","    pool_3 = layers.MaxPooling2D(pool_size=(2,2))(dn_3)\n","    \n","    dn_4 = conv_block(pool_3, kernelsize, filters[3], dropout, batchnorm)\n","    pool_4 = layers.MaxPooling2D(pool_size=(2,2))(dn_4)\n","    \n","    dn_5 = conv_block(pool_4, kernelsize, filters[4], dropout, batchnorm)\n","\n","    # Upsampling layers   \n","    up_5 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(dn_5)\n","    up_5 = layers.concatenate([up_5, dn_4], axis=3)\n","    up_conv_5 = conv_block(up_5, kernelsize, filters[3], dropout, batchnorm)\n","    \n","    up_4 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_5)\n","    up_4 = layers.concatenate([up_4, dn_3], axis=3)\n","    up_conv_4 = conv_block(up_4, kernelsize, filters[2], dropout, batchnorm)\n","       \n","    up_3 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_4)\n","    up_3 = layers.concatenate([up_3, dn_2], axis=3)\n","    up_conv_3 = conv_block(up_3, kernelsize, filters[1], dropout, batchnorm)\n","    \n","    up_2 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_3)\n","    up_2 = layers.concatenate([up_2, dn_1], axis=3)\n","    up_conv_2 = conv_block(up_2, kernelsize, filters[0], dropout, batchnorm)    \n","   \n","    conv_final = layers.Conv2D(1, kernel_size=(1,1))(up_conv_2)\n","    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n","    outputs = layers.Activation('sigmoid')(conv_final)  \n","\n","    model = models.Model(inputs=[inputs], outputs=[outputs])     \n","    return model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:32:57.952705Z","iopub.status.busy":"2023-04-03T09:32:57.952328Z","iopub.status.idle":"2023-04-03T09:33:00.610754Z","shell.execute_reply":"2023-04-03T09:33:00.609940Z","shell.execute_reply.started":"2023-04-03T09:32:57.952669Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 256, 256, 1  0           []                               \n","                                0)]                                                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 256, 256, 32  2912        ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 256, 256, 32  128        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," activation (Activation)        (None, 256, 256, 32  0           ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 256, 256, 32  9248        ['activation[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 256, 256, 32  128        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_1 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 128, 128, 32  0           ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 128, 128, 64  18496       ['max_pooling2d[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 128, 128, 64  256        ['conv2d_2[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_2 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_2[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 128, 128, 64  36928       ['activation_2[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 128, 128, 64  256        ['conv2d_3[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_3 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_3[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 64, 64, 128)  73856       ['max_pooling2d_1[0][0]']        \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_4[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_4 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 64, 64, 128)  147584      ['activation_4[0][0]']           \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_5[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_5 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0          ['activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 32, 32, 256)  295168      ['max_pooling2d_2[0][0]']        \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_6[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_6 (Activation)      (None, 32, 32, 256)  0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 32, 32, 256)  590080      ['activation_6[0][0]']           \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_7[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_7 (Activation)      (None, 32, 32, 256)  0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0          ['activation_7[0][0]']           \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 16, 16, 512)  1180160     ['max_pooling2d_3[0][0]']        \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_8[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_8 (Activation)      (None, 16, 16, 512)  0           ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 16, 16, 512)  2359808     ['activation_8[0][0]']           \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_9[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_9 (Activation)      (None, 16, 16, 512)  0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," up_sampling2d (UpSampling2D)   (None, 32, 32, 512)  0           ['activation_9[0][0]']           \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 32, 32, 768)  0           ['up_sampling2d[0][0]',          \n","                                                                  'activation_7[0][0]']           \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 32, 32, 256)  1769728     ['concatenate[0][0]']            \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_10[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_10 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_10[0][0]'] \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_10[0][0]']          \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_11[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_11 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0          ['activation_11[0][0]']          \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 64, 64, 384)  0           ['up_sampling2d_1[0][0]',        \n","                                                                  'activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 64, 64, 128)  442496      ['concatenate_1[0][0]']          \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 64, 64, 128)  512        ['conv2d_12[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_12 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_12[0][0]']          \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 64, 64, 128)  512        ['conv2d_13[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_13 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 12  0          ['activation_13[0][0]']          \n","                                8)                                                                \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 128, 128, 19  0           ['up_sampling2d_2[0][0]',        \n","                                2)                                'activation_3[0][0]']           \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 128, 128, 64  110656      ['concatenate_2[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 128, 128, 64  256        ['conv2d_14[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_14 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_14[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 128, 128, 64  36928       ['activation_14[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 128, 128, 64  256        ['conv2d_15[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_15 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_15[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64  0          ['activation_15[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 256, 256, 96  0           ['up_sampling2d_3[0][0]',        \n","                                )                                 'activation_1[0][0]']           \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 256, 256, 32  27680       ['concatenate_3[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 256, 256, 32  128        ['conv2d_16[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_16 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_16[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 256, 256, 32  9248        ['activation_16[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 256, 256, 32  128        ['conv2d_17[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_17 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_17[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 256, 256, 1)  33          ['activation_17[0][0]']          \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 256, 256, 1)  4          ['conv2d_18[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_18 (Activation)     (None, 256, 256, 1)  0           ['batch_normalization_18[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 7,860,453\n","Trainable params: 7,854,563\n","Non-trainable params: 5,890\n","__________________________________________________________________________________________________\n"]}],"source":["from keras import metrics\n","unet= unetmodel(input_shape=(256,256,10))\n","#gl_sl_wrapper(alpha)\n","unet.compile(optimizer = adam_v2.Adam(learning_rate = 1e-4), loss =binary_crossentropy, metrics = ['accuracy'])\n","unet.summary()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:33:11.487203Z","iopub.status.busy":"2023-04-03T09:33:11.486808Z","iopub.status.idle":"2023-04-03T11:27:29.734196Z","shell.execute_reply":"2023-04-03T11:27:29.733024Z","shell.execute_reply.started":"2023-04-03T09:33:11.487169Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.9033\n","Epoch 1: val_loss improved from inf to 0.59373, saving model to unet_bce.hdf5\n","56/56 [==============================] - 25s 299ms/step - loss: 0.4356 - accuracy: 0.9033 - val_loss: 0.5937 - val_accuracy: 0.7385 - lr: 1.0000e-04\n","Epoch 2/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3898 - accuracy: 0.9563\n","Epoch 2: val_loss did not improve from 0.59373\n","56/56 [==============================] - 12s 217ms/step - loss: 0.3898 - accuracy: 0.9563 - val_loss: 0.6319 - val_accuracy: 0.6638 - lr: 1.0000e-04\n","Epoch 3/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3836 - accuracy: 0.9598\n","Epoch 3: val_loss did not improve from 0.59373\n","56/56 [==============================] - 12s 217ms/step - loss: 0.3836 - accuracy: 0.9598 - val_loss: 0.6063 - val_accuracy: 0.7191 - lr: 1.0000e-04\n","Epoch 4/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3791 - accuracy: 0.9642\n","Epoch 4: val_loss improved from 0.59373 to 0.52320, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.3791 - accuracy: 0.9642 - val_loss: 0.5232 - val_accuracy: 0.8027 - lr: 1.0000e-04\n","Epoch 5/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3721 - accuracy: 0.9694\n","Epoch 5: val_loss improved from 0.52320 to 0.51568, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.3721 - accuracy: 0.9694 - val_loss: 0.5157 - val_accuracy: 0.8480 - lr: 1.0000e-04\n","Epoch 6/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3647 - accuracy: 0.9754\n","Epoch 6: val_loss improved from 0.51568 to 0.48256, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.3647 - accuracy: 0.9754 - val_loss: 0.4826 - val_accuracy: 0.8715 - lr: 1.0000e-04\n","Epoch 7/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3662 - accuracy: 0.9733\n","Epoch 7: val_loss improved from 0.48256 to 0.38296, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.3662 - accuracy: 0.9733 - val_loss: 0.3830 - val_accuracy: 0.9740 - lr: 1.0000e-04\n","Epoch 8/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3571 - accuracy: 0.9787\n","Epoch 8: val_loss improved from 0.38296 to 0.37377, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 223ms/step - loss: 0.3571 - accuracy: 0.9787 - val_loss: 0.3738 - val_accuracy: 0.9752 - lr: 1.0000e-04\n","Epoch 9/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3601 - accuracy: 0.9725\n","Epoch 9: val_loss improved from 0.37377 to 0.35723, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 227ms/step - loss: 0.3601 - accuracy: 0.9725 - val_loss: 0.3572 - val_accuracy: 0.9769 - lr: 1.0000e-04\n","Epoch 10/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3588 - accuracy: 0.9746\n","Epoch 10: val_loss improved from 0.35723 to 0.35373, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.3588 - accuracy: 0.9746 - val_loss: 0.3537 - val_accuracy: 0.9820 - lr: 1.0000e-04\n","Epoch 11/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.9761\n","Epoch 11: val_loss improved from 0.35373 to 0.34267, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.3536 - accuracy: 0.9761 - val_loss: 0.3427 - val_accuracy: 0.9842 - lr: 1.0000e-04\n","Epoch 12/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3470 - accuracy: 0.9821\n","Epoch 12: val_loss improved from 0.34267 to 0.34075, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.3470 - accuracy: 0.9821 - val_loss: 0.3408 - val_accuracy: 0.9851 - lr: 1.0000e-04\n","Epoch 13/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3477 - accuracy: 0.9798\n","Epoch 13: val_loss improved from 0.34075 to 0.33861, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.3477 - accuracy: 0.9798 - val_loss: 0.3386 - val_accuracy: 0.9844 - lr: 1.0000e-04\n","Epoch 14/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3466 - accuracy: 0.9802\n","Epoch 14: val_loss did not improve from 0.33861\n","56/56 [==============================] - 12s 223ms/step - loss: 0.3466 - accuracy: 0.9802 - val_loss: 0.3417 - val_accuracy: 0.9793 - lr: 1.0000e-04\n","Epoch 15/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3404 - accuracy: 0.9825\n","Epoch 15: val_loss improved from 0.33861 to 0.33395, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.3404 - accuracy: 0.9825 - val_loss: 0.3340 - val_accuracy: 0.9864 - lr: 1.0000e-04\n","Epoch 16/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3382 - accuracy: 0.9846\n","Epoch 16: val_loss did not improve from 0.33395\n","56/56 [==============================] - 12s 218ms/step - loss: 0.3382 - accuracy: 0.9846 - val_loss: 0.3398 - val_accuracy: 0.9848 - lr: 1.0000e-04\n","Epoch 17/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3339 - accuracy: 0.9840\n","Epoch 17: val_loss improved from 0.33395 to 0.33156, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.3339 - accuracy: 0.9840 - val_loss: 0.3316 - val_accuracy: 0.9852 - lr: 1.0000e-04\n","Epoch 18/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.9849\n","Epoch 18: val_loss did not improve from 0.33156\n","56/56 [==============================] - 12s 217ms/step - loss: 0.3330 - accuracy: 0.9849 - val_loss: 0.3365 - val_accuracy: 0.9857 - lr: 1.0000e-04\n","Epoch 19/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3293 - accuracy: 0.9855\n","Epoch 19: val_loss did not improve from 0.33156\n","56/56 [==============================] - 12s 217ms/step - loss: 0.3293 - accuracy: 0.9855 - val_loss: 0.3327 - val_accuracy: 0.9820 - lr: 1.0000e-04\n","Epoch 20/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3285 - accuracy: 0.9859\n","Epoch 20: val_loss improved from 0.33156 to 0.32807, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.3285 - accuracy: 0.9859 - val_loss: 0.3281 - val_accuracy: 0.9845 - lr: 1.0000e-04\n","Epoch 21/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3272 - accuracy: 0.9856\n","Epoch 21: val_loss improved from 0.32807 to 0.32448, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.3272 - accuracy: 0.9856 - val_loss: 0.3245 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 22/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3230 - accuracy: 0.9862\n","Epoch 22: val_loss improved from 0.32448 to 0.32229, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.3230 - accuracy: 0.9862 - val_loss: 0.3223 - val_accuracy: 0.9855 - lr: 1.0000e-04\n","Epoch 23/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.9883\n","Epoch 23: val_loss improved from 0.32229 to 0.32155, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 224ms/step - loss: 0.3191 - accuracy: 0.9883 - val_loss: 0.3215 - val_accuracy: 0.9869 - lr: 1.0000e-04\n","Epoch 24/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3184 - accuracy: 0.9874\n","Epoch 24: val_loss did not improve from 0.32155\n","56/56 [==============================] - 12s 217ms/step - loss: 0.3184 - accuracy: 0.9874 - val_loss: 0.3222 - val_accuracy: 0.9841 - lr: 1.0000e-04\n","Epoch 25/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3178 - accuracy: 0.9870\n","Epoch 25: val_loss improved from 0.32155 to 0.31461, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.3178 - accuracy: 0.9870 - val_loss: 0.3146 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 26/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3141 - accuracy: 0.9886\n","Epoch 26: val_loss did not improve from 0.31461\n","56/56 [==============================] - 13s 224ms/step - loss: 0.3141 - accuracy: 0.9886 - val_loss: 0.3156 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 27/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3158 - accuracy: 0.9869\n","Epoch 27: val_loss improved from 0.31461 to 0.31078, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.3158 - accuracy: 0.9869 - val_loss: 0.3108 - val_accuracy: 0.9846 - lr: 1.0000e-04\n","Epoch 28/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.9816\n","Epoch 28: val_loss did not improve from 0.31078\n","56/56 [==============================] - 12s 217ms/step - loss: 0.3175 - accuracy: 0.9816 - val_loss: 0.3206 - val_accuracy: 0.9763 - lr: 1.0000e-04\n","Epoch 29/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3202 - accuracy: 0.9781\n","Epoch 29: val_loss did not improve from 0.31078\n","56/56 [==============================] - 13s 224ms/step - loss: 0.3202 - accuracy: 0.9781 - val_loss: 0.3690 - val_accuracy: 0.9399 - lr: 1.0000e-04\n","Epoch 30/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3135 - accuracy: 0.9829\n","Epoch 30: val_loss improved from 0.31078 to 0.30662, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.3135 - accuracy: 0.9829 - val_loss: 0.3066 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 31/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.9865\n","Epoch 31: val_loss improved from 0.30662 to 0.30479, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.3068 - accuracy: 0.9865 - val_loss: 0.3048 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 32/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.9881\n","Epoch 32: val_loss did not improve from 0.30479\n","56/56 [==============================] - 12s 217ms/step - loss: 0.3021 - accuracy: 0.9881 - val_loss: 0.3055 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 33/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3023 - accuracy: 0.9884\n","Epoch 33: val_loss improved from 0.30479 to 0.30405, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.3023 - accuracy: 0.9884 - val_loss: 0.3041 - val_accuracy: 0.9866 - lr: 1.0000e-04\n","Epoch 34/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.9887\n","Epoch 34: val_loss improved from 0.30405 to 0.29973, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.2999 - accuracy: 0.9887 - val_loss: 0.2997 - val_accuracy: 0.9886 - lr: 1.0000e-04\n","Epoch 35/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.9877\n","Epoch 35: val_loss improved from 0.29973 to 0.29327, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.2999 - accuracy: 0.9877 - val_loss: 0.2933 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 36/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9900\n","Epoch 36: val_loss did not improve from 0.29327\n","56/56 [==============================] - 12s 219ms/step - loss: 0.2941 - accuracy: 0.9900 - val_loss: 0.2957 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 37/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2959 - accuracy: 0.9894\n","Epoch 37: val_loss did not improve from 0.29327\n","56/56 [==============================] - 12s 222ms/step - loss: 0.2959 - accuracy: 0.9894 - val_loss: 0.2940 - val_accuracy: 0.9882 - lr: 1.0000e-04\n","Epoch 38/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2937 - accuracy: 0.9886\n","Epoch 38: val_loss improved from 0.29327 to 0.29177, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.2937 - accuracy: 0.9886 - val_loss: 0.2918 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 39/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.9882\n","Epoch 39: val_loss did not improve from 0.29177\n","56/56 [==============================] - 12s 220ms/step - loss: 0.2913 - accuracy: 0.9882 - val_loss: 0.2948 - val_accuracy: 0.9879 - lr: 1.0000e-04\n","Epoch 40/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2910 - accuracy: 0.9887\n","Epoch 40: val_loss improved from 0.29177 to 0.28863, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.2910 - accuracy: 0.9887 - val_loss: 0.2886 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 41/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.9905\n","Epoch 41: val_loss did not improve from 0.28863\n","56/56 [==============================] - 12s 218ms/step - loss: 0.2848 - accuracy: 0.9905 - val_loss: 0.2931 - val_accuracy: 0.9880 - lr: 1.0000e-04\n","Epoch 42/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9897\n","Epoch 42: val_loss improved from 0.28863 to 0.28745, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.2850 - accuracy: 0.9897 - val_loss: 0.2874 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 43/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.9842\n","Epoch 43: val_loss improved from 0.28745 to 0.27583, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 223ms/step - loss: 0.2892 - accuracy: 0.9842 - val_loss: 0.2758 - val_accuracy: 0.9877 - lr: 1.0000e-04\n","Epoch 44/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.9871\n","Epoch 44: val_loss did not improve from 0.27583\n","56/56 [==============================] - 12s 217ms/step - loss: 0.2849 - accuracy: 0.9871 - val_loss: 0.2807 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 45/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2799 - accuracy: 0.9901\n","Epoch 45: val_loss did not improve from 0.27583\n","56/56 [==============================] - 12s 216ms/step - loss: 0.2799 - accuracy: 0.9901 - val_loss: 0.2828 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 46/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.9900\n","Epoch 46: val_loss did not improve from 0.27583\n","56/56 [==============================] - 12s 221ms/step - loss: 0.2785 - accuracy: 0.9900 - val_loss: 0.2775 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 47/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.9904\n","Epoch 47: val_loss did not improve from 0.27583\n","56/56 [==============================] - 12s 218ms/step - loss: 0.2759 - accuracy: 0.9904 - val_loss: 0.2791 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 48/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2743 - accuracy: 0.9900\n","Epoch 48: val_loss did not improve from 0.27583\n","56/56 [==============================] - 12s 221ms/step - loss: 0.2743 - accuracy: 0.9900 - val_loss: 0.2857 - val_accuracy: 0.9786 - lr: 1.0000e-04\n","Epoch 49/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.9823\n","Epoch 49: val_loss improved from 0.27583 to 0.26889, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.2834 - accuracy: 0.9823 - val_loss: 0.2689 - val_accuracy: 0.9863 - lr: 1.0000e-04\n","Epoch 50/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2777 - accuracy: 0.9874\n","Epoch 50: val_loss did not improve from 0.26889\n","56/56 [==============================] - 12s 217ms/step - loss: 0.2777 - accuracy: 0.9874 - val_loss: 0.2725 - val_accuracy: 0.9882 - lr: 1.0000e-04\n","Epoch 51/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2748 - accuracy: 0.9863\n","Epoch 51: val_loss did not improve from 0.26889\n","56/56 [==============================] - 12s 222ms/step - loss: 0.2748 - accuracy: 0.9863 - val_loss: 0.2698 - val_accuracy: 0.9879 - lr: 1.0000e-04\n","Epoch 52/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.9900\n","Epoch 52: val_loss did not improve from 0.26889\n","56/56 [==============================] - 12s 218ms/step - loss: 0.2684 - accuracy: 0.9900 - val_loss: 0.2713 - val_accuracy: 0.9881 - lr: 1.0000e-04\n","Epoch 53/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2679 - accuracy: 0.9901\n","Epoch 53: val_loss improved from 0.26889 to 0.26775, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 224ms/step - loss: 0.2679 - accuracy: 0.9901 - val_loss: 0.2678 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 54/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.9907\n","Epoch 54: val_loss improved from 0.26775 to 0.26671, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.2681 - accuracy: 0.9907 - val_loss: 0.2667 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 55/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.9916\n","Epoch 55: val_loss improved from 0.26671 to 0.26658, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.2638 - accuracy: 0.9916 - val_loss: 0.2666 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 56/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.9916\n","Epoch 56: val_loss improved from 0.26658 to 0.26188, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.2608 - accuracy: 0.9916 - val_loss: 0.2619 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 57/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9905\n","Epoch 57: val_loss did not improve from 0.26188\n","56/56 [==============================] - 12s 223ms/step - loss: 0.2625 - accuracy: 0.9905 - val_loss: 0.2663 - val_accuracy: 0.9836 - lr: 1.0000e-04\n","Epoch 58/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.9907\n","Epoch 58: val_loss improved from 0.26188 to 0.25668, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.2573 - accuracy: 0.9907 - val_loss: 0.2567 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 59/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.9916\n","Epoch 59: val_loss improved from 0.25668 to 0.25650, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.2559 - accuracy: 0.9916 - val_loss: 0.2565 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 60/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2546 - accuracy: 0.9923\n","Epoch 60: val_loss improved from 0.25650 to 0.25510, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.2546 - accuracy: 0.9923 - val_loss: 0.2551 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 61/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.9919\n","Epoch 61: val_loss improved from 0.25510 to 0.25278, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.2552 - accuracy: 0.9919 - val_loss: 0.2528 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 62/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2546 - accuracy: 0.9899\n","Epoch 62: val_loss improved from 0.25278 to 0.25246, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 227ms/step - loss: 0.2546 - accuracy: 0.9899 - val_loss: 0.2525 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 63/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2599 - accuracy: 0.9834\n","Epoch 63: val_loss did not improve from 0.25246\n","56/56 [==============================] - 12s 217ms/step - loss: 0.2599 - accuracy: 0.9834 - val_loss: 0.2686 - val_accuracy: 0.9796 - lr: 1.0000e-04\n","Epoch 64/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2603 - accuracy: 0.9846\n","Epoch 64: val_loss improved from 0.25246 to 0.24407, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.2603 - accuracy: 0.9846 - val_loss: 0.2441 - val_accuracy: 0.9879 - lr: 1.0000e-04\n","Epoch 65/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.9870\n","Epoch 65: val_loss did not improve from 0.24407\n","56/56 [==============================] - 12s 223ms/step - loss: 0.2564 - accuracy: 0.9870 - val_loss: 0.2528 - val_accuracy: 0.9857 - lr: 1.0000e-04\n","Epoch 66/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.9890\n","Epoch 66: val_loss did not improve from 0.24407\n","56/56 [==============================] - 12s 216ms/step - loss: 0.2507 - accuracy: 0.9890 - val_loss: 0.2506 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 67/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9911\n","Epoch 67: val_loss did not improve from 0.24407\n","56/56 [==============================] - 12s 217ms/step - loss: 0.2466 - accuracy: 0.9911 - val_loss: 0.2494 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 68/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2439 - accuracy: 0.9916\n","Epoch 68: val_loss did not improve from 0.24407\n","56/56 [==============================] - 12s 217ms/step - loss: 0.2439 - accuracy: 0.9916 - val_loss: 0.2458 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 69/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.9913\n","Epoch 69: val_loss improved from 0.24407 to 0.24231, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 237ms/step - loss: 0.2442 - accuracy: 0.9913 - val_loss: 0.2423 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 70/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9921\n","Epoch 70: val_loss improved from 0.24231 to 0.24195, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.2421 - accuracy: 0.9921 - val_loss: 0.2420 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 71/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9912\n","Epoch 71: val_loss improved from 0.24195 to 0.23649, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.2411 - accuracy: 0.9912 - val_loss: 0.2365 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 72/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2390 - accuracy: 0.9908\n","Epoch 72: val_loss improved from 0.23649 to 0.23571, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.2390 - accuracy: 0.9908 - val_loss: 0.2357 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 73/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.9908\n","Epoch 73: val_loss did not improve from 0.23571\n","56/56 [==============================] - 12s 222ms/step - loss: 0.2385 - accuracy: 0.9908 - val_loss: 0.2368 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 74/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.9924\n","Epoch 74: val_loss did not improve from 0.23571\n","56/56 [==============================] - 12s 222ms/step - loss: 0.2358 - accuracy: 0.9924 - val_loss: 0.2363 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 75/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2361 - accuracy: 0.9910\n","Epoch 75: val_loss improved from 0.23571 to 0.23163, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.2361 - accuracy: 0.9910 - val_loss: 0.2316 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 76/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2353 - accuracy: 0.9911\n","Epoch 76: val_loss improved from 0.23163 to 0.23141, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.2353 - accuracy: 0.9911 - val_loss: 0.2314 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 77/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9927\n","Epoch 77: val_loss improved from 0.23141 to 0.23091, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.2321 - accuracy: 0.9927 - val_loss: 0.2309 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 78/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2265 - accuracy: 0.9932\n","Epoch 78: val_loss improved from 0.23091 to 0.22922, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.2265 - accuracy: 0.9932 - val_loss: 0.2292 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 79/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2305 - accuracy: 0.9923\n","Epoch 79: val_loss improved from 0.22922 to 0.22850, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.2305 - accuracy: 0.9923 - val_loss: 0.2285 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 80/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9929\n","Epoch 80: val_loss improved from 0.22850 to 0.22821, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.2269 - accuracy: 0.9929 - val_loss: 0.2282 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 81/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2311 - accuracy: 0.9912\n","Epoch 81: val_loss improved from 0.22821 to 0.22602, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.2311 - accuracy: 0.9912 - val_loss: 0.2260 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 82/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.9919\n","Epoch 82: val_loss improved from 0.22602 to 0.22232, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.2255 - accuracy: 0.9919 - val_loss: 0.2223 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 83/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2250 - accuracy: 0.9929\n","Epoch 83: val_loss did not improve from 0.22232\n","56/56 [==============================] - 12s 219ms/step - loss: 0.2250 - accuracy: 0.9929 - val_loss: 0.2259 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 84/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.9931\n","Epoch 84: val_loss did not improve from 0.22232\n","56/56 [==============================] - 12s 222ms/step - loss: 0.2217 - accuracy: 0.9931 - val_loss: 0.2236 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 85/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.9932\n","Epoch 85: val_loss improved from 0.22232 to 0.22081, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.2200 - accuracy: 0.9932 - val_loss: 0.2208 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 86/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.9933\n","Epoch 86: val_loss improved from 0.22081 to 0.21943, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.2217 - accuracy: 0.9933 - val_loss: 0.2194 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 87/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2180 - accuracy: 0.9934\n","Epoch 87: val_loss improved from 0.21943 to 0.21719, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.2180 - accuracy: 0.9934 - val_loss: 0.2172 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 88/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 0.9936\n","Epoch 88: val_loss did not improve from 0.21719\n","56/56 [==============================] - 12s 218ms/step - loss: 0.2181 - accuracy: 0.9936 - val_loss: 0.2173 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 89/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9932\n","Epoch 89: val_loss improved from 0.21719 to 0.21577, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 233ms/step - loss: 0.2176 - accuracy: 0.9932 - val_loss: 0.2158 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 90/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2163 - accuracy: 0.9933\n","Epoch 90: val_loss improved from 0.21577 to 0.21305, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.2163 - accuracy: 0.9933 - val_loss: 0.2130 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 91/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2122 - accuracy: 0.9937\n","Epoch 91: val_loss improved from 0.21305 to 0.21223, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.2122 - accuracy: 0.9937 - val_loss: 0.2122 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 92/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9937\n","Epoch 92: val_loss improved from 0.21223 to 0.21129, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.2138 - accuracy: 0.9937 - val_loss: 0.2113 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 93/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2107 - accuracy: 0.9937\n","Epoch 93: val_loss improved from 0.21129 to 0.20980, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.2107 - accuracy: 0.9937 - val_loss: 0.2098 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 94/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2100 - accuracy: 0.9935\n","Epoch 94: val_loss improved from 0.20980 to 0.20737, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.2100 - accuracy: 0.9935 - val_loss: 0.2074 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 95/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2095 - accuracy: 0.9930\n","Epoch 95: val_loss improved from 0.20737 to 0.20596, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.2095 - accuracy: 0.9930 - val_loss: 0.2060 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 96/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9937\n","Epoch 96: val_loss did not improve from 0.20596\n","56/56 [==============================] - 12s 217ms/step - loss: 0.2053 - accuracy: 0.9937 - val_loss: 0.2062 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 97/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2091 - accuracy: 0.9928\n","Epoch 97: val_loss did not improve from 0.20596\n","56/56 [==============================] - 12s 223ms/step - loss: 0.2091 - accuracy: 0.9928 - val_loss: 0.2217 - val_accuracy: 0.9793 - lr: 1.0000e-04\n","Epoch 98/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2055 - accuracy: 0.9923\n","Epoch 98: val_loss improved from 0.20596 to 0.20254, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 224ms/step - loss: 0.2055 - accuracy: 0.9923 - val_loss: 0.2025 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 99/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2045 - accuracy: 0.9937\n","Epoch 99: val_loss did not improve from 0.20254\n","56/56 [==============================] - 12s 217ms/step - loss: 0.2045 - accuracy: 0.9937 - val_loss: 0.2045 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 100/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2058 - accuracy: 0.9925\n","Epoch 100: val_loss did not improve from 0.20254\n","56/56 [==============================] - 12s 223ms/step - loss: 0.2058 - accuracy: 0.9925 - val_loss: 0.2129 - val_accuracy: 0.9823 - lr: 1.0000e-04\n","Epoch 101/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2042 - accuracy: 0.9911\n","Epoch 101: val_loss improved from 0.20254 to 0.19947, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.2042 - accuracy: 0.9911 - val_loss: 0.1995 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 102/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.9925\n","Epoch 102: val_loss improved from 0.19947 to 0.19742, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.2020 - accuracy: 0.9925 - val_loss: 0.1974 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 103/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2027 - accuracy: 0.9931\n","Epoch 103: val_loss did not improve from 0.19742\n","56/56 [==============================] - 12s 217ms/step - loss: 0.2027 - accuracy: 0.9931 - val_loss: 0.1994 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 104/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.9938\n","Epoch 104: val_loss did not improve from 0.19742\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1980 - accuracy: 0.9938 - val_loss: 0.1993 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 105/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1962 - accuracy: 0.9940\n","Epoch 105: val_loss improved from 0.19742 to 0.19633, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1962 - accuracy: 0.9940 - val_loss: 0.1963 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 106/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9943\n","Epoch 106: val_loss improved from 0.19633 to 0.19545, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.1950 - accuracy: 0.9943 - val_loss: 0.1954 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 107/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9940\n","Epoch 107: val_loss improved from 0.19545 to 0.19216, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.1942 - accuracy: 0.9940 - val_loss: 0.1922 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 108/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9942\n","Epoch 108: val_loss did not improve from 0.19216\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1937 - accuracy: 0.9942 - val_loss: 0.1933 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 109/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9940\n","Epoch 109: val_loss improved from 0.19216 to 0.19050, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.1938 - accuracy: 0.9940 - val_loss: 0.1905 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 110/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.9939\n","Epoch 110: val_loss improved from 0.19050 to 0.18929, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1915 - accuracy: 0.9939 - val_loss: 0.1893 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 111/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9932\n","Epoch 111: val_loss did not improve from 0.18929\n","56/56 [==============================] - 12s 222ms/step - loss: 0.1932 - accuracy: 0.9932 - val_loss: 0.1909 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 112/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1892 - accuracy: 0.9942\n","Epoch 112: val_loss improved from 0.18929 to 0.18834, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1892 - accuracy: 0.9942 - val_loss: 0.1883 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 113/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1893 - accuracy: 0.9943\n","Epoch 113: val_loss improved from 0.18834 to 0.18794, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1893 - accuracy: 0.9943 - val_loss: 0.1879 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 114/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1858 - accuracy: 0.9946\n","Epoch 114: val_loss improved from 0.18794 to 0.18572, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1858 - accuracy: 0.9946 - val_loss: 0.1857 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 115/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9944\n","Epoch 115: val_loss improved from 0.18572 to 0.18362, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1873 - accuracy: 0.9944 - val_loss: 0.1836 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 116/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9947\n","Epoch 116: val_loss improved from 0.18362 to 0.18268, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1862 - accuracy: 0.9947 - val_loss: 0.1827 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 117/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1849 - accuracy: 0.9944\n","Epoch 117: val_loss improved from 0.18268 to 0.18141, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1849 - accuracy: 0.9944 - val_loss: 0.1814 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 118/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1850 - accuracy: 0.9933\n","Epoch 118: val_loss improved from 0.18141 to 0.17924, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.1850 - accuracy: 0.9933 - val_loss: 0.1792 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 119/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.9940\n","Epoch 119: val_loss did not improve from 0.17924\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1833 - accuracy: 0.9940 - val_loss: 0.1812 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 120/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1827 - accuracy: 0.9944\n","Epoch 120: val_loss did not improve from 0.17924\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1827 - accuracy: 0.9944 - val_loss: 0.1793 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 121/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9947\n","Epoch 121: val_loss improved from 0.17924 to 0.17901, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 223ms/step - loss: 0.1797 - accuracy: 0.9947 - val_loss: 0.1790 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 122/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9949\n","Epoch 122: val_loss improved from 0.17901 to 0.17681, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 227ms/step - loss: 0.1759 - accuracy: 0.9949 - val_loss: 0.1768 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 123/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1776 - accuracy: 0.9950\n","Epoch 123: val_loss did not improve from 0.17681\n","56/56 [==============================] - 12s 216ms/step - loss: 0.1776 - accuracy: 0.9950 - val_loss: 0.1771 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 124/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9949\n","Epoch 124: val_loss improved from 0.17681 to 0.17490, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1774 - accuracy: 0.9949 - val_loss: 0.1749 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 125/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.9947\n","Epoch 125: val_loss improved from 0.17490 to 0.17275, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.1755 - accuracy: 0.9947 - val_loss: 0.1728 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 126/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9949\n","Epoch 126: val_loss did not improve from 0.17275\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1744 - accuracy: 0.9949 - val_loss: 0.1730 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 127/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9949\n","Epoch 127: val_loss improved from 0.17275 to 0.17024, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.1749 - accuracy: 0.9949 - val_loss: 0.1702 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 128/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9950\n","Epoch 128: val_loss did not improve from 0.17024\n","56/56 [==============================] - 12s 222ms/step - loss: 0.1712 - accuracy: 0.9950 - val_loss: 0.1705 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 129/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 0.9952\n","Epoch 129: val_loss improved from 0.17024 to 0.16752, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.1688 - accuracy: 0.9952 - val_loss: 0.1675 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 130/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1695 - accuracy: 0.9952\n","Epoch 130: val_loss improved from 0.16752 to 0.16695, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1695 - accuracy: 0.9952 - val_loss: 0.1670 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 131/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9953\n","Epoch 131: val_loss improved from 0.16695 to 0.16652, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.1682 - accuracy: 0.9953 - val_loss: 0.1665 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 132/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.9951\n","Epoch 132: val_loss improved from 0.16652 to 0.16570, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.1714 - accuracy: 0.9951 - val_loss: 0.1657 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 133/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1676 - accuracy: 0.9953\n","Epoch 133: val_loss improved from 0.16570 to 0.16441, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1676 - accuracy: 0.9953 - val_loss: 0.1644 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 134/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9953\n","Epoch 134: val_loss did not improve from 0.16441\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1667 - accuracy: 0.9953 - val_loss: 0.1645 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 135/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9953\n","Epoch 135: val_loss improved from 0.16441 to 0.16255, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1656 - accuracy: 0.9953 - val_loss: 0.1625 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 136/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9955\n","Epoch 136: val_loss did not improve from 0.16255\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1642 - accuracy: 0.9955 - val_loss: 0.1630 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 137/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9953\n","Epoch 137: val_loss improved from 0.16255 to 0.16117, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.1656 - accuracy: 0.9953 - val_loss: 0.1612 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 138/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1636 - accuracy: 0.9952\n","Epoch 138: val_loss did not improve from 0.16117\n","56/56 [==============================] - 12s 222ms/step - loss: 0.1636 - accuracy: 0.9952 - val_loss: 0.1630 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 139/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9950\n","Epoch 139: val_loss did not improve from 0.16117\n","56/56 [==============================] - 12s 222ms/step - loss: 0.1627 - accuracy: 0.9950 - val_loss: 0.1696 - val_accuracy: 0.9873 - lr: 1.0000e-04\n","Epoch 140/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1662 - accuracy: 0.9927\n","Epoch 140: val_loss improved from 0.16117 to 0.15888, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.1662 - accuracy: 0.9927 - val_loss: 0.1589 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 141/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9830\n","Epoch 141: val_loss did not improve from 0.15888\n","56/56 [==============================] - 12s 221ms/step - loss: 0.1790 - accuracy: 0.9830 - val_loss: 0.1773 - val_accuracy: 0.9754 - lr: 1.0000e-04\n","Epoch 142/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9871\n","Epoch 142: val_loss did not improve from 0.15888\n","56/56 [==============================] - 12s 223ms/step - loss: 0.1731 - accuracy: 0.9871 - val_loss: 0.1686 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 143/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9920\n","Epoch 143: val_loss did not improve from 0.15888\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1632 - accuracy: 0.9920 - val_loss: 0.1654 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 144/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9935\n","Epoch 144: val_loss did not improve from 0.15888\n","56/56 [==============================] - 12s 222ms/step - loss: 0.1598 - accuracy: 0.9935 - val_loss: 0.1618 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 145/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1587 - accuracy: 0.9942\n","Epoch 145: val_loss did not improve from 0.15888\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1587 - accuracy: 0.9942 - val_loss: 0.1589 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 146/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9948\n","Epoch 146: val_loss improved from 0.15888 to 0.15696, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.1557 - accuracy: 0.9948 - val_loss: 0.1570 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 147/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.9947\n","Epoch 147: val_loss improved from 0.15696 to 0.15582, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 235ms/step - loss: 0.1565 - accuracy: 0.9947 - val_loss: 0.1558 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 148/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1547 - accuracy: 0.9947\n","Epoch 148: val_loss improved from 0.15582 to 0.15452, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1547 - accuracy: 0.9947 - val_loss: 0.1545 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 149/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1524 - accuracy: 0.9951\n","Epoch 149: val_loss improved from 0.15452 to 0.15224, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.1524 - accuracy: 0.9951 - val_loss: 0.1522 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 150/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9953\n","Epoch 150: val_loss improved from 0.15224 to 0.15168, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1529 - accuracy: 0.9953 - val_loss: 0.1517 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 151/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1537 - accuracy: 0.9948\n","Epoch 151: val_loss improved from 0.15168 to 0.14922, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.1537 - accuracy: 0.9948 - val_loss: 0.1492 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 152/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1536 - accuracy: 0.9949\n","Epoch 152: val_loss improved from 0.14922 to 0.14575, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 227ms/step - loss: 0.1536 - accuracy: 0.9949 - val_loss: 0.1457 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 153/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9953\n","Epoch 153: val_loss did not improve from 0.14575\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1503 - accuracy: 0.9953 - val_loss: 0.1480 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 154/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.9952\n","Epoch 154: val_loss did not improve from 0.14575\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1517 - accuracy: 0.9952 - val_loss: 0.1462 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 155/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1532 - accuracy: 0.9954\n","Epoch 155: val_loss did not improve from 0.14575\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1532 - accuracy: 0.9954 - val_loss: 0.1476 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 156/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.9948\n","Epoch 156: val_loss did not improve from 0.14575\n","56/56 [==============================] - 12s 216ms/step - loss: 0.1502 - accuracy: 0.9948 - val_loss: 0.1523 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 157/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.9934\n","Epoch 157: val_loss did not improve from 0.14575\n","56/56 [==============================] - 12s 221ms/step - loss: 0.1517 - accuracy: 0.9934 - val_loss: 0.1470 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 158/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.9949\n","Epoch 158: val_loss did not improve from 0.14575\n","56/56 [==============================] - 12s 218ms/step - loss: 0.1486 - accuracy: 0.9949 - val_loss: 0.1459 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 159/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.9950\n","Epoch 159: val_loss improved from 0.14575 to 0.14457, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.1462 - accuracy: 0.9950 - val_loss: 0.1446 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 160/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1475 - accuracy: 0.9953\n","Epoch 160: val_loss improved from 0.14457 to 0.14389, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1475 - accuracy: 0.9953 - val_loss: 0.1439 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 161/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9956\n","Epoch 161: val_loss improved from 0.14389 to 0.14334, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1447 - accuracy: 0.9956 - val_loss: 0.1433 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 162/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.9955\n","Epoch 162: val_loss improved from 0.14334 to 0.14130, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1419 - accuracy: 0.9955 - val_loss: 0.1413 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 163/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1415 - accuracy: 0.9955\n","Epoch 163: val_loss improved from 0.14130 to 0.13932, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.1415 - accuracy: 0.9955 - val_loss: 0.1393 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 164/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.9957\n","Epoch 164: val_loss improved from 0.13932 to 0.13859, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1413 - accuracy: 0.9957 - val_loss: 0.1386 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 165/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1404 - accuracy: 0.9958\n","Epoch 165: val_loss improved from 0.13859 to 0.13845, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1404 - accuracy: 0.9958 - val_loss: 0.1385 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 166/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9959\n","Epoch 166: val_loss improved from 0.13845 to 0.13789, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.1411 - accuracy: 0.9959 - val_loss: 0.1379 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 167/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9959\n","Epoch 167: val_loss improved from 0.13789 to 0.13655, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.1392 - accuracy: 0.9959 - val_loss: 0.1365 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 168/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1393 - accuracy: 0.9959\n","Epoch 168: val_loss improved from 0.13655 to 0.13593, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.1393 - accuracy: 0.9959 - val_loss: 0.1359 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 169/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1391 - accuracy: 0.9960\n","Epoch 169: val_loss improved from 0.13593 to 0.13549, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1391 - accuracy: 0.9960 - val_loss: 0.1355 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 170/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9960\n","Epoch 170: val_loss improved from 0.13549 to 0.13422, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.1365 - accuracy: 0.9960 - val_loss: 0.1342 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 171/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9959\n","Epoch 171: val_loss improved from 0.13422 to 0.13321, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.1396 - accuracy: 0.9959 - val_loss: 0.1332 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 172/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1376 - accuracy: 0.9958\n","Epoch 172: val_loss improved from 0.13321 to 0.13144, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.1376 - accuracy: 0.9958 - val_loss: 0.1314 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 173/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9959\n","Epoch 173: val_loss did not improve from 0.13144\n","56/56 [==============================] - 12s 223ms/step - loss: 0.1354 - accuracy: 0.9959 - val_loss: 0.1324 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 174/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1340 - accuracy: 0.9957\n","Epoch 174: val_loss improved from 0.13144 to 0.13108, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.1340 - accuracy: 0.9957 - val_loss: 0.1311 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 175/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1320 - accuracy: 0.9960\n","Epoch 175: val_loss improved from 0.13108 to 0.13010, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 227ms/step - loss: 0.1320 - accuracy: 0.9960 - val_loss: 0.1301 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 176/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 0.9961\n","Epoch 176: val_loss improved from 0.13010 to 0.12893, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.1325 - accuracy: 0.9961 - val_loss: 0.1289 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 177/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.9962\n","Epoch 177: val_loss improved from 0.12893 to 0.12826, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1326 - accuracy: 0.9962 - val_loss: 0.1283 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 178/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9961\n","Epoch 178: val_loss did not improve from 0.12826\n","56/56 [==============================] - 12s 218ms/step - loss: 0.1318 - accuracy: 0.9961 - val_loss: 0.1290 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 179/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.9960\n","Epoch 179: val_loss improved from 0.12826 to 0.12743, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 223ms/step - loss: 0.1357 - accuracy: 0.9960 - val_loss: 0.1274 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 180/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9963\n","Epoch 180: val_loss improved from 0.12743 to 0.12714, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.1296 - accuracy: 0.9963 - val_loss: 0.1271 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 181/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1339 - accuracy: 0.9962\n","Epoch 181: val_loss improved from 0.12714 to 0.12709, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1339 - accuracy: 0.9962 - val_loss: 0.1271 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 182/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9963\n","Epoch 182: val_loss improved from 0.12709 to 0.12671, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.1292 - accuracy: 0.9963 - val_loss: 0.1267 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 183/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9953\n","Epoch 183: val_loss improved from 0.12671 to 0.12352, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.1323 - accuracy: 0.9953 - val_loss: 0.1235 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 184/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1270 - accuracy: 0.9961\n","Epoch 184: val_loss did not improve from 0.12352\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1270 - accuracy: 0.9961 - val_loss: 0.1257 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 185/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9964\n","Epoch 185: val_loss did not improve from 0.12352\n","56/56 [==============================] - 12s 223ms/step - loss: 0.1247 - accuracy: 0.9964 - val_loss: 0.1248 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 186/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9962\n","Epoch 186: val_loss improved from 0.12352 to 0.12229, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1260 - accuracy: 0.9962 - val_loss: 0.1223 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 187/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9964\n","Epoch 187: val_loss improved from 0.12229 to 0.12213, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1250 - accuracy: 0.9964 - val_loss: 0.1221 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 188/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9964\n","Epoch 188: val_loss improved from 0.12213 to 0.12192, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 227ms/step - loss: 0.1232 - accuracy: 0.9964 - val_loss: 0.1219 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 189/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9964\n","Epoch 189: val_loss improved from 0.12192 to 0.12098, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1241 - accuracy: 0.9964 - val_loss: 0.1210 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 190/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9964\n","Epoch 190: val_loss improved from 0.12098 to 0.12032, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.1252 - accuracy: 0.9964 - val_loss: 0.1203 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 191/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1226 - accuracy: 0.9965\n","Epoch 191: val_loss did not improve from 0.12032\n","56/56 [==============================] - 12s 222ms/step - loss: 0.1226 - accuracy: 0.9965 - val_loss: 0.1205 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 192/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9964\n","Epoch 192: val_loss improved from 0.12032 to 0.11800, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1225 - accuracy: 0.9964 - val_loss: 0.1180 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 193/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9964\n","Epoch 193: val_loss did not improve from 0.11800\n","56/56 [==============================] - 12s 218ms/step - loss: 0.1235 - accuracy: 0.9964 - val_loss: 0.1195 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 194/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9965\n","Epoch 194: val_loss improved from 0.11800 to 0.11724, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 224ms/step - loss: 0.1211 - accuracy: 0.9965 - val_loss: 0.1172 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 195/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9945\n","Epoch 195: val_loss did not improve from 0.11724\n","56/56 [==============================] - 12s 224ms/step - loss: 0.1225 - accuracy: 0.9945 - val_loss: 0.1337 - val_accuracy: 0.9803 - lr: 1.0000e-04\n","Epoch 196/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1303 - accuracy: 0.9911\n","Epoch 196: val_loss did not improve from 0.11724\n","56/56 [==============================] - 12s 216ms/step - loss: 0.1303 - accuracy: 0.9911 - val_loss: 0.1309 - val_accuracy: 0.9831 - lr: 1.0000e-04\n","Epoch 197/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9898\n","Epoch 197: val_loss did not improve from 0.11724\n","56/56 [==============================] - 12s 222ms/step - loss: 0.1268 - accuracy: 0.9898 - val_loss: 0.1344 - val_accuracy: 0.9812 - lr: 1.0000e-04\n","Epoch 198/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9912\n","Epoch 198: val_loss did not improve from 0.11724\n","56/56 [==============================] - 12s 219ms/step - loss: 0.1257 - accuracy: 0.9912 - val_loss: 0.1250 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 199/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.9935\n","Epoch 199: val_loss did not improve from 0.11724\n","56/56 [==============================] - 12s 222ms/step - loss: 0.1217 - accuracy: 0.9935 - val_loss: 0.1202 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 200/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9947\n","Epoch 200: val_loss did not improve from 0.11724\n","56/56 [==============================] - 12s 218ms/step - loss: 0.1246 - accuracy: 0.9947 - val_loss: 0.1185 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 201/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9956\n","Epoch 201: val_loss did not improve from 0.11724\n","56/56 [==============================] - 12s 221ms/step - loss: 0.1166 - accuracy: 0.9956 - val_loss: 0.1186 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 202/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9958\n","Epoch 202: val_loss improved from 0.11724 to 0.11613, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.1187 - accuracy: 0.9958 - val_loss: 0.1161 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 203/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9960\n","Epoch 203: val_loss improved from 0.11613 to 0.11554, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1161 - accuracy: 0.9960 - val_loss: 0.1155 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 204/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9960\n","Epoch 204: val_loss improved from 0.11554 to 0.11348, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1170 - accuracy: 0.9960 - val_loss: 0.1135 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 205/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9961\n","Epoch 205: val_loss improved from 0.11348 to 0.11129, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.1160 - accuracy: 0.9961 - val_loss: 0.1113 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 206/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1124 - accuracy: 0.9964\n","Epoch 206: val_loss improved from 0.11129 to 0.11028, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1124 - accuracy: 0.9964 - val_loss: 0.1103 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 207/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9964\n","Epoch 207: val_loss improved from 0.11028 to 0.11020, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.1155 - accuracy: 0.9964 - val_loss: 0.1102 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 208/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1122 - accuracy: 0.9965\n","Epoch 208: val_loss improved from 0.11020 to 0.10876, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 232ms/step - loss: 0.1122 - accuracy: 0.9965 - val_loss: 0.1088 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 209/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.9965\n","Epoch 209: val_loss did not improve from 0.10876\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1118 - accuracy: 0.9965 - val_loss: 0.1091 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 210/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9965\n","Epoch 210: val_loss improved from 0.10876 to 0.10783, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.1126 - accuracy: 0.9965 - val_loss: 0.1078 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 211/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1105 - accuracy: 0.9966\n","Epoch 211: val_loss improved from 0.10783 to 0.10672, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1105 - accuracy: 0.9966 - val_loss: 0.1067 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 212/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9967\n","Epoch 212: val_loss did not improve from 0.10672\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1103 - accuracy: 0.9967 - val_loss: 0.1072 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 213/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9968\n","Epoch 213: val_loss improved from 0.10672 to 0.10557, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.1074 - accuracy: 0.9968 - val_loss: 0.1056 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 214/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9968\n","Epoch 214: val_loss did not improve from 0.10557\n","56/56 [==============================] - 12s 222ms/step - loss: 0.1082 - accuracy: 0.9968 - val_loss: 0.1057 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 215/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9964\n","Epoch 215: val_loss improved from 0.10557 to 0.10474, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.1116 - accuracy: 0.9964 - val_loss: 0.1047 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 216/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9955\n","Epoch 216: val_loss did not improve from 0.10474\n","56/56 [==============================] - 12s 222ms/step - loss: 0.1098 - accuracy: 0.9955 - val_loss: 0.1055 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 217/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9961\n","Epoch 217: val_loss improved from 0.10474 to 0.10227, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1108 - accuracy: 0.9961 - val_loss: 0.1023 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 218/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9964\n","Epoch 218: val_loss did not improve from 0.10227\n","56/56 [==============================] - 13s 225ms/step - loss: 0.1077 - accuracy: 0.9964 - val_loss: 0.1052 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 219/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9966\n","Epoch 219: val_loss did not improve from 0.10227\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1043 - accuracy: 0.9966 - val_loss: 0.1030 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 220/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9967\n","Epoch 220: val_loss did not improve from 0.10227\n","56/56 [==============================] - 12s 223ms/step - loss: 0.1035 - accuracy: 0.9967 - val_loss: 0.1025 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 221/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9967\n","Epoch 221: val_loss did not improve from 0.10227\n","56/56 [==============================] - 12s 223ms/step - loss: 0.1045 - accuracy: 0.9967 - val_loss: 0.1024 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 222/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9967\n","Epoch 222: val_loss improved from 0.10227 to 0.10205, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1039 - accuracy: 0.9967 - val_loss: 0.1020 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 223/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9968\n","Epoch 223: val_loss improved from 0.10205 to 0.10091, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.1023 - accuracy: 0.9968 - val_loss: 0.1009 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 224/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9968\n","Epoch 224: val_loss improved from 0.10091 to 0.09916, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1026 - accuracy: 0.9968 - val_loss: 0.0992 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 225/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9968\n","Epoch 225: val_loss did not improve from 0.09916\n","56/56 [==============================] - 12s 222ms/step - loss: 0.1004 - accuracy: 0.9968 - val_loss: 0.0992 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 226/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9968\n","Epoch 226: val_loss did not improve from 0.09916\n","56/56 [==============================] - 12s 223ms/step - loss: 0.1026 - accuracy: 0.9968 - val_loss: 0.0997 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 227/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9969\n","Epoch 227: val_loss did not improve from 0.09916\n","56/56 [==============================] - 12s 217ms/step - loss: 0.1020 - accuracy: 0.9969 - val_loss: 0.0997 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 228/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9970\n","Epoch 228: val_loss improved from 0.09916 to 0.09869, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 227ms/step - loss: 0.1021 - accuracy: 0.9970 - val_loss: 0.0987 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 229/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9969\n","Epoch 229: val_loss improved from 0.09869 to 0.09817, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.1050 - accuracy: 0.9969 - val_loss: 0.0982 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 230/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9969\n","Epoch 230: val_loss improved from 0.09817 to 0.09776, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.1035 - accuracy: 0.9969 - val_loss: 0.0978 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 231/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9968\n","Epoch 231: val_loss improved from 0.09776 to 0.09760, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.1019 - accuracy: 0.9968 - val_loss: 0.0976 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 232/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9969\n","Epoch 232: val_loss improved from 0.09760 to 0.09653, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0977 - accuracy: 0.9969 - val_loss: 0.0965 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 233/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9970\n","Epoch 233: val_loss improved from 0.09653 to 0.09582, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0999 - accuracy: 0.9970 - val_loss: 0.0958 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 234/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9970\n","Epoch 234: val_loss did not improve from 0.09582\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0985 - accuracy: 0.9970 - val_loss: 0.0963 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 235/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9971\n","Epoch 235: val_loss did not improve from 0.09582\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0989 - accuracy: 0.9971 - val_loss: 0.0964 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 236/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9971\n","Epoch 236: val_loss improved from 0.09582 to 0.09359, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.0977 - accuracy: 0.9971 - val_loss: 0.0936 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 237/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9972\n","Epoch 237: val_loss did not improve from 0.09359\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0953 - accuracy: 0.9972 - val_loss: 0.0943 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 238/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.9972\n","Epoch 238: val_loss did not improve from 0.09359\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0956 - accuracy: 0.9972 - val_loss: 0.0939 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 239/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9971\n","Epoch 239: val_loss improved from 0.09359 to 0.09307, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0938 - accuracy: 0.9971 - val_loss: 0.0931 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 240/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0939 - accuracy: 0.9971\n","Epoch 240: val_loss improved from 0.09307 to 0.09147, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0939 - accuracy: 0.9971 - val_loss: 0.0915 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 241/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9971\n","Epoch 241: val_loss improved from 0.09147 to 0.09097, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.0938 - accuracy: 0.9971 - val_loss: 0.0910 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 242/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9972\n","Epoch 242: val_loss did not improve from 0.09097\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0934 - accuracy: 0.9972 - val_loss: 0.0913 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 243/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9972\n","Epoch 243: val_loss did not improve from 0.09097\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0946 - accuracy: 0.9972 - val_loss: 0.0922 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 244/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9968\n","Epoch 244: val_loss did not improve from 0.09097\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0934 - accuracy: 0.9968 - val_loss: 0.0930 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 245/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9963\n","Epoch 245: val_loss improved from 0.09097 to 0.08853, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0973 - accuracy: 0.9963 - val_loss: 0.0885 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 246/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 0.9969\n","Epoch 246: val_loss did not improve from 0.08853\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0926 - accuracy: 0.9969 - val_loss: 0.0901 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 247/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9968\n","Epoch 247: val_loss did not improve from 0.08853\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0955 - accuracy: 0.9968 - val_loss: 0.0923 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 248/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0902 - accuracy: 0.9969\n","Epoch 248: val_loss did not improve from 0.08853\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0902 - accuracy: 0.9969 - val_loss: 0.0891 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 249/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9971\n","Epoch 249: val_loss did not improve from 0.08853\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0904 - accuracy: 0.9971 - val_loss: 0.0890 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 250/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9973\n","Epoch 250: val_loss improved from 0.08853 to 0.08792, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0905 - accuracy: 0.9973 - val_loss: 0.0879 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 251/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9970\n","Epoch 251: val_loss did not improve from 0.08792\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0900 - accuracy: 0.9970 - val_loss: 0.0900 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 252/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9971\n","Epoch 252: val_loss improved from 0.08792 to 0.08730, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 224ms/step - loss: 0.0900 - accuracy: 0.9971 - val_loss: 0.0873 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 253/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9972\n","Epoch 253: val_loss did not improve from 0.08730\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0916 - accuracy: 0.9972 - val_loss: 0.0879 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 254/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9957\n","Epoch 254: val_loss did not improve from 0.08730\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0927 - accuracy: 0.9957 - val_loss: 0.0910 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 255/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9969\n","Epoch 255: val_loss did not improve from 0.08730\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0889 - accuracy: 0.9969 - val_loss: 0.0882 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 256/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9971\n","Epoch 256: val_loss improved from 0.08730 to 0.08612, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0873 - accuracy: 0.9971 - val_loss: 0.0861 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 257/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9968\n","Epoch 257: val_loss improved from 0.08612 to 0.08426, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0898 - accuracy: 0.9968 - val_loss: 0.0843 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 258/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9962\n","Epoch 258: val_loss did not improve from 0.08426\n","56/56 [==============================] - 12s 216ms/step - loss: 0.0905 - accuracy: 0.9962 - val_loss: 0.1020 - val_accuracy: 0.9857 - lr: 1.0000e-04\n","Epoch 259/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9951\n","Epoch 259: val_loss did not improve from 0.08426\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0892 - accuracy: 0.9951 - val_loss: 0.0898 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 260/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9922\n","Epoch 260: val_loss did not improve from 0.08426\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0970 - accuracy: 0.9922 - val_loss: 0.0979 - val_accuracy: 0.9865 - lr: 1.0000e-04\n","Epoch 261/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9945\n","Epoch 261: val_loss did not improve from 0.08426\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0895 - accuracy: 0.9945 - val_loss: 0.0878 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 262/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9958\n","Epoch 262: val_loss did not improve from 0.08426\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0897 - accuracy: 0.9958 - val_loss: 0.0853 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 263/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9966\n","Epoch 263: val_loss did not improve from 0.08426\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0846 - accuracy: 0.9966 - val_loss: 0.0860 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 264/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9968\n","Epoch 264: val_loss improved from 0.08426 to 0.08405, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0874 - accuracy: 0.9968 - val_loss: 0.0841 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 265/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9969\n","Epoch 265: val_loss did not improve from 0.08405\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0860 - accuracy: 0.9969 - val_loss: 0.0858 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 266/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9971\n","Epoch 266: val_loss improved from 0.08405 to 0.08295, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.0805 - accuracy: 0.9971 - val_loss: 0.0829 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 267/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9971\n","Epoch 267: val_loss improved from 0.08295 to 0.08154, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0844 - accuracy: 0.9971 - val_loss: 0.0815 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 268/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9973\n","Epoch 268: val_loss improved from 0.08154 to 0.08027, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0833 - accuracy: 0.9973 - val_loss: 0.0803 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 269/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9973\n","Epoch 269: val_loss did not improve from 0.08027\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0861 - accuracy: 0.9973 - val_loss: 0.0808 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 270/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9974\n","Epoch 270: val_loss improved from 0.08027 to 0.07958, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0814 - accuracy: 0.9974 - val_loss: 0.0796 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 271/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9974\n","Epoch 271: val_loss improved from 0.07958 to 0.07916, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0806 - accuracy: 0.9974 - val_loss: 0.0792 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 272/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9974\n","Epoch 272: val_loss improved from 0.07916 to 0.07797, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0819 - accuracy: 0.9974 - val_loss: 0.0780 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 273/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9973\n","Epoch 273: val_loss did not improve from 0.07797\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0829 - accuracy: 0.9973 - val_loss: 0.0789 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 274/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9974\n","Epoch 274: val_loss improved from 0.07797 to 0.07793, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0807 - accuracy: 0.9974 - val_loss: 0.0779 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 275/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9975\n","Epoch 275: val_loss improved from 0.07793 to 0.07692, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0791 - accuracy: 0.9975 - val_loss: 0.0769 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 276/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9975\n","Epoch 276: val_loss did not improve from 0.07692\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0792 - accuracy: 0.9975 - val_loss: 0.0774 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 277/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9975\n","Epoch 277: val_loss improved from 0.07692 to 0.07663, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 224ms/step - loss: 0.0801 - accuracy: 0.9975 - val_loss: 0.0766 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 278/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9976\n","Epoch 278: val_loss improved from 0.07663 to 0.07589, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0789 - accuracy: 0.9976 - val_loss: 0.0759 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 279/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9975\n","Epoch 279: val_loss did not improve from 0.07589\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0800 - accuracy: 0.9975 - val_loss: 0.0760 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 280/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9976\n","Epoch 280: val_loss did not improve from 0.07589\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0772 - accuracy: 0.9976 - val_loss: 0.0760 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 281/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9976\n","Epoch 281: val_loss improved from 0.07589 to 0.07458, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0767 - accuracy: 0.9976 - val_loss: 0.0746 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 282/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9977\n","Epoch 282: val_loss did not improve from 0.07458\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0779 - accuracy: 0.9977 - val_loss: 0.0747 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 283/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9977\n","Epoch 283: val_loss improved from 0.07458 to 0.07408, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0764 - accuracy: 0.9977 - val_loss: 0.0741 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 284/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9976\n","Epoch 284: val_loss did not improve from 0.07408\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0783 - accuracy: 0.9976 - val_loss: 0.0752 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 285/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9976\n","Epoch 285: val_loss did not improve from 0.07408\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0773 - accuracy: 0.9976 - val_loss: 0.0741 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 286/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9977\n","Epoch 286: val_loss improved from 0.07408 to 0.07346, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0772 - accuracy: 0.9977 - val_loss: 0.0735 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 287/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9977\n","Epoch 287: val_loss improved from 0.07346 to 0.07327, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0759 - accuracy: 0.9977 - val_loss: 0.0733 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 288/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9978\n","Epoch 288: val_loss improved from 0.07327 to 0.07167, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0723 - accuracy: 0.9978 - val_loss: 0.0717 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 289/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9978\n","Epoch 289: val_loss did not improve from 0.07167\n","56/56 [==============================] - 12s 220ms/step - loss: 0.0731 - accuracy: 0.9978 - val_loss: 0.0720 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 290/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9977\n","Epoch 290: val_loss did not improve from 0.07167\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0751 - accuracy: 0.9977 - val_loss: 0.0724 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 291/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9978\n","Epoch 291: val_loss improved from 0.07167 to 0.07165, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0723 - accuracy: 0.9978 - val_loss: 0.0717 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 292/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9978\n","Epoch 292: val_loss did not improve from 0.07165\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0709 - accuracy: 0.9978 - val_loss: 0.0717 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 293/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9978\n","Epoch 293: val_loss improved from 0.07165 to 0.07110, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0742 - accuracy: 0.9978 - val_loss: 0.0711 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 294/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9978\n","Epoch 294: val_loss improved from 0.07110 to 0.07035, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.0721 - accuracy: 0.9978 - val_loss: 0.0704 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 295/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9979\n","Epoch 295: val_loss did not improve from 0.07035\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0721 - accuracy: 0.9979 - val_loss: 0.0704 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 296/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9978\n","Epoch 296: val_loss improved from 0.07035 to 0.06929, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0708 - accuracy: 0.9978 - val_loss: 0.0693 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 297/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9979\n","Epoch 297: val_loss did not improve from 0.06929\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0702 - accuracy: 0.9979 - val_loss: 0.0695 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 298/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9979\n","Epoch 298: val_loss improved from 0.06929 to 0.06830, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.0724 - accuracy: 0.9979 - val_loss: 0.0683 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 299/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9979\n","Epoch 299: val_loss did not improve from 0.06830\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0685 - accuracy: 0.9979 - val_loss: 0.0688 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 300/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9973\n","Epoch 300: val_loss improved from 0.06830 to 0.06667, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0709 - accuracy: 0.9973 - val_loss: 0.0667 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 301/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9976\n","Epoch 301: val_loss did not improve from 0.06667\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0699 - accuracy: 0.9976 - val_loss: 0.0684 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 302/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9979\n","Epoch 302: val_loss did not improve from 0.06667\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0685 - accuracy: 0.9979 - val_loss: 0.0681 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 303/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9979\n","Epoch 303: val_loss did not improve from 0.06667\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0678 - accuracy: 0.9979 - val_loss: 0.0676 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 304/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9980\n","Epoch 304: val_loss did not improve from 0.06667\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0685 - accuracy: 0.9980 - val_loss: 0.0670 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 305/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9980\n","Epoch 305: val_loss improved from 0.06667 to 0.06586, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0696 - accuracy: 0.9980 - val_loss: 0.0659 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 306/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9979\n","Epoch 306: val_loss did not improve from 0.06586\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0692 - accuracy: 0.9979 - val_loss: 0.0667 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 307/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9980\n","Epoch 307: val_loss did not improve from 0.06586\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0695 - accuracy: 0.9980 - val_loss: 0.0659 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 308/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9980\n","Epoch 308: val_loss improved from 0.06586 to 0.06479, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.0672 - accuracy: 0.9980 - val_loss: 0.0648 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 309/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9980\n","Epoch 309: val_loss did not improve from 0.06479\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0656 - accuracy: 0.9980 - val_loss: 0.0655 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 310/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9978\n","Epoch 310: val_loss did not improve from 0.06479\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0684 - accuracy: 0.9978 - val_loss: 0.0652 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 311/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9980\n","Epoch 311: val_loss improved from 0.06479 to 0.06404, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0667 - accuracy: 0.9980 - val_loss: 0.0640 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 312/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9979\n","Epoch 312: val_loss did not improve from 0.06404\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0671 - accuracy: 0.9979 - val_loss: 0.0645 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 313/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9978\n","Epoch 313: val_loss improved from 0.06404 to 0.06253, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.0673 - accuracy: 0.9978 - val_loss: 0.0625 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 314/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9980\n","Epoch 314: val_loss did not improve from 0.06253\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0666 - accuracy: 0.9980 - val_loss: 0.0636 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 315/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9981\n","Epoch 315: val_loss did not improve from 0.06253\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0636 - accuracy: 0.9981 - val_loss: 0.0631 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 316/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9980\n","Epoch 316: val_loss improved from 0.06253 to 0.06238, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0657 - accuracy: 0.9980 - val_loss: 0.0624 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 317/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9980\n","Epoch 317: val_loss did not improve from 0.06238\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0671 - accuracy: 0.9980 - val_loss: 0.0642 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 318/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9980\n","Epoch 318: val_loss improved from 0.06238 to 0.06204, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0637 - accuracy: 0.9980 - val_loss: 0.0620 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 319/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9981\n","Epoch 319: val_loss did not improve from 0.06204\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0652 - accuracy: 0.9981 - val_loss: 0.0633 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 320/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9981\n","Epoch 320: val_loss did not improve from 0.06204\n","56/56 [==============================] - 12s 216ms/step - loss: 0.0630 - accuracy: 0.9981 - val_loss: 0.0628 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 321/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9981\n","Epoch 321: val_loss improved from 0.06204 to 0.06171, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0630 - accuracy: 0.9981 - val_loss: 0.0617 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 322/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9981\n","Epoch 322: val_loss did not improve from 0.06171\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0665 - accuracy: 0.9981 - val_loss: 0.0625 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 323/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9982\n","Epoch 323: val_loss improved from 0.06171 to 0.06000, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0612 - accuracy: 0.9982 - val_loss: 0.0600 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 324/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9980\n","Epoch 324: val_loss did not improve from 0.06000\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0645 - accuracy: 0.9980 - val_loss: 0.0618 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 325/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9982\n","Epoch 325: val_loss did not improve from 0.06000\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0637 - accuracy: 0.9982 - val_loss: 0.0615 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 326/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9982\n","Epoch 326: val_loss did not improve from 0.06000\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0613 - accuracy: 0.9982 - val_loss: 0.0606 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 327/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9983\n","Epoch 327: val_loss improved from 0.06000 to 0.05949, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0608 - accuracy: 0.9983 - val_loss: 0.0595 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 328/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9982\n","Epoch 328: val_loss improved from 0.05949 to 0.05945, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0626 - accuracy: 0.9982 - val_loss: 0.0594 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 329/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9982\n","Epoch 329: val_loss improved from 0.05945 to 0.05906, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0595 - accuracy: 0.9982 - val_loss: 0.0591 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 330/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9983\n","Epoch 330: val_loss improved from 0.05906 to 0.05811, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0615 - accuracy: 0.9983 - val_loss: 0.0581 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 331/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9983\n","Epoch 331: val_loss did not improve from 0.05811\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0595 - accuracy: 0.9983 - val_loss: 0.0585 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 332/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9983\n","Epoch 332: val_loss improved from 0.05811 to 0.05771, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.0590 - accuracy: 0.9983 - val_loss: 0.0577 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 333/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9978\n","Epoch 333: val_loss did not improve from 0.05771\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0632 - accuracy: 0.9978 - val_loss: 0.0935 - val_accuracy: 0.9801 - lr: 1.0000e-04\n","Epoch 334/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9964\n","Epoch 334: val_loss did not improve from 0.05771\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0613 - accuracy: 0.9964 - val_loss: 0.0736 - val_accuracy: 0.9876 - lr: 1.0000e-04\n","Epoch 335/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9922\n","Epoch 335: val_loss did not improve from 0.05771\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0703 - accuracy: 0.9922 - val_loss: 0.3049 - val_accuracy: 0.8697 - lr: 1.0000e-04\n","Epoch 336/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9946\n","Epoch 336: val_loss did not improve from 0.05771\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0680 - accuracy: 0.9946 - val_loss: 0.0639 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 337/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9970\n","Epoch 337: val_loss did not improve from 0.05771\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0599 - accuracy: 0.9970 - val_loss: 0.0625 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 338/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9975\n","Epoch 338: val_loss did not improve from 0.05771\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0603 - accuracy: 0.9975 - val_loss: 0.0608 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 339/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9977\n","Epoch 339: val_loss did not improve from 0.05771\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0580 - accuracy: 0.9977 - val_loss: 0.0595 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 340/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9978\n","Epoch 340: val_loss did not improve from 0.05771\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0588 - accuracy: 0.9978 - val_loss: 0.0581 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 341/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9980\n","Epoch 341: val_loss improved from 0.05771 to 0.05704, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0574 - accuracy: 0.9980 - val_loss: 0.0570 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 342/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9981\n","Epoch 342: val_loss did not improve from 0.05704\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0573 - accuracy: 0.9981 - val_loss: 0.0571 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 343/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9982\n","Epoch 343: val_loss improved from 0.05704 to 0.05684, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.0595 - accuracy: 0.9982 - val_loss: 0.0568 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 344/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9982\n","Epoch 344: val_loss improved from 0.05684 to 0.05591, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0581 - accuracy: 0.9982 - val_loss: 0.0559 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 345/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9983\n","Epoch 345: val_loss improved from 0.05591 to 0.05483, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0541 - accuracy: 0.9983 - val_loss: 0.0548 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 346/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9983\n","Epoch 346: val_loss did not improve from 0.05483\n","56/56 [==============================] - 12s 216ms/step - loss: 0.0559 - accuracy: 0.9983 - val_loss: 0.0549 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 347/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9983\n","Epoch 347: val_loss did not improve from 0.05483\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0571 - accuracy: 0.9983 - val_loss: 0.0559 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 348/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9982\n","Epoch 348: val_loss improved from 0.05483 to 0.05438, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.0547 - accuracy: 0.9982 - val_loss: 0.0544 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 349/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9984\n","Epoch 349: val_loss did not improve from 0.05438\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0557 - accuracy: 0.9984 - val_loss: 0.0549 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 350/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9984\n","Epoch 350: val_loss improved from 0.05438 to 0.05421, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.0554 - accuracy: 0.9984 - val_loss: 0.0542 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 351/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.9984\n","Epoch 351: val_loss improved from 0.05421 to 0.05408, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.0546 - accuracy: 0.9984 - val_loss: 0.0541 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 352/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9984\n","Epoch 352: val_loss improved from 0.05408 to 0.05342, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0544 - accuracy: 0.9984 - val_loss: 0.0534 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 353/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9985\n","Epoch 353: val_loss improved from 0.05342 to 0.05301, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 224ms/step - loss: 0.0544 - accuracy: 0.9985 - val_loss: 0.0530 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 354/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9985\n","Epoch 354: val_loss improved from 0.05301 to 0.05245, saving model to unet_bce.hdf5\n","56/56 [==============================] - 12s 224ms/step - loss: 0.0536 - accuracy: 0.9985 - val_loss: 0.0524 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 355/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9985\n","Epoch 355: val_loss improved from 0.05245 to 0.05207, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0529 - accuracy: 0.9985 - val_loss: 0.0521 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 356/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9985\n","Epoch 356: val_loss improved from 0.05207 to 0.05173, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0529 - accuracy: 0.9985 - val_loss: 0.0517 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 357/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9985\n","Epoch 357: val_loss improved from 0.05173 to 0.05171, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 227ms/step - loss: 0.0530 - accuracy: 0.9985 - val_loss: 0.0517 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 358/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9985\n","Epoch 358: val_loss did not improve from 0.05171\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0525 - accuracy: 0.9985 - val_loss: 0.0523 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 359/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9986\n","Epoch 359: val_loss did not improve from 0.05171\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0528 - accuracy: 0.9986 - val_loss: 0.0521 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 360/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9986\n","Epoch 360: val_loss improved from 0.05171 to 0.05070, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0510 - accuracy: 0.9986 - val_loss: 0.0507 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 361/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9985\n","Epoch 361: val_loss did not improve from 0.05070\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0534 - accuracy: 0.9985 - val_loss: 0.0514 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 362/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9986\n","Epoch 362: val_loss improved from 0.05070 to 0.05037, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0531 - accuracy: 0.9986 - val_loss: 0.0504 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 363/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9986\n","Epoch 363: val_loss did not improve from 0.05037\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0522 - accuracy: 0.9986 - val_loss: 0.0504 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 364/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9986\n","Epoch 364: val_loss did not improve from 0.05037\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0507 - accuracy: 0.9986 - val_loss: 0.0508 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 365/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9986\n","Epoch 365: val_loss improved from 0.05037 to 0.04963, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 227ms/step - loss: 0.0507 - accuracy: 0.9986 - val_loss: 0.0496 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 366/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9987\n","Epoch 366: val_loss improved from 0.04963 to 0.04910, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0509 - accuracy: 0.9987 - val_loss: 0.0491 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 367/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9986\n","Epoch 367: val_loss did not improve from 0.04910\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0498 - accuracy: 0.9986 - val_loss: 0.0496 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 368/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9986\n","Epoch 368: val_loss did not improve from 0.04910\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0500 - accuracy: 0.9986 - val_loss: 0.0494 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 369/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9987\n","Epoch 369: val_loss improved from 0.04910 to 0.04881, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0513 - accuracy: 0.9987 - val_loss: 0.0488 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 370/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9986\n","Epoch 370: val_loss did not improve from 0.04881\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0497 - accuracy: 0.9986 - val_loss: 0.0494 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 371/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9987\n","Epoch 371: val_loss did not improve from 0.04881\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0485 - accuracy: 0.9987 - val_loss: 0.0489 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 372/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9986\n","Epoch 372: val_loss improved from 0.04881 to 0.04843, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 227ms/step - loss: 0.0488 - accuracy: 0.9986 - val_loss: 0.0484 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 373/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9986\n","Epoch 373: val_loss improved from 0.04843 to 0.04762, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0490 - accuracy: 0.9986 - val_loss: 0.0476 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 374/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9986\n","Epoch 374: val_loss did not improve from 0.04762\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0494 - accuracy: 0.9986 - val_loss: 0.0479 - val_accuracy: 0.9971 - lr: 1.0000e-04\n","Epoch 375/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9987\n","Epoch 375: val_loss did not improve from 0.04762\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0484 - accuracy: 0.9987 - val_loss: 0.0478 - val_accuracy: 0.9971 - lr: 1.0000e-04\n","Epoch 376/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9987\n","Epoch 376: val_loss did not improve from 0.04762\n","56/56 [==============================] - 12s 216ms/step - loss: 0.0484 - accuracy: 0.9987 - val_loss: 0.0477 - val_accuracy: 0.9971 - lr: 1.0000e-04\n","Epoch 377/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9987\n","Epoch 377: val_loss did not improve from 0.04762\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0483 - accuracy: 0.9987 - val_loss: 0.0478 - val_accuracy: 0.9971 - lr: 1.0000e-04\n","Epoch 378/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9987\n","Epoch 378: val_loss improved from 0.04762 to 0.04576, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0496 - accuracy: 0.9987 - val_loss: 0.0458 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 379/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9987\n","Epoch 379: val_loss did not improve from 0.04576\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0501 - accuracy: 0.9987 - val_loss: 0.0469 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 380/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9988\n","Epoch 380: val_loss did not improve from 0.04576\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0468 - accuracy: 0.9988 - val_loss: 0.0472 - val_accuracy: 0.9971 - lr: 1.0000e-04\n","Epoch 381/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9988\n","Epoch 381: val_loss did not improve from 0.04576\n","56/56 [==============================] - 12s 216ms/step - loss: 0.0479 - accuracy: 0.9988 - val_loss: 0.0470 - val_accuracy: 0.9971 - lr: 1.0000e-04\n","Epoch 382/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9988\n","Epoch 382: val_loss did not improve from 0.04576\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0451 - accuracy: 0.9988 - val_loss: 0.0461 - val_accuracy: 0.9971 - lr: 1.0000e-04\n","Epoch 383/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9987\n","Epoch 383: val_loss improved from 0.04576 to 0.04440, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.0480 - accuracy: 0.9987 - val_loss: 0.0444 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 384/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9988\n","Epoch 384: val_loss did not improve from 0.04440\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0461 - accuracy: 0.9988 - val_loss: 0.0461 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 385/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9988\n","Epoch 385: val_loss did not improve from 0.04440\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0479 - accuracy: 0.9988 - val_loss: 0.0464 - val_accuracy: 0.9971 - lr: 1.0000e-04\n","Epoch 386/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9988\n","Epoch 386: val_loss did not improve from 0.04440\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0458 - accuracy: 0.9988 - val_loss: 0.0458 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 387/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9988\n","Epoch 387: val_loss did not improve from 0.04440\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0449 - accuracy: 0.9988 - val_loss: 0.0453 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 388/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9988\n","Epoch 388: val_loss did not improve from 0.04440\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0457 - accuracy: 0.9988 - val_loss: 0.0458 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 389/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9988\n","Epoch 389: val_loss did not improve from 0.04440\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0473 - accuracy: 0.9988 - val_loss: 0.0449 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 390/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9988\n","Epoch 390: val_loss did not improve from 0.04440\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0463 - accuracy: 0.9988 - val_loss: 0.0448 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 391/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9988\n","Epoch 391: val_loss improved from 0.04440 to 0.04414, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0465 - accuracy: 0.9988 - val_loss: 0.0441 - val_accuracy: 0.9971 - lr: 1.0000e-04\n","Epoch 392/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9988\n","Epoch 392: val_loss did not improve from 0.04414\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0455 - accuracy: 0.9988 - val_loss: 0.0445 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 393/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9989\n","Epoch 393: val_loss did not improve from 0.04414\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0443 - accuracy: 0.9989 - val_loss: 0.0449 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 394/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9989\n","Epoch 394: val_loss did not improve from 0.04414\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0459 - accuracy: 0.9989 - val_loss: 0.0448 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 395/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9988\n","Epoch 395: val_loss improved from 0.04414 to 0.04323, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.0448 - accuracy: 0.9988 - val_loss: 0.0432 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 396/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9973\n","Epoch 396: val_loss did not improve from 0.04323\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0501 - accuracy: 0.9973 - val_loss: 0.1759 - val_accuracy: 0.9492 - lr: 1.0000e-04\n","Epoch 397/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9921\n","Epoch 397: val_loss did not improve from 0.04323\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0559 - accuracy: 0.9921 - val_loss: 0.0931 - val_accuracy: 0.9749 - lr: 1.0000e-04\n","Epoch 398/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9934\n","Epoch 398: val_loss did not improve from 0.04323\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0556 - accuracy: 0.9934 - val_loss: 0.0520 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 399/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9962\n","Epoch 399: val_loss did not improve from 0.04323\n","56/56 [==============================] - 12s 216ms/step - loss: 0.0480 - accuracy: 0.9962 - val_loss: 0.0519 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 400/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9974\n","Epoch 400: val_loss did not improve from 0.04323\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0447 - accuracy: 0.9974 - val_loss: 0.0484 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 401/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9979\n","Epoch 401: val_loss did not improve from 0.04323\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0449 - accuracy: 0.9979 - val_loss: 0.0477 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 402/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9981\n","Epoch 402: val_loss did not improve from 0.04323\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0440 - accuracy: 0.9981 - val_loss: 0.0449 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 403/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9983\n","Epoch 403: val_loss did not improve from 0.04323\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0422 - accuracy: 0.9983 - val_loss: 0.0443 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 404/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9984\n","Epoch 404: val_loss did not improve from 0.04323\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0432 - accuracy: 0.9984 - val_loss: 0.0434 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 405/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9981\n","Epoch 405: val_loss improved from 0.04323 to 0.04309, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.0441 - accuracy: 0.9981 - val_loss: 0.0431 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 406/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9984\n","Epoch 406: val_loss improved from 0.04309 to 0.04257, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0425 - accuracy: 0.9984 - val_loss: 0.0426 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 407/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9985\n","Epoch 407: val_loss improved from 0.04257 to 0.04075, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0441 - accuracy: 0.9985 - val_loss: 0.0407 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 408/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9985\n","Epoch 408: val_loss did not improve from 0.04075\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0426 - accuracy: 0.9985 - val_loss: 0.0413 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 409/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9987\n","Epoch 409: val_loss did not improve from 0.04075\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0406 - accuracy: 0.9987 - val_loss: 0.0416 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 410/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9987\n","Epoch 410: val_loss did not improve from 0.04075\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0413 - accuracy: 0.9987 - val_loss: 0.0412 - val_accuracy: 0.9971 - lr: 1.0000e-04\n","Epoch 411/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9988\n","Epoch 411: val_loss did not improve from 0.04075\n","56/56 [==============================] - 12s 224ms/step - loss: 0.0411 - accuracy: 0.9988 - val_loss: 0.0411 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 412/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9988\n","Epoch 412: val_loss did not improve from 0.04075\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0394 - accuracy: 0.9988 - val_loss: 0.0410 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 413/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9988\n","Epoch 413: val_loss improved from 0.04075 to 0.04042, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.0412 - accuracy: 0.9988 - val_loss: 0.0404 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 414/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9988\n","Epoch 414: val_loss did not improve from 0.04042\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0410 - accuracy: 0.9988 - val_loss: 0.0408 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 415/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9988\n","Epoch 415: val_loss improved from 0.04042 to 0.04014, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0410 - accuracy: 0.9988 - val_loss: 0.0401 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 416/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9989\n","Epoch 416: val_loss did not improve from 0.04014\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0408 - accuracy: 0.9989 - val_loss: 0.0406 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 417/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9989\n","Epoch 417: val_loss improved from 0.04014 to 0.03993, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0390 - accuracy: 0.9989 - val_loss: 0.0399 - val_accuracy: 0.9973 - lr: 1.0000e-04\n","Epoch 418/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9988\n","Epoch 418: val_loss did not improve from 0.03993\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0407 - accuracy: 0.9988 - val_loss: 0.0408 - val_accuracy: 0.9971 - lr: 1.0000e-04\n","Epoch 419/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9989\n","Epoch 419: val_loss improved from 0.03993 to 0.03875, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.0413 - accuracy: 0.9989 - val_loss: 0.0388 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 420/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9989\n","Epoch 420: val_loss did not improve from 0.03875\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0411 - accuracy: 0.9989 - val_loss: 0.0400 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 421/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9990\n","Epoch 421: val_loss did not improve from 0.03875\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0390 - accuracy: 0.9990 - val_loss: 0.0398 - val_accuracy: 0.9973 - lr: 1.0000e-04\n","Epoch 422/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9989\n","Epoch 422: val_loss did not improve from 0.03875\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0402 - accuracy: 0.9989 - val_loss: 0.0395 - val_accuracy: 0.9973 - lr: 1.0000e-04\n","Epoch 423/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9989\n","Epoch 423: val_loss did not improve from 0.03875\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0397 - accuracy: 0.9989 - val_loss: 0.0389 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 424/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9990\n","Epoch 424: val_loss did not improve from 0.03875\n","56/56 [==============================] - 12s 216ms/step - loss: 0.0375 - accuracy: 0.9990 - val_loss: 0.0389 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 425/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9991\n","Epoch 425: val_loss did not improve from 0.03875\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0385 - accuracy: 0.9991 - val_loss: 0.0390 - val_accuracy: 0.9973 - lr: 1.0000e-04\n","Epoch 426/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9991\n","Epoch 426: val_loss improved from 0.03875 to 0.03816, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0370 - accuracy: 0.9991 - val_loss: 0.0382 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 427/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9990\n","Epoch 427: val_loss did not improve from 0.03816\n","56/56 [==============================] - 12s 216ms/step - loss: 0.0403 - accuracy: 0.9990 - val_loss: 0.0384 - val_accuracy: 0.9973 - lr: 1.0000e-04\n","Epoch 428/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9990\n","Epoch 428: val_loss improved from 0.03816 to 0.03770, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 233ms/step - loss: 0.0389 - accuracy: 0.9990 - val_loss: 0.0377 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 429/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9991\n","Epoch 429: val_loss did not improve from 0.03770\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0354 - accuracy: 0.9991 - val_loss: 0.0381 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 430/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9990\n","Epoch 430: val_loss did not improve from 0.03770\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0379 - accuracy: 0.9990 - val_loss: 0.0380 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 431/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9991\n","Epoch 431: val_loss did not improve from 0.03770\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0372 - accuracy: 0.9991 - val_loss: 0.0379 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 432/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9991\n","Epoch 432: val_loss improved from 0.03770 to 0.03756, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0364 - accuracy: 0.9991 - val_loss: 0.0376 - val_accuracy: 0.9975 - lr: 1.0000e-04\n","Epoch 433/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9992\n","Epoch 433: val_loss improved from 0.03756 to 0.03722, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0359 - accuracy: 0.9992 - val_loss: 0.0372 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 434/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9991\n","Epoch 434: val_loss improved from 0.03722 to 0.03721, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0351 - accuracy: 0.9991 - val_loss: 0.0372 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 435/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9991\n","Epoch 435: val_loss improved from 0.03721 to 0.03685, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.0362 - accuracy: 0.9991 - val_loss: 0.0369 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 436/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9991\n","Epoch 436: val_loss improved from 0.03685 to 0.03644, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.0361 - accuracy: 0.9991 - val_loss: 0.0364 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 437/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9991\n","Epoch 437: val_loss improved from 0.03644 to 0.03626, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0379 - accuracy: 0.9991 - val_loss: 0.0363 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 438/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9992\n","Epoch 438: val_loss improved from 0.03626 to 0.03619, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.0359 - accuracy: 0.9992 - val_loss: 0.0362 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 439/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9992\n","Epoch 439: val_loss did not improve from 0.03619\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0360 - accuracy: 0.9992 - val_loss: 0.0364 - val_accuracy: 0.9974 - lr: 1.0000e-04\n","Epoch 440/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9992\n","Epoch 440: val_loss did not improve from 0.03619\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0349 - accuracy: 0.9992 - val_loss: 0.0362 - val_accuracy: 0.9975 - lr: 1.0000e-04\n","Epoch 441/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9990\n","Epoch 441: val_loss did not improve from 0.03619\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0365 - accuracy: 0.9990 - val_loss: 0.0364 - val_accuracy: 0.9973 - lr: 1.0000e-04\n","Epoch 442/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9991\n","Epoch 442: val_loss improved from 0.03619 to 0.03609, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0354 - accuracy: 0.9991 - val_loss: 0.0361 - val_accuracy: 0.9975 - lr: 1.0000e-04\n","Epoch 443/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9985\n","Epoch 443: val_loss did not improve from 0.03609\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0383 - accuracy: 0.9985 - val_loss: 0.0697 - val_accuracy: 0.9836 - lr: 1.0000e-04\n","Epoch 444/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9907\n","Epoch 444: val_loss did not improve from 0.03609\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0573 - accuracy: 0.9907 - val_loss: 0.0706 - val_accuracy: 0.9809 - lr: 1.0000e-04\n","Epoch 445/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9968\n","Epoch 445: val_loss did not improve from 0.03609\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0417 - accuracy: 0.9968 - val_loss: 0.0395 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 446/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9977\n","Epoch 446: val_loss did not improve from 0.03609\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0358 - accuracy: 0.9977 - val_loss: 0.0404 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 447/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9981\n","Epoch 447: val_loss did not improve from 0.03609\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0369 - accuracy: 0.9981 - val_loss: 0.0405 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 448/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9984\n","Epoch 448: val_loss did not improve from 0.03609\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0356 - accuracy: 0.9984 - val_loss: 0.0391 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 449/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9984\n","Epoch 449: val_loss did not improve from 0.03609\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0392 - accuracy: 0.9984 - val_loss: 0.0391 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 450/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9987\n","Epoch 450: val_loss did not improve from 0.03609\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0347 - accuracy: 0.9987 - val_loss: 0.0381 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 451/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9988\n","Epoch 451: val_loss did not improve from 0.03609\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0350 - accuracy: 0.9988 - val_loss: 0.0370 - val_accuracy: 0.9971 - lr: 1.0000e-04\n","Epoch 452/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9988\n","Epoch 452: val_loss did not improve from 0.03609\n","\n","Epoch 452: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0352 - accuracy: 0.9988 - val_loss: 0.0362 - val_accuracy: 0.9972 - lr: 1.0000e-04\n","Epoch 453/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9989\n","Epoch 453: val_loss improved from 0.03609 to 0.03543, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0359 - accuracy: 0.9989 - val_loss: 0.0354 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 454/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9990\n","Epoch 454: val_loss improved from 0.03543 to 0.03483, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0329 - accuracy: 0.9990 - val_loss: 0.0348 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 455/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9990\n","Epoch 455: val_loss improved from 0.03483 to 0.03459, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0347 - accuracy: 0.9990 - val_loss: 0.0346 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 456/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9989\n","Epoch 456: val_loss improved from 0.03459 to 0.03449, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.0355 - accuracy: 0.9989 - val_loss: 0.0345 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 457/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9990\n","Epoch 457: val_loss improved from 0.03449 to 0.03443, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 228ms/step - loss: 0.0340 - accuracy: 0.9990 - val_loss: 0.0344 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 458/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9990\n","Epoch 458: val_loss improved from 0.03443 to 0.03433, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.0341 - accuracy: 0.9990 - val_loss: 0.0343 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 459/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9990\n","Epoch 459: val_loss improved from 0.03433 to 0.03427, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.0342 - accuracy: 0.9990 - val_loss: 0.0343 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 460/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9990\n","Epoch 460: val_loss did not improve from 0.03427\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0347 - accuracy: 0.9990 - val_loss: 0.0343 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 461/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9990\n","Epoch 461: val_loss did not improve from 0.03427\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0355 - accuracy: 0.9990 - val_loss: 0.0343 - val_accuracy: 0.9973 - lr: 1.0000e-05\n","Epoch 462/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9991\n","Epoch 462: val_loss improved from 0.03427 to 0.03411, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0321 - accuracy: 0.9991 - val_loss: 0.0341 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 463/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9990\n","Epoch 463: val_loss improved from 0.03411 to 0.03404, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0327 - accuracy: 0.9990 - val_loss: 0.0340 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 464/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9990\n","Epoch 464: val_loss improved from 0.03404 to 0.03398, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0333 - accuracy: 0.9990 - val_loss: 0.0340 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 465/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9991\n","Epoch 465: val_loss did not improve from 0.03398\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0342 - accuracy: 0.9991 - val_loss: 0.0341 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 466/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9991\n","Epoch 466: val_loss did not improve from 0.03398\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0337 - accuracy: 0.9991 - val_loss: 0.0341 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 467/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9991\n","Epoch 467: val_loss improved from 0.03398 to 0.03394, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0327 - accuracy: 0.9991 - val_loss: 0.0339 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 468/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9991\n","Epoch 468: val_loss did not improve from 0.03394\n","56/56 [==============================] - 12s 216ms/step - loss: 0.0337 - accuracy: 0.9991 - val_loss: 0.0340 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 469/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9991\n","Epoch 469: val_loss improved from 0.03394 to 0.03382, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.0328 - accuracy: 0.9991 - val_loss: 0.0338 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 470/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9991\n","Epoch 470: val_loss did not improve from 0.03382\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0344 - accuracy: 0.9991 - val_loss: 0.0340 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 471/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9991\n","Epoch 471: val_loss did not improve from 0.03382\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0335 - accuracy: 0.9991 - val_loss: 0.0339 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 472/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9991\n","Epoch 472: val_loss did not improve from 0.03382\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0346 - accuracy: 0.9991 - val_loss: 0.0341 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 473/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9991\n","Epoch 473: val_loss did not improve from 0.03382\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0341 - accuracy: 0.9991 - val_loss: 0.0339 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 474/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9991\n","Epoch 474: val_loss did not improve from 0.03382\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0323 - accuracy: 0.9991 - val_loss: 0.0339 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 475/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9991\n","Epoch 475: val_loss did not improve from 0.03382\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0345 - accuracy: 0.9991 - val_loss: 0.0339 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 476/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9991\n","Epoch 476: val_loss improved from 0.03382 to 0.03376, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0323 - accuracy: 0.9991 - val_loss: 0.0338 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 477/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9991\n","Epoch 477: val_loss did not improve from 0.03376\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0340 - accuracy: 0.9991 - val_loss: 0.0339 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 478/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9992\n","Epoch 478: val_loss improved from 0.03376 to 0.03372, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0319 - accuracy: 0.9992 - val_loss: 0.0337 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 479/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9991\n","Epoch 479: val_loss did not improve from 0.03372\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0341 - accuracy: 0.9991 - val_loss: 0.0338 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 480/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9991\n","Epoch 480: val_loss did not improve from 0.03372\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0330 - accuracy: 0.9991 - val_loss: 0.0338 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 481/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9991\n","Epoch 481: val_loss did not improve from 0.03372\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0342 - accuracy: 0.9991 - val_loss: 0.0338 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 482/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9992\n","Epoch 482: val_loss improved from 0.03372 to 0.03371, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0324 - accuracy: 0.9992 - val_loss: 0.0337 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 483/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9991\n","Epoch 483: val_loss did not improve from 0.03371\n","56/56 [==============================] - 12s 221ms/step - loss: 0.0347 - accuracy: 0.9991 - val_loss: 0.0338 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 484/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9992\n","Epoch 484: val_loss did not improve from 0.03371\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0330 - accuracy: 0.9992 - val_loss: 0.0338 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 485/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9991\n","Epoch 485: val_loss improved from 0.03371 to 0.03366, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0343 - accuracy: 0.9991 - val_loss: 0.0337 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 486/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9992\n","Epoch 486: val_loss did not improve from 0.03366\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0344 - accuracy: 0.9992 - val_loss: 0.0338 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 487/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9991\n","Epoch 487: val_loss improved from 0.03366 to 0.03365, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0345 - accuracy: 0.9991 - val_loss: 0.0337 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 488/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9992\n","Epoch 488: val_loss did not improve from 0.03365\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0326 - accuracy: 0.9992 - val_loss: 0.0337 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 489/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9991\n","Epoch 489: val_loss improved from 0.03365 to 0.03352, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0327 - accuracy: 0.9991 - val_loss: 0.0335 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 490/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9992\n","Epoch 490: val_loss did not improve from 0.03352\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0326 - accuracy: 0.9992 - val_loss: 0.0336 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 491/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9992\n","Epoch 491: val_loss did not improve from 0.03352\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0328 - accuracy: 0.9992 - val_loss: 0.0335 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 492/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9992\n","Epoch 492: val_loss improved from 0.03352 to 0.03338, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0327 - accuracy: 0.9992 - val_loss: 0.0334 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 493/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9992\n","Epoch 493: val_loss improved from 0.03338 to 0.03327, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0315 - accuracy: 0.9992 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 494/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9991\n","Epoch 494: val_loss did not improve from 0.03327\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0354 - accuracy: 0.9991 - val_loss: 0.0337 - val_accuracy: 0.9974 - lr: 1.0000e-05\n","Epoch 495/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9992\n","Epoch 495: val_loss did not improve from 0.03327\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0334 - accuracy: 0.9992 - val_loss: 0.0334 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 496/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9992\n","Epoch 496: val_loss did not improve from 0.03327\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0343 - accuracy: 0.9992 - val_loss: 0.0335 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 497/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9992\n","Epoch 497: val_loss did not improve from 0.03327\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0330 - accuracy: 0.9992 - val_loss: 0.0336 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 498/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9992\n","Epoch 498: val_loss did not improve from 0.03327\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0351 - accuracy: 0.9992 - val_loss: 0.0335 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 499/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9992\n","Epoch 499: val_loss did not improve from 0.03327\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0330 - accuracy: 0.9992 - val_loss: 0.0336 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 500/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9992\n","Epoch 500: val_loss did not improve from 0.03327\n","56/56 [==============================] - 12s 220ms/step - loss: 0.0336 - accuracy: 0.9992 - val_loss: 0.0336 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 501/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9992\n","Epoch 501: val_loss did not improve from 0.03327\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0325 - accuracy: 0.9992 - val_loss: 0.0334 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 502/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9993\n","Epoch 502: val_loss did not improve from 0.03327\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0324 - accuracy: 0.9993 - val_loss: 0.0334 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 503/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9992\n","Epoch 503: val_loss did not improve from 0.03327\n","\n","Epoch 503: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0324 - accuracy: 0.9992 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 504/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9993\n","Epoch 504: val_loss improved from 0.03327 to 0.03326, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 230ms/step - loss: 0.0326 - accuracy: 0.9993 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 505/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9993\n","Epoch 505: val_loss improved from 0.03326 to 0.03315, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 225ms/step - loss: 0.0317 - accuracy: 0.9993 - val_loss: 0.0331 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 506/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9992\n","Epoch 506: val_loss did not improve from 0.03315\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0329 - accuracy: 0.9992 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 507/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9992\n","Epoch 507: val_loss did not improve from 0.03315\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0354 - accuracy: 0.9992 - val_loss: 0.0334 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 508/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9992\n","Epoch 508: val_loss did not improve from 0.03315\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0327 - accuracy: 0.9992 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 509/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9993\n","Epoch 509: val_loss did not improve from 0.03315\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0320 - accuracy: 0.9993 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 510/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9993\n","Epoch 510: val_loss improved from 0.03315 to 0.03312, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 226ms/step - loss: 0.0317 - accuracy: 0.9993 - val_loss: 0.0331 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 511/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9992\n","Epoch 511: val_loss did not improve from 0.03312\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0337 - accuracy: 0.9992 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 512/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9992\n","Epoch 512: val_loss did not improve from 0.03312\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0335 - accuracy: 0.9992 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 513/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9993\n","Epoch 513: val_loss did not improve from 0.03312\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0337 - accuracy: 0.9993 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 514/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9993\n","Epoch 514: val_loss did not improve from 0.03312\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0311 - accuracy: 0.9993 - val_loss: 0.0331 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 515/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9993\n","Epoch 515: val_loss did not improve from 0.03312\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0330 - accuracy: 0.9993 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 516/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9993\n","Epoch 516: val_loss did not improve from 0.03312\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0322 - accuracy: 0.9993 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 517/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9992\n","Epoch 517: val_loss did not improve from 0.03312\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0341 - accuracy: 0.9992 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 518/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9993\n","Epoch 518: val_loss did not improve from 0.03312\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0316 - accuracy: 0.9993 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 519/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9993\n","Epoch 519: val_loss improved from 0.03312 to 0.03309, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 229ms/step - loss: 0.0319 - accuracy: 0.9993 - val_loss: 0.0331 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 520/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9993\n","Epoch 520: val_loss did not improve from 0.03309\n","\n","Epoch 520: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0326 - accuracy: 0.9993 - val_loss: 0.0331 - val_accuracy: 0.9975 - lr: 1.0000e-06\n","Epoch 521/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9993\n","Epoch 521: val_loss did not improve from 0.03309\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0343 - accuracy: 0.9993 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-07\n","Epoch 522/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9992\n","Epoch 522: val_loss did not improve from 0.03309\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0331 - accuracy: 0.9992 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-07\n","Epoch 523/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9993\n","Epoch 523: val_loss did not improve from 0.03309\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0329 - accuracy: 0.9993 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-07\n","Epoch 524/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9993\n","Epoch 524: val_loss did not improve from 0.03309\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0323 - accuracy: 0.9993 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-07\n","Epoch 525/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9993\n","Epoch 525: val_loss did not improve from 0.03309\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0331 - accuracy: 0.9993 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-07\n","Epoch 526/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9993\n","Epoch 526: val_loss did not improve from 0.03309\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0340 - accuracy: 0.9993 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-07\n","Epoch 527/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9993\n","Epoch 527: val_loss did not improve from 0.03309\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0326 - accuracy: 0.9993 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-07\n","Epoch 528/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9992\n","Epoch 528: val_loss did not improve from 0.03309\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0334 - accuracy: 0.9992 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-07\n","Epoch 529/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9993\n","Epoch 529: val_loss did not improve from 0.03309\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0325 - accuracy: 0.9993 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-07\n","Epoch 530/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9992\n","Epoch 530: val_loss did not improve from 0.03309\n","\n","Epoch 530: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n","56/56 [==============================] - 12s 220ms/step - loss: 0.0332 - accuracy: 0.9992 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-07\n","Epoch 531/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9992\n","Epoch 531: val_loss did not improve from 0.03309\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0342 - accuracy: 0.9992 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 532/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9993\n","Epoch 532: val_loss did not improve from 0.03309\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0317 - accuracy: 0.9993 - val_loss: 0.0331 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 533/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9992\n","Epoch 533: val_loss improved from 0.03309 to 0.03305, saving model to unet_bce.hdf5\n","56/56 [==============================] - 13s 231ms/step - loss: 0.0321 - accuracy: 0.9992 - val_loss: 0.0330 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 534/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9992\n","Epoch 534: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0342 - accuracy: 0.9992 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 535/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9992\n","Epoch 535: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0345 - accuracy: 0.9992 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 536/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9993\n","Epoch 536: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0341 - accuracy: 0.9993 - val_loss: 0.0334 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 537/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9993\n","Epoch 537: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0333 - accuracy: 0.9993 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 538/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9993\n","Epoch 538: val_loss did not improve from 0.03305\n","56/56 [==============================] - 13s 224ms/step - loss: 0.0328 - accuracy: 0.9993 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 539/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9992\n","Epoch 539: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0343 - accuracy: 0.9992 - val_loss: 0.0334 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 540/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9993\n","Epoch 540: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0333 - accuracy: 0.9993 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 541/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9993\n","Epoch 541: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0328 - accuracy: 0.9993 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 542/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9993\n","Epoch 542: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0337 - accuracy: 0.9993 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 543/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9993\n","Epoch 543: val_loss did not improve from 0.03305\n","\n","Epoch 543: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n","56/56 [==============================] - 12s 219ms/step - loss: 0.0319 - accuracy: 0.9993 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-08\n","Epoch 544/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9993\n","Epoch 544: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 217ms/step - loss: 0.0327 - accuracy: 0.9993 - val_loss: 0.0332 - val_accuracy: 0.9975 - lr: 1.0000e-09\n","Epoch 545/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9991\n","Epoch 545: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0350 - accuracy: 0.9991 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-09\n","Epoch 546/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9993\n","Epoch 546: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 218ms/step - loss: 0.0337 - accuracy: 0.9993 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-09\n","Epoch 547/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9992\n","Epoch 547: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 222ms/step - loss: 0.0329 - accuracy: 0.9992 - val_loss: 0.0333 - val_accuracy: 0.9975 - lr: 1.0000e-09\n","Epoch 548/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9993\n","Epoch 548: val_loss did not improve from 0.03305\n","56/56 [==============================] - 12s 223ms/step - loss: 0.0308 - accuracy: 0.9993 - val_loss: 0.0331 - val_accuracy: 0.9975 - lr: 1.0000e-09\n","Epoch 548: early stopping\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f330e78a9d0>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Train model\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr=ReduceLROnPlateau(monitor='val_loss',\n","                         factor=0.1,\n","                         patience=10,\n","                         verbose=1,\n","                         mode='auto',\n","                         min_delta=0.00003,\n","                         cooldown=0,\n","                         min_lr=0)\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15,verbose=1,mode='min')\n","save_model= ModelCheckpoint('unet_bce.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","unet.fit(images, masks, validation_data=(val_images,val_masks), batch_size=16, epochs=1000,verbose=1,shuffle=True,callbacks=[save_model,reduce_lr,early_stop])"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:30:33.149536Z","iopub.status.busy":"2023-04-03T11:30:33.148613Z","iopub.status.idle":"2023-04-03T11:30:33.157327Z","shell.execute_reply":"2023-04-03T11:30:33.156110Z","shell.execute_reply.started":"2023-04-03T11:30:33.149500Z"},"trusted":true},"outputs":[],"source":["np.save('unet_bce-history.npy',unet.history.history)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:30:36.548905Z","iopub.status.busy":"2023-04-03T11:30:36.548407Z","iopub.status.idle":"2023-04-03T11:30:36.555632Z","shell.execute_reply":"2023-04-03T11:30:36.554312Z","shell.execute_reply.started":"2023-04-03T11:30:36.548866Z"},"trusted":true},"outputs":[],"source":["# model = load_model('daunet_325.hdf5',custom_objects={'bce_iou_hybrid_loss': bce_iou_hybrid_loss})\n","model_history = np.load('unet_bce-history.npy', allow_pickle='TRUE').item()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:30:40.257114Z","iopub.status.busy":"2023-04-03T11:30:40.256396Z","iopub.status.idle":"2023-04-03T11:30:41.312565Z","shell.execute_reply":"2023-04-03T11:30:41.311604Z","shell.execute_reply.started":"2023-04-03T11:30:40.257075Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<matplotlib.legend.Legend at 0x7f33090cd510>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#@title Plot accuracy and loss \n","from matplotlib import pyplot as plt\n","## Accuracy\n","plt.plot(model_history['accuracy'])\n","plt.plot(model_history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')\n","plt.show()\n","\n","## Loss\n","plt.plot(model_history['loss'])\n","plt.plot(model_history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:31:45.175636Z","iopub.status.busy":"2023-04-03T11:31:45.174590Z","iopub.status.idle":"2023-04-03T11:31:45.183800Z","shell.execute_reply":"2023-04-03T11:31:45.182696Z","shell.execute_reply.started":"2023-04-03T11:31:45.175595Z"},"trusted":true},"outputs":[],"source":["#@title Returns an image or array plot of mask prediction\n","\n","def reconstruct_image(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","  # Standardise between 0-1\n","  reconstruction = reconstruction/np.max(reconstruction)\n","\n","  # Round to 0-1, binary pixel-by-pixel classification \n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  # Plot reconstructed mask (prediction)\n","  plt.imshow(reconstruction) \n","'''\n","  Returns array of mask prediction, given model and image\n","'''\n","def reconstruct_array(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","\n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  return reconstruction # Returns array"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:31:49.146527Z","iopub.status.busy":"2023-04-03T11:31:49.146157Z","iopub.status.idle":"2023-04-03T11:31:49.172318Z","shell.execute_reply":"2023-04-03T11:31:49.170990Z","shell.execute_reply.started":"2023-04-03T11:31:49.146495Z"},"trusted":true},"outputs":[],"source":["#@title Metric functions for evaluation\n","def accuracy_eval(model, image, mask): # Gives score of mask vs prediction\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","    return accuracy_score(mask.flatten(), reconstruction)\n","\n","  else: # If a list of images input, find accuracy for each\n","    accuracy = []\n","    for i in range(len(image)):\n","      reconstruction = model.predict(image[i].reshape(1, 256, 256, 10))\n","      reconstruction = np.round(reconstruction).flatten()\n","      accuracy.append(accuracy_score(mask[i].flatten(), reconstruction))\n","    return accuracy\n","\n","def recall_eval(model, image, mask): # Find recall score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return recall_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    recall = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        recall.append(recall_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return recall\n","\n","def precision_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return precision_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    precision = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        precision.append(precision_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return precision\n","\n","def iou_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return jaccard_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    iou = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        iou.append(jaccard_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return iou\n","\n","def f1_score_eval_basic(precision, recall):\n","    prec = np.mean(precision)\n","    rec = np.mean(recall)\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def produce_mask(image): # Outputs rounded image (binary)\n","  return np.round(image)\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:31:53.508171Z","iopub.status.busy":"2023-04-03T11:31:53.507524Z","iopub.status.idle":"2023-04-03T11:33:25.543293Z","shell.execute_reply":"2023-04-03T11:33:25.542151Z","shell.execute_reply.started":"2023-04-03T11:31:53.508132Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 783ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 50ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 57ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 54ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 54ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 55ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 51ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n"]}],"source":["accuracy = (accuracy_eval(unet, test_images, test_masks))\n","precision = (precision_eval(unet, test_images, test_masks))\n","recall = (recall_eval(unet, test_images, test_masks))\n","iou = (iou_eval(unet, test_images, test_masks))"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:34:26.029303Z","iopub.status.busy":"2023-04-03T11:34:26.028935Z","iopub.status.idle":"2023-04-03T11:34:26.035087Z","shell.execute_reply":"2023-04-03T11:34:26.033839Z","shell.execute_reply.started":"2023-04-03T11:34:26.029272Z"},"trusted":true},"outputs":[],"source":["f1_score = (f1_score_eval_basic(precision, recall))"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:34:29.290814Z","iopub.status.busy":"2023-04-03T11:34:29.289789Z","iopub.status.idle":"2023-04-03T11:34:29.299499Z","shell.execute_reply":"2023-04-03T11:34:29.298170Z","shell.execute_reply.started":"2023-04-03T11:34:29.290774Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model accuracy:  0.98225328259002 0.0193894454217881\n","model precision:  0.9424929817805804 0.10842287133369483\n","model recall:  0.9784169042228712 0.03561059231296698\n","model F1-score:  0.9601190271388728\n","model iou:  0.9239982009541337\n"]}],"source":["\n","# Print score eval results for each model\n","print('model accuracy: ', np.mean(accuracy), np.std(accuracy))\n","# Print precision eval results for each model\n","print('model precision: ', np.mean(precision), np.std(precision))\n","# Print recall eval results for each model\n","print('model recall: ', np.mean(recall), np.std(recall))\n","# Print f1-score eval results for each model\n","print('model F1-score: ', np.mean(f1_score))\n","print('model iou: ', np.mean(iou))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
