{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:27:26.492902Z","iopub.status.busy":"2023-04-03T09:27:26.492633Z","iopub.status.idle":"2023-04-03T09:28:02.774325Z","shell.execute_reply":"2023-04-03T09:28:02.773301Z","shell.execute_reply.started":"2023-04-03T09:27:26.492875Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/davej23/image-segmentation-keras.git\n","  Cloning https://github.com/davej23/image-segmentation-keras.git to /tmp/pip-req-build-4w9y3fms\n","  Running command git clone --filter=blob:none --quiet https://github.com/davej23/image-segmentation-keras.git /tmp/pip-req-build-4w9y3fms\n","  Resolved https://github.com/davej23/image-segmentation-keras.git to commit e01b0a8d5859854cd9d259a618829889166439f5\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting rarfile\n","  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n","Collecting segmentation-models\n","  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n","Collecting rioxarray\n","  Downloading rioxarray-0.9.1.tar.gz (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting image-classifiers==1.0.0\n","  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n","Collecting keras-applications<=1.0.8,>=1.0.7\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting efficientnet==1.0.0\n","  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n","Collecting h5py<=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: Keras>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (2.11.0)\n","Collecting imageio==2.5.0\n","  Downloading imageio-2.5.0-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (0.4.0)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.5.4.60)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.64.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (1.21.6)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (9.4.0)\n","Requirement already satisfied: pyproj>=2.2 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (3.1.0)\n","Requirement already satisfied: xarray>=0.17 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (0.20.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from rioxarray) (23.0)\n","Requirement already satisfied: rasterio in /opt/conda/lib/python3.7/site-packages (from rioxarray) (1.2.10)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py<=2.10.0->keras-segmentation==0.3.0) (1.16.0)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.7.3)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (3.5.3)\n","Requirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.8.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from pyproj>=2.2->rioxarray) (2022.12.7)\n","Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.4.0)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.11.4)\n","Requirement already satisfied: pandas>=1.1 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (1.3.5)\n","Requirement already satisfied: click-plugins in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.1.1)\n","Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (8.1.3)\n","Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (22.2.0)\n","Requirement already satisfied: affine in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (2.4.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (59.8.0)\n","Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.4.7)\n","Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (0.7.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2022.7.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.11.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.0)\n","Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n","Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio->rioxarray) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->xarray>=0.17->rioxarray) (3.11.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (4.38.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (1.4.4)\n","Building wheels for collected packages: keras-segmentation, rioxarray\n","  Building wheel for keras-segmentation (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for keras-segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34377 sha256=2c3dd58e817adbe367e6203d0e19437180d40f98e50dca926e07820f7013a8d5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-5upayd0_/wheels/f4/fb/07/8f81ceb3d9fe936f5e4dcd1a64cbc489e42e6e7f9c2f166785\n","  Building wheel for rioxarray (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rioxarray: filename=rioxarray-0.9.1-py3-none-any.whl size=54590 sha256=5c6db3609ae9d92bf3d2fd936a362fefca264db54a30c33c09a8753cdc892639\n","  Stored in directory: /root/.cache/pip/wheels/03/b2/26/2e2cc1797ac99cc070d2cae87c340bd3429bbb583c90b1c780\n","Successfully built keras-segmentation rioxarray\n","Installing collected packages: rarfile, imageio, h5py, keras-applications, image-classifiers, efficientnet, segmentation-models, keras-segmentation, rioxarray\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.25.0\n","    Uninstalling imageio-2.25.0:\n","      Successfully uninstalled imageio-2.25.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n","tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed efficientnet-1.0.0 h5py-2.10.0 image-classifiers-1.0.0 imageio-2.5.0 keras-applications-1.0.8 keras-segmentation-0.3.0 rarfile-4.0 rioxarray-0.9.1 segmentation-models-1.0.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#@title import packages\n","import keras\n","import numpy as np\n","import os\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.losses import *\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint\n","import sys\n","\n","!pip install rarfile segmentation-models git+https://github.com/davej23/image-segmentation-keras.git rioxarray\n","from rarfile import RarFile\n","from sklearn.metrics import *\n","import rioxarray as rxr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:28:41.460666Z","iopub.status.busy":"2023-04-03T09:28:41.460286Z","iopub.status.idle":"2023-04-03T09:30:23.047350Z","shell.execute_reply":"2023-04-03T09:30:23.046302Z","shell.execute_reply.started":"2023-04-03T09:28:41.460633Z"},"trusted":true},"outputs":[],"source":["base_dir = r\"/content/gdrive/MyDrive/mudtrain/\"\n","#@title Read training images and normalise\n","training_images_list = os.listdir(r\"{}train/images/\".format(base_dir))\n","training_masks_list = []\n","training_images = []\n","for n in training_images_list:\n","  training_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}train/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  training_images.append(a)\n","\n","## Training masks\n","training_masks = []\n","for n in training_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}train/labels/{}\".format(base_dir,n))))\n","  training_masks.append(a)\n","\n","\n","## Validation images\n","validation_images_list = os.listdir(r\"{}val/images/\".format(base_dir))\n","validation_masks_list = []\n","validation_images = []\n","for n in validation_images_list:\n","  validation_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}val/images/{}\".format(base_dir,n))))\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  validation_images.append(a)\n","\n","## Validation masks\n","validation_masks = []\n","for n in validation_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}val/labels/{}\".format(base_dir,n))))\n","  validation_masks.append(a)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:28:28.526865Z","iopub.status.busy":"2023-04-03T11:28:28.526499Z","iopub.status.idle":"2023-04-03T11:28:53.044547Z","shell.execute_reply":"2023-04-03T11:28:53.043520Z","shell.execute_reply.started":"2023-04-03T11:28:28.526832Z"},"trusted":true},"outputs":[],"source":["## Test images\n","test_dir=r'/content/gdrive/MyDrive/mudtest/'\n","test_images_list = os.listdir(r\"{}images/\".format(test_dir))\n","test_masks_list = []\n","test_images = []\n","for n in test_images_list:\n","    test_masks_list.append(n)\n","    a = (np.array(rxr.open_rasterio(r\"{}/images/{}\".format(test_dir,n))))\n","    a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","    test_images.append(a)\n","            \n","## Test masks\n","test_masks = []\n","for n in test_masks_list:\n","    a = (np.array(rxr.open_rasterio(r\"{}/labels/{}\".format(test_dir,n))))\n","    test_masks.append(a)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:28:54.670184Z","iopub.status.busy":"2023-04-03T11:28:54.669780Z","iopub.status.idle":"2023-04-03T11:28:54.816296Z","shell.execute_reply":"2023-04-03T11:28:54.815215Z","shell.execute_reply.started":"2023-04-03T11:28:54.670147Z"},"trusted":true},"outputs":[],"source":["for i in range(len(test_images)):\n","  test_images[i] = test_images[i].astype('float32')\n","  test_images[i] = test_images[i].T\n","\n","for i in range(len(test_masks)):\n","  test_masks[i] = test_masks[i].reshape(1,256,256,1)\n","  test_masks[i] = test_masks[i].T\n","\n","for i in range(len(test_images)):\n","  test_images[i] = test_images[i].reshape(-1,256,256,10)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T00:19:22.998130Z","iopub.status.busy":"2023-03-28T00:19:22.997771Z","iopub.status.idle":"2023-03-28T00:19:23.003426Z","shell.execute_reply":"2023-03-28T00:19:23.002069Z","shell.execute_reply.started":"2023-03-28T00:19:22.998100Z"},"trusted":true},"outputs":[],"source":["del test_masks,test_images,test_masks_list,test_images_list"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:30:36.817247Z","iopub.status.busy":"2023-04-03T09:30:36.816721Z","iopub.status.idle":"2023-04-03T09:30:37.534909Z","shell.execute_reply":"2023-04-03T09:30:37.533899Z","shell.execute_reply.started":"2023-04-03T09:30:36.817206Z"},"trusted":true},"outputs":[],"source":["#@title Pre-process data, reshaping and transposing\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].astype('float32')\n","  training_images[i] = training_images[i].T\n","\n","for i in range(len(training_masks)):\n","  training_masks[i] = training_masks[i].reshape(1,256,256)\n","  training_masks[i] = training_masks[i].T\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].astype('float32')\n","  validation_images[i] = validation_images[i].T\n","\n","for i in range(len(validation_masks)):\n","  validation_masks[i] = validation_masks[i].reshape(1,256,256)\n","  validation_masks[i] = validation_masks[i].T\n","\n","\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].reshape(256,256,10)\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].reshape(256,256,10)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:30:40.275157Z","iopub.status.busy":"2023-04-03T09:30:40.274735Z","iopub.status.idle":"2023-04-03T09:30:45.418147Z","shell.execute_reply":"2023-04-03T09:30:45.416962Z","shell.execute_reply.started":"2023-04-03T09:30:40.275118Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 10)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["images=np.vstack([training_images])\n","val_images=np.vstack([validation_images])\n","images.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:30:45.420655Z","iopub.status.busy":"2023-04-03T09:30:45.420261Z","iopub.status.idle":"2023-04-03T09:30:45.576843Z","shell.execute_reply":"2023-04-03T09:30:45.575697Z","shell.execute_reply.started":"2023-04-03T09:30:45.420617Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 1)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["masks=np.vstack([training_masks])\n","val_masks=np.vstack([validation_masks])\n","val_masks.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:30:47.810124Z","iopub.status.busy":"2023-04-03T09:30:47.809521Z","iopub.status.idle":"2023-04-03T09:30:48.038388Z","shell.execute_reply":"2023-04-03T09:30:48.037309Z","shell.execute_reply.started":"2023-04-03T09:30:47.810085Z"},"trusted":true},"outputs":[{"data":{"text/plain":["889"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del training_images,validation_images,training_masks,validation_masks,\n","training_images_list,validation_images_list,\n","training_masks_list,validation_masks_list\n","gc.collect()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-02T23:44:46.545644Z","iopub.status.busy":"2023-04-02T23:44:46.544622Z","iopub.status.idle":"2023-04-02T23:44:46.550571Z","shell.execute_reply":"2023-04-02T23:44:46.549441Z","shell.execute_reply.started":"2023-04-02T23:44:46.545596Z"},"trusted":true},"outputs":[],"source":["del unet"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:28:13.851403Z","iopub.status.busy":"2023-04-03T11:28:13.850558Z","iopub.status.idle":"2023-04-03T11:28:13.864678Z","shell.execute_reply":"2023-04-03T11:28:13.863696Z","shell.execute_reply.started":"2023-04-03T11:28:13.851266Z"},"trusted":true},"outputs":[],"source":["del images,masks,val_images,val_masks"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:30:58.679100Z","iopub.status.busy":"2023-04-03T09:30:58.677913Z","iopub.status.idle":"2023-04-03T09:30:58.690778Z","shell.execute_reply":"2023-04-03T09:30:58.689778Z","shell.execute_reply.started":"2023-04-03T09:30:58.679031Z"},"trusted":true},"outputs":[],"source":["#@title boundary_loss\n","import numpy as np\n","import tensorflow as tf\n","tf.config.run_functions_eagerly(False)\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import models\n","\n","OUTPUT_SHAPE = (256, 256, 1)\n","\n","def boundary_loss(y_true, y_pred):\n","\n","    \"\"\"\n","    Paper Implemented : https://arxiv.org/abs/1905.07852\n","    Using Binary Segmentation mask, generates boundary mask on fly and claculates boundary loss.\n","    :param y_true:\n","    :param y_pred:\n","    :return:\n","    \"\"\"\n","    y_true=tf.cast(y_true,tf.float32)\n","    y_pred=tf.cast(y_pred,tf.float32)\n","    y_pred_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd = y_pred_bd - (1 - y_pred)\n","    y_true_bd = y_true_bd - (1 - y_true)\n","\n","    y_pred_bd_ext = layers.MaxPooling2D((5, 5), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd_ext = layers.MaxPooling2D((5, 5), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd_ext = y_pred_bd_ext - (1 - y_pred)\n","    y_true_bd_ext = y_true_bd_ext - (1 - y_true)\n","\n","    P = K.sum(y_pred_bd * y_true_bd_ext) / K.sum(y_pred_bd) + 1e-7\n","    R = K.sum(y_true_bd * y_pred_bd_ext) / K.sum(y_true_bd) + 1e-7\n","    F1_Score = 2 * P * R / (P + R + 1e-7)\n","    loss = K.mean(1 - F1_Score)\n","    return loss"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:31:42.099285Z","iopub.status.busy":"2023-04-03T09:31:42.098258Z","iopub.status.idle":"2023-04-03T09:31:44.370109Z","shell.execute_reply":"2023-04-03T09:31:44.369045Z","shell.execute_reply.started":"2023-04-03T09:31:42.099244Z"},"trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, Callback\n","\n","class AlphaScheduler(Callback):\n","  def init(self, alpha, update_fn):\n","    self.alpha = alpha\n","    self.update_fn = update_fn\n","  def on_epoch_end(self, epoch, logs=None):\n","    updated_alpha = self.update_fn(K.get_value(self.alpha))\n","\n","alpha = K.variable(1, dtype='float32')\n","\n","def update_alpha(value):\n","  return np.clip(value - 0.005, 0.005, 1)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:31:46.976111Z","iopub.status.busy":"2023-04-03T09:31:46.975716Z","iopub.status.idle":"2023-04-03T09:31:46.982095Z","shell.execute_reply":"2023-04-03T09:31:46.980648Z","shell.execute_reply.started":"2023-04-03T09:31:46.976075Z"},"trusted":true},"outputs":[],"source":["def gl_sl_wrapper(alpha):\n","    def gl_sl(y_true, y_pred):\n","        return alpha* binary_crossentropy(y_true, y_pred) +  (1-alpha)* boundary_loss(y_true, y_pred)\n","    return gl_sl"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:31:52.891957Z","iopub.status.busy":"2023-04-03T09:31:52.891596Z","iopub.status.idle":"2023-04-03T09:31:53.183651Z","shell.execute_reply":"2023-04-03T09:31:53.182693Z","shell.execute_reply.started":"2023-04-03T09:31:52.891924Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import models, layers, regularizers\n","from tensorflow.keras import backend as K\n","\n","#convolutional block\n","def conv_block(x, kernelsize, filters, dropout, batchnorm=True): \n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    if dropout > 0:\n","        conv = layers.Dropout(dropout)(conv)\n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(conv)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    return conv\n","\n","#Simple U-NET\n","def unetmodel(input_shape, dropout=0, batchnorm=True):    \n","    \n","    filters = [32,64, 128, 256,512]\n","    kernelsize = 3\n","    upsample_size = 2\n","    \n","    inputs = layers.Input(input_shape)    \n","\n","    # Downsampling layers\n","    dn_1 = conv_block(inputs, kernelsize, filters[0], dropout, batchnorm)\n","    pool_1 = layers.MaxPooling2D(pool_size=(2,2))(dn_1)\n","    \n","    dn_2 = conv_block(pool_1, kernelsize, filters[1], dropout, batchnorm)\n","    pool_2 = layers.MaxPooling2D(pool_size=(2,2))(dn_2)\n","    \n","    dn_3 = conv_block(pool_2, kernelsize, filters[2], dropout, batchnorm)\n","    pool_3 = layers.MaxPooling2D(pool_size=(2,2))(dn_3)\n","    \n","    dn_4 = conv_block(pool_3, kernelsize, filters[3], dropout, batchnorm)\n","    pool_4 = layers.MaxPooling2D(pool_size=(2,2))(dn_4)\n","    \n","    dn_5 = conv_block(pool_4, kernelsize, filters[4], dropout, batchnorm)\n","\n","    # Upsampling layers   \n","    up_5 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(dn_5)\n","    up_5 = layers.concatenate([up_5, dn_4], axis=3)\n","    up_conv_5 = conv_block(up_5, kernelsize, filters[3], dropout, batchnorm)\n","    \n","    up_4 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_5)\n","    up_4 = layers.concatenate([up_4, dn_3], axis=3)\n","    up_conv_4 = conv_block(up_4, kernelsize, filters[2], dropout, batchnorm)\n","       \n","    up_3 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_4)\n","    up_3 = layers.concatenate([up_3, dn_2], axis=3)\n","    up_conv_3 = conv_block(up_3, kernelsize, filters[1], dropout, batchnorm)\n","    \n","    up_2 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_3)\n","    up_2 = layers.concatenate([up_2, dn_1], axis=3)\n","    up_conv_2 = conv_block(up_2, kernelsize, filters[0], dropout, batchnorm)    \n","   \n","    conv_final = layers.Conv2D(1, kernel_size=(1,1))(up_conv_2)\n","    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n","    outputs = layers.Activation('sigmoid')(conv_final)  \n","\n","    model = models.Model(inputs=[inputs], outputs=[outputs])     \n","    return model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:31:59.379003Z","iopub.status.busy":"2023-04-03T09:31:59.377982Z","iopub.status.idle":"2023-04-03T09:32:00.044368Z","shell.execute_reply":"2023-04-03T09:32:00.043570Z","shell.execute_reply.started":"2023-04-03T09:31:59.378963Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 256, 256, 1  0           []                               \n","                                0)]                                                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 256, 256, 32  2912        ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 256, 256, 32  128        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," activation (Activation)        (None, 256, 256, 32  0           ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 256, 256, 32  9248        ['activation[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 256, 256, 32  128        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_1 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 128, 128, 32  0           ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 128, 128, 64  18496       ['max_pooling2d[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 128, 128, 64  256        ['conv2d_2[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_2 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_2[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 128, 128, 64  36928       ['activation_2[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 128, 128, 64  256        ['conv2d_3[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_3 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_3[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 64, 64, 128)  73856       ['max_pooling2d_1[0][0]']        \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_4[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_4 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 64, 64, 128)  147584      ['activation_4[0][0]']           \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_5[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_5 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0          ['activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 32, 32, 256)  295168      ['max_pooling2d_2[0][0]']        \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_6[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_6 (Activation)      (None, 32, 32, 256)  0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 32, 32, 256)  590080      ['activation_6[0][0]']           \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_7[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_7 (Activation)      (None, 32, 32, 256)  0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0          ['activation_7[0][0]']           \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 16, 16, 512)  1180160     ['max_pooling2d_3[0][0]']        \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_8[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_8 (Activation)      (None, 16, 16, 512)  0           ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 16, 16, 512)  2359808     ['activation_8[0][0]']           \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_9[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_9 (Activation)      (None, 16, 16, 512)  0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," up_sampling2d (UpSampling2D)   (None, 32, 32, 512)  0           ['activation_9[0][0]']           \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 32, 32, 768)  0           ['up_sampling2d[0][0]',          \n","                                                                  'activation_7[0][0]']           \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 32, 32, 256)  1769728     ['concatenate[0][0]']            \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_10[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_10 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_10[0][0]'] \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_10[0][0]']          \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_11[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_11 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0          ['activation_11[0][0]']          \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 64, 64, 384)  0           ['up_sampling2d_1[0][0]',        \n","                                                                  'activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 64, 64, 128)  442496      ['concatenate_1[0][0]']          \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 64, 64, 128)  512        ['conv2d_12[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_12 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_12[0][0]']          \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 64, 64, 128)  512        ['conv2d_13[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_13 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 12  0          ['activation_13[0][0]']          \n","                                8)                                                                \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 128, 128, 19  0           ['up_sampling2d_2[0][0]',        \n","                                2)                                'activation_3[0][0]']           \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 128, 128, 64  110656      ['concatenate_2[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 128, 128, 64  256        ['conv2d_14[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_14 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_14[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 128, 128, 64  36928       ['activation_14[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 128, 128, 64  256        ['conv2d_15[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_15 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_15[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64  0          ['activation_15[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 256, 256, 96  0           ['up_sampling2d_3[0][0]',        \n","                                )                                 'activation_1[0][0]']           \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 256, 256, 32  27680       ['concatenate_3[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 256, 256, 32  128        ['conv2d_16[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_16 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_16[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 256, 256, 32  9248        ['activation_16[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 256, 256, 32  128        ['conv2d_17[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_17 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_17[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 256, 256, 1)  33          ['activation_17[0][0]']          \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 256, 256, 1)  4          ['conv2d_18[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_18 (Activation)     (None, 256, 256, 1)  0           ['batch_normalization_18[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 7,860,453\n","Trainable params: 7,854,563\n","Non-trainable params: 5,890\n","__________________________________________________________________________________________________\n"]}],"source":["unet = unetmodel(input_shape=(256,256,10))\n","# gl_sl_wrapper(alpha)\n","unet.compile(optimizer = adam_v2.Adam(learning_rate = 1e-4), loss =gl_sl_wrapper(alpha), metrics = ['accuracy'])\n","unet.summary()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T09:32:19.129481Z","iopub.status.busy":"2023-04-03T09:32:19.128541Z","iopub.status.idle":"2023-04-03T11:25:49.265063Z","shell.execute_reply":"2023-04-03T11:25:49.264098Z","shell.execute_reply.started":"2023-04-03T09:32:19.129429Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.4384 - accuracy: 0.8949\n","Epoch 1: val_loss improved from inf to 0.60182, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 28s 338ms/step - loss: 0.4384 - accuracy: 0.8949 - val_loss: 0.6018 - val_accuracy: 0.7931 - lr: 1.0000e-04\n","Epoch 2/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3924 - accuracy: 0.9495\n","Epoch 2: val_loss improved from 0.60182 to 0.52604, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.3924 - accuracy: 0.9495 - val_loss: 0.5260 - val_accuracy: 0.8573 - lr: 1.0000e-04\n","Epoch 3/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3840 - accuracy: 0.9590\n","Epoch 3: val_loss did not improve from 0.52604\n","56/56 [==============================] - 14s 247ms/step - loss: 0.3840 - accuracy: 0.9590 - val_loss: 0.5622 - val_accuracy: 0.8110 - lr: 1.0000e-04\n","Epoch 4/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3761 - accuracy: 0.9658\n","Epoch 4: val_loss improved from 0.52604 to 0.52429, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 256ms/step - loss: 0.3761 - accuracy: 0.9658 - val_loss: 0.5243 - val_accuracy: 0.8603 - lr: 1.0000e-04\n","Epoch 5/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3685 - accuracy: 0.9707\n","Epoch 5: val_loss improved from 0.52429 to 0.50429, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.3685 - accuracy: 0.9707 - val_loss: 0.5043 - val_accuracy: 0.8808 - lr: 1.0000e-04\n","Epoch 6/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.9701\n","Epoch 6: val_loss improved from 0.50429 to 0.44445, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.3691 - accuracy: 0.9701 - val_loss: 0.4445 - val_accuracy: 0.8893 - lr: 1.0000e-04\n","Epoch 7/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3601 - accuracy: 0.9760\n","Epoch 7: val_loss did not improve from 0.44445\n","56/56 [==============================] - 14s 250ms/step - loss: 0.3601 - accuracy: 0.9760 - val_loss: 0.4753 - val_accuracy: 0.8894 - lr: 1.0000e-04\n","Epoch 8/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3631 - accuracy: 0.9721\n","Epoch 8: val_loss improved from 0.44445 to 0.36949, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.3631 - accuracy: 0.9721 - val_loss: 0.3695 - val_accuracy: 0.9798 - lr: 1.0000e-04\n","Epoch 9/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3555 - accuracy: 0.9774\n","Epoch 9: val_loss improved from 0.36949 to 0.35679, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.3555 - accuracy: 0.9774 - val_loss: 0.3568 - val_accuracy: 0.9802 - lr: 1.0000e-04\n","Epoch 10/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3528 - accuracy: 0.9777\n","Epoch 10: val_loss improved from 0.35679 to 0.35625, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.3528 - accuracy: 0.9777 - val_loss: 0.3563 - val_accuracy: 0.9770 - lr: 1.0000e-04\n","Epoch 11/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3519 - accuracy: 0.9782\n","Epoch 11: val_loss improved from 0.35625 to 0.34926, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 256ms/step - loss: 0.3519 - accuracy: 0.9782 - val_loss: 0.3493 - val_accuracy: 0.9738 - lr: 1.0000e-04\n","Epoch 12/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3505 - accuracy: 0.9773\n","Epoch 12: val_loss improved from 0.34926 to 0.34785, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.3505 - accuracy: 0.9773 - val_loss: 0.3478 - val_accuracy: 0.9805 - lr: 1.0000e-04\n","Epoch 13/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.9743\n","Epoch 13: val_loss improved from 0.34785 to 0.34768, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.3508 - accuracy: 0.9743 - val_loss: 0.3477 - val_accuracy: 0.9791 - lr: 1.0000e-04\n","Epoch 14/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3442 - accuracy: 0.9801\n","Epoch 14: val_loss improved from 0.34768 to 0.34229, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.3442 - accuracy: 0.9801 - val_loss: 0.3423 - val_accuracy: 0.9823 - lr: 1.0000e-04\n","Epoch 15/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3380 - accuracy: 0.9830\n","Epoch 15: val_loss did not improve from 0.34229\n","56/56 [==============================] - 14s 245ms/step - loss: 0.3380 - accuracy: 0.9830 - val_loss: 0.3526 - val_accuracy: 0.9852 - lr: 1.0000e-04\n","Epoch 16/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.9806\n","Epoch 16: val_loss improved from 0.34229 to 0.33423, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.3395 - accuracy: 0.9806 - val_loss: 0.3342 - val_accuracy: 0.9839 - lr: 1.0000e-04\n","Epoch 17/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3352 - accuracy: 0.9836\n","Epoch 17: val_loss did not improve from 0.33423\n","56/56 [==============================] - 14s 249ms/step - loss: 0.3352 - accuracy: 0.9836 - val_loss: 0.3374 - val_accuracy: 0.9847 - lr: 1.0000e-04\n","Epoch 18/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3335 - accuracy: 0.9833\n","Epoch 18: val_loss did not improve from 0.33423\n","56/56 [==============================] - 14s 250ms/step - loss: 0.3335 - accuracy: 0.9833 - val_loss: 0.3372 - val_accuracy: 0.9850 - lr: 1.0000e-04\n","Epoch 19/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3322 - accuracy: 0.9824\n","Epoch 19: val_loss improved from 0.33423 to 0.32767, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.3322 - accuracy: 0.9824 - val_loss: 0.3277 - val_accuracy: 0.9862 - lr: 1.0000e-04\n","Epoch 20/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3310 - accuracy: 0.9826\n","Epoch 20: val_loss did not improve from 0.32767\n","56/56 [==============================] - 14s 245ms/step - loss: 0.3310 - accuracy: 0.9826 - val_loss: 0.3366 - val_accuracy: 0.9861 - lr: 1.0000e-04\n","Epoch 21/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.9824\n","Epoch 21: val_loss improved from 0.32767 to 0.32250, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.3270 - accuracy: 0.9824 - val_loss: 0.3225 - val_accuracy: 0.9843 - lr: 1.0000e-04\n","Epoch 22/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.9826\n","Epoch 22: val_loss did not improve from 0.32250\n","56/56 [==============================] - 14s 245ms/step - loss: 0.3270 - accuracy: 0.9826 - val_loss: 0.3235 - val_accuracy: 0.9848 - lr: 1.0000e-04\n","Epoch 23/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3224 - accuracy: 0.9844\n","Epoch 23: val_loss improved from 0.32250 to 0.32000, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.3224 - accuracy: 0.9844 - val_loss: 0.3200 - val_accuracy: 0.9858 - lr: 1.0000e-04\n","Epoch 24/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3199 - accuracy: 0.9865\n","Epoch 24: val_loss did not improve from 0.32000\n","56/56 [==============================] - 14s 245ms/step - loss: 0.3199 - accuracy: 0.9865 - val_loss: 0.3202 - val_accuracy: 0.9854 - lr: 1.0000e-04\n","Epoch 25/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3198 - accuracy: 0.9852\n","Epoch 25: val_loss improved from 0.32000 to 0.31424, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.3198 - accuracy: 0.9852 - val_loss: 0.3142 - val_accuracy: 0.9870 - lr: 1.0000e-04\n","Epoch 26/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3152 - accuracy: 0.9876\n","Epoch 26: val_loss did not improve from 0.31424\n","56/56 [==============================] - 14s 249ms/step - loss: 0.3152 - accuracy: 0.9876 - val_loss: 0.3173 - val_accuracy: 0.9846 - lr: 1.0000e-04\n","Epoch 27/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3153 - accuracy: 0.9850\n","Epoch 27: val_loss did not improve from 0.31424\n","56/56 [==============================] - 14s 249ms/step - loss: 0.3153 - accuracy: 0.9850 - val_loss: 0.3169 - val_accuracy: 0.9879 - lr: 1.0000e-04\n","Epoch 28/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3123 - accuracy: 0.9867\n","Epoch 28: val_loss improved from 0.31424 to 0.31110, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 256ms/step - loss: 0.3123 - accuracy: 0.9867 - val_loss: 0.3111 - val_accuracy: 0.9861 - lr: 1.0000e-04\n","Epoch 29/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.9864\n","Epoch 29: val_loss did not improve from 0.31110\n","56/56 [==============================] - 14s 249ms/step - loss: 0.3111 - accuracy: 0.9864 - val_loss: 0.3114 - val_accuracy: 0.9874 - lr: 1.0000e-04\n","Epoch 30/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 0.9860\n","Epoch 30: val_loss did not improve from 0.31110\n","56/56 [==============================] - 14s 251ms/step - loss: 0.3117 - accuracy: 0.9860 - val_loss: 0.3195 - val_accuracy: 0.9839 - lr: 1.0000e-04\n","Epoch 31/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3064 - accuracy: 0.9865\n","Epoch 31: val_loss improved from 0.31110 to 0.30889, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 256ms/step - loss: 0.3064 - accuracy: 0.9865 - val_loss: 0.3089 - val_accuracy: 0.9832 - lr: 1.0000e-04\n","Epoch 32/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3044 - accuracy: 0.9879\n","Epoch 32: val_loss improved from 0.30889 to 0.30700, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.3044 - accuracy: 0.9879 - val_loss: 0.3070 - val_accuracy: 0.9870 - lr: 1.0000e-04\n","Epoch 33/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3028 - accuracy: 0.9869\n","Epoch 33: val_loss did not improve from 0.30700\n","56/56 [==============================] - 14s 250ms/step - loss: 0.3028 - accuracy: 0.9869 - val_loss: 0.3418 - val_accuracy: 0.9206 - lr: 1.0000e-04\n","Epoch 34/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3028 - accuracy: 0.9845\n","Epoch 34: val_loss improved from 0.30700 to 0.30406, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.3028 - accuracy: 0.9845 - val_loss: 0.3041 - val_accuracy: 0.9887 - lr: 1.0000e-04\n","Epoch 35/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3005 - accuracy: 0.9859\n","Epoch 35: val_loss did not improve from 0.30406\n","56/56 [==============================] - 14s 249ms/step - loss: 0.3005 - accuracy: 0.9859 - val_loss: 0.3134 - val_accuracy: 0.9828 - lr: 1.0000e-04\n","Epoch 36/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3013 - accuracy: 0.9852\n","Epoch 36: val_loss improved from 0.30406 to 0.30098, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.3013 - accuracy: 0.9852 - val_loss: 0.3010 - val_accuracy: 0.9871 - lr: 1.0000e-04\n","Epoch 37/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.9869\n","Epoch 37: val_loss improved from 0.30098 to 0.29757, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 256ms/step - loss: 0.2974 - accuracy: 0.9869 - val_loss: 0.2976 - val_accuracy: 0.9872 - lr: 1.0000e-04\n","Epoch 38/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2946 - accuracy: 0.9864\n","Epoch 38: val_loss improved from 0.29757 to 0.29315, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.2946 - accuracy: 0.9864 - val_loss: 0.2931 - val_accuracy: 0.9865 - lr: 1.0000e-04\n","Epoch 39/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2935 - accuracy: 0.9859\n","Epoch 39: val_loss improved from 0.29315 to 0.29079, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.2935 - accuracy: 0.9859 - val_loss: 0.2908 - val_accuracy: 0.9876 - lr: 1.0000e-04\n","Epoch 40/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9875\n","Epoch 40: val_loss did not improve from 0.29079\n","56/56 [==============================] - 14s 245ms/step - loss: 0.2903 - accuracy: 0.9875 - val_loss: 0.3103 - val_accuracy: 0.9774 - lr: 1.0000e-04\n","Epoch 41/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.9878\n","Epoch 41: val_loss did not improve from 0.29079\n","56/56 [==============================] - 14s 246ms/step - loss: 0.2884 - accuracy: 0.9878 - val_loss: 0.2918 - val_accuracy: 0.9847 - lr: 1.0000e-04\n","Epoch 42/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9892\n","Epoch 42: val_loss did not improve from 0.29079\n","56/56 [==============================] - 14s 249ms/step - loss: 0.2845 - accuracy: 0.9892 - val_loss: 0.2942 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 43/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2853 - accuracy: 0.9876\n","Epoch 43: val_loss did not improve from 0.29079\n","56/56 [==============================] - 14s 246ms/step - loss: 0.2853 - accuracy: 0.9876 - val_loss: 0.2941 - val_accuracy: 0.9867 - lr: 1.0000e-04\n","Epoch 44/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2823 - accuracy: 0.9890\n","Epoch 44: val_loss improved from 0.29079 to 0.28107, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.2823 - accuracy: 0.9890 - val_loss: 0.2811 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 45/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.9889\n","Epoch 45: val_loss improved from 0.28107 to 0.28001, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.2820 - accuracy: 0.9889 - val_loss: 0.2800 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 46/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2776 - accuracy: 0.9900\n","Epoch 46: val_loss did not improve from 0.28001\n","56/56 [==============================] - 14s 249ms/step - loss: 0.2776 - accuracy: 0.9900 - val_loss: 0.2802 - val_accuracy: 0.9887 - lr: 1.0000e-04\n","Epoch 47/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2780 - accuracy: 0.9900\n","Epoch 47: val_loss improved from 0.28001 to 0.27844, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.2780 - accuracy: 0.9900 - val_loss: 0.2784 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 48/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.9901\n","Epoch 48: val_loss did not improve from 0.27844\n","56/56 [==============================] - 14s 246ms/step - loss: 0.2769 - accuracy: 0.9901 - val_loss: 0.2794 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 49/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2773 - accuracy: 0.9907\n","Epoch 49: val_loss improved from 0.27844 to 0.27673, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 256ms/step - loss: 0.2773 - accuracy: 0.9907 - val_loss: 0.2767 - val_accuracy: 0.9878 - lr: 1.0000e-04\n","Epoch 50/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2725 - accuracy: 0.9903\n","Epoch 50: val_loss improved from 0.27673 to 0.27293, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.2725 - accuracy: 0.9903 - val_loss: 0.2729 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 51/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9900\n","Epoch 51: val_loss improved from 0.27293 to 0.27037, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.2709 - accuracy: 0.9900 - val_loss: 0.2704 - val_accuracy: 0.9886 - lr: 1.0000e-04\n","Epoch 52/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.9902\n","Epoch 52: val_loss improved from 0.27037 to 0.26865, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.2685 - accuracy: 0.9902 - val_loss: 0.2687 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 53/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.9885\n","Epoch 53: val_loss did not improve from 0.26865\n","56/56 [==============================] - 14s 249ms/step - loss: 0.2680 - accuracy: 0.9885 - val_loss: 0.2717 - val_accuracy: 0.9883 - lr: 1.0000e-04\n","Epoch 54/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2674 - accuracy: 0.9883\n","Epoch 54: val_loss improved from 0.26865 to 0.26629, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.2674 - accuracy: 0.9883 - val_loss: 0.2663 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 55/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.9872\n","Epoch 55: val_loss did not improve from 0.26629\n","56/56 [==============================] - 14s 249ms/step - loss: 0.2673 - accuracy: 0.9872 - val_loss: 0.2668 - val_accuracy: 0.9841 - lr: 1.0000e-04\n","Epoch 56/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.9860\n","Epoch 56: val_loss did not improve from 0.26629\n","56/56 [==============================] - 14s 250ms/step - loss: 0.2673 - accuracy: 0.9860 - val_loss: 0.2689 - val_accuracy: 0.9858 - lr: 1.0000e-04\n","Epoch 57/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9880\n","Epoch 57: val_loss did not improve from 0.26629\n","56/56 [==============================] - 14s 251ms/step - loss: 0.2647 - accuracy: 0.9880 - val_loss: 0.2737 - val_accuracy: 0.9882 - lr: 1.0000e-04\n","Epoch 58/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 0.9901\n","Epoch 58: val_loss did not improve from 0.26629\n","56/56 [==============================] - 14s 245ms/step - loss: 0.2597 - accuracy: 0.9901 - val_loss: 0.2744 - val_accuracy: 0.9885 - lr: 1.0000e-04\n","Epoch 59/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9900\n","Epoch 59: val_loss did not improve from 0.26629\n","56/56 [==============================] - 14s 251ms/step - loss: 0.2579 - accuracy: 0.9900 - val_loss: 0.2725 - val_accuracy: 0.9893 - lr: 1.0000e-04\n","Epoch 60/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.9908\n","Epoch 60: val_loss improved from 0.26629 to 0.26173, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.2571 - accuracy: 0.9908 - val_loss: 0.2617 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 61/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.9899\n","Epoch 61: val_loss did not improve from 0.26173\n","56/56 [==============================] - 14s 246ms/step - loss: 0.2552 - accuracy: 0.9899 - val_loss: 0.2711 - val_accuracy: 0.9857 - lr: 1.0000e-04\n","Epoch 62/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9879\n","Epoch 62: val_loss improved from 0.26173 to 0.25795, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.2569 - accuracy: 0.9879 - val_loss: 0.2580 - val_accuracy: 0.9893 - lr: 1.0000e-04\n","Epoch 63/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.9882\n","Epoch 63: val_loss did not improve from 0.25795\n","56/56 [==============================] - 14s 250ms/step - loss: 0.2557 - accuracy: 0.9882 - val_loss: 0.3340 - val_accuracy: 0.9212 - lr: 1.0000e-04\n","Epoch 64/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9894\n","Epoch 64: val_loss improved from 0.25795 to 0.24643, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.2506 - accuracy: 0.9894 - val_loss: 0.2464 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 65/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9903\n","Epoch 65: val_loss did not improve from 0.24643\n","56/56 [==============================] - 14s 246ms/step - loss: 0.2491 - accuracy: 0.9903 - val_loss: 0.2489 - val_accuracy: 0.9886 - lr: 1.0000e-04\n","Epoch 66/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9894\n","Epoch 66: val_loss improved from 0.24643 to 0.24638, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.2487 - accuracy: 0.9894 - val_loss: 0.2464 - val_accuracy: 0.9894 - lr: 1.0000e-04\n","Epoch 67/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.9897\n","Epoch 67: val_loss improved from 0.24638 to 0.24619, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.2490 - accuracy: 0.9897 - val_loss: 0.2462 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 68/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.9904\n","Epoch 68: val_loss improved from 0.24619 to 0.24403, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.2454 - accuracy: 0.9904 - val_loss: 0.2440 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 69/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.9916\n","Epoch 69: val_loss improved from 0.24403 to 0.24049, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.2433 - accuracy: 0.9916 - val_loss: 0.2405 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 70/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9919\n","Epoch 70: val_loss did not improve from 0.24049\n","56/56 [==============================] - 14s 250ms/step - loss: 0.2448 - accuracy: 0.9919 - val_loss: 0.2421 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 71/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.9919\n","Epoch 71: val_loss did not improve from 0.24049\n","56/56 [==============================] - 14s 245ms/step - loss: 0.2413 - accuracy: 0.9919 - val_loss: 0.2408 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 72/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.9916\n","Epoch 72: val_loss improved from 0.24049 to 0.23950, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.2381 - accuracy: 0.9916 - val_loss: 0.2395 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 73/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2374 - accuracy: 0.9920\n","Epoch 73: val_loss improved from 0.23950 to 0.23694, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 256ms/step - loss: 0.2374 - accuracy: 0.9920 - val_loss: 0.2369 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 74/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9921\n","Epoch 74: val_loss did not improve from 0.23694\n","56/56 [==============================] - 14s 247ms/step - loss: 0.2369 - accuracy: 0.9921 - val_loss: 0.2371 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 75/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.9925\n","Epoch 75: val_loss improved from 0.23694 to 0.23509, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 251ms/step - loss: 0.2333 - accuracy: 0.9925 - val_loss: 0.2351 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 76/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2360 - accuracy: 0.9891\n","Epoch 76: val_loss improved from 0.23509 to 0.23075, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 251ms/step - loss: 0.2360 - accuracy: 0.9891 - val_loss: 0.2308 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 77/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2350 - accuracy: 0.9889\n","Epoch 77: val_loss did not improve from 0.23075\n","56/56 [==============================] - 14s 246ms/step - loss: 0.2350 - accuracy: 0.9889 - val_loss: 0.2308 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 78/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.9919\n","Epoch 78: val_loss improved from 0.23075 to 0.22969, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.2288 - accuracy: 0.9919 - val_loss: 0.2297 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 79/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2298 - accuracy: 0.9916\n","Epoch 79: val_loss did not improve from 0.22969\n","56/56 [==============================] - 14s 250ms/step - loss: 0.2298 - accuracy: 0.9916 - val_loss: 0.2313 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 80/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2325 - accuracy: 0.9915\n","Epoch 80: val_loss improved from 0.22969 to 0.22867, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 256ms/step - loss: 0.2325 - accuracy: 0.9915 - val_loss: 0.2287 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 81/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2296 - accuracy: 0.9892\n","Epoch 81: val_loss improved from 0.22867 to 0.21950, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.2296 - accuracy: 0.9892 - val_loss: 0.2195 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 82/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2285 - accuracy: 0.9911\n","Epoch 82: val_loss did not improve from 0.21950\n","56/56 [==============================] - 14s 245ms/step - loss: 0.2285 - accuracy: 0.9911 - val_loss: 0.2398 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 83/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2265 - accuracy: 0.9918\n","Epoch 83: val_loss did not improve from 0.21950\n","56/56 [==============================] - 14s 250ms/step - loss: 0.2265 - accuracy: 0.9918 - val_loss: 0.2325 - val_accuracy: 0.9880 - lr: 1.0000e-04\n","Epoch 84/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9918\n","Epoch 84: val_loss did not improve from 0.21950\n","56/56 [==============================] - 14s 244ms/step - loss: 0.2236 - accuracy: 0.9918 - val_loss: 0.2282 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 85/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2201 - accuracy: 0.9928\n","Epoch 85: val_loss did not improve from 0.21950\n","56/56 [==============================] - 14s 245ms/step - loss: 0.2201 - accuracy: 0.9928 - val_loss: 0.2223 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 86/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2239 - accuracy: 0.9922\n","Epoch 86: val_loss did not improve from 0.21950\n","56/56 [==============================] - 14s 252ms/step - loss: 0.2239 - accuracy: 0.9922 - val_loss: 0.2207 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 87/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9922\n","Epoch 87: val_loss did not improve from 0.21950\n","56/56 [==============================] - 14s 245ms/step - loss: 0.2204 - accuracy: 0.9922 - val_loss: 0.2233 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 88/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9918\n","Epoch 88: val_loss improved from 0.21950 to 0.21900, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.2197 - accuracy: 0.9918 - val_loss: 0.2190 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 89/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9930\n","Epoch 89: val_loss improved from 0.21900 to 0.21603, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.2158 - accuracy: 0.9930 - val_loss: 0.2160 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 90/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2122 - accuracy: 0.9934\n","Epoch 90: val_loss improved from 0.21603 to 0.21479, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.2122 - accuracy: 0.9934 - val_loss: 0.2148 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 91/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9927\n","Epoch 91: val_loss improved from 0.21479 to 0.21298, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 256ms/step - loss: 0.2158 - accuracy: 0.9927 - val_loss: 0.2130 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 92/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9931\n","Epoch 92: val_loss improved from 0.21298 to 0.21259, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.2125 - accuracy: 0.9931 - val_loss: 0.2126 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 93/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9922\n","Epoch 93: val_loss did not improve from 0.21259\n","56/56 [==============================] - 14s 249ms/step - loss: 0.2134 - accuracy: 0.9922 - val_loss: 0.2164 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 94/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2127 - accuracy: 0.9905\n","Epoch 94: val_loss did not improve from 0.21259\n","56/56 [==============================] - 14s 249ms/step - loss: 0.2127 - accuracy: 0.9905 - val_loss: 0.2647 - val_accuracy: 0.9212 - lr: 1.0000e-04\n","Epoch 95/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9912\n","Epoch 95: val_loss did not improve from 0.21259\n","56/56 [==============================] - 14s 247ms/step - loss: 0.2125 - accuracy: 0.9912 - val_loss: 0.2173 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 96/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2159 - accuracy: 0.9868\n","Epoch 96: val_loss did not improve from 0.21259\n","56/56 [==============================] - 14s 249ms/step - loss: 0.2159 - accuracy: 0.9868 - val_loss: 0.2465 - val_accuracy: 0.9634 - lr: 1.0000e-04\n","Epoch 97/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2152 - accuracy: 0.9860\n","Epoch 97: val_loss improved from 0.21259 to 0.19946, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.2152 - accuracy: 0.9860 - val_loss: 0.1995 - val_accuracy: 0.9882 - lr: 1.0000e-04\n","Epoch 98/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2084 - accuracy: 0.9907\n","Epoch 98: val_loss did not improve from 0.19946\n","56/56 [==============================] - 14s 245ms/step - loss: 0.2084 - accuracy: 0.9907 - val_loss: 0.2054 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 99/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9922\n","Epoch 99: val_loss did not improve from 0.19946\n","56/56 [==============================] - 14s 251ms/step - loss: 0.2064 - accuracy: 0.9922 - val_loss: 0.2077 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 100/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2037 - accuracy: 0.9926\n","Epoch 100: val_loss did not improve from 0.19946\n","56/56 [==============================] - 14s 249ms/step - loss: 0.2037 - accuracy: 0.9926 - val_loss: 0.2066 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 101/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.9928\n","Epoch 101: val_loss did not improve from 0.19946\n","56/56 [==============================] - 14s 246ms/step - loss: 0.2029 - accuracy: 0.9928 - val_loss: 0.2018 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 102/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2004 - accuracy: 0.9931\n","Epoch 102: val_loss did not improve from 0.19946\n","56/56 [==============================] - 14s 249ms/step - loss: 0.2004 - accuracy: 0.9931 - val_loss: 0.2021 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 103/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2026 - accuracy: 0.9921\n","Epoch 103: val_loss improved from 0.19946 to 0.19908, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.2026 - accuracy: 0.9921 - val_loss: 0.1991 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 104/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9932\n","Epoch 104: val_loss did not improve from 0.19908\n","56/56 [==============================] - 14s 252ms/step - loss: 0.2005 - accuracy: 0.9932 - val_loss: 0.2007 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 105/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.9935\n","Epoch 105: val_loss improved from 0.19908 to 0.19725, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.1972 - accuracy: 0.9935 - val_loss: 0.1972 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 106/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1983 - accuracy: 0.9936\n","Epoch 106: val_loss did not improve from 0.19725\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1983 - accuracy: 0.9936 - val_loss: 0.1986 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 107/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9931\n","Epoch 107: val_loss did not improve from 0.19725\n","56/56 [==============================] - 14s 245ms/step - loss: 0.1942 - accuracy: 0.9931 - val_loss: 0.2500 - val_accuracy: 0.9266 - lr: 1.0000e-04\n","Epoch 108/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9932\n","Epoch 108: val_loss improved from 0.19725 to 0.19374, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.1942 - accuracy: 0.9932 - val_loss: 0.1937 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 109/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1910 - accuracy: 0.9937\n","Epoch 109: val_loss improved from 0.19374 to 0.19300, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.1910 - accuracy: 0.9937 - val_loss: 0.1930 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 110/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1913 - accuracy: 0.9933\n","Epoch 110: val_loss improved from 0.19300 to 0.19184, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1913 - accuracy: 0.9933 - val_loss: 0.1918 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 111/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.9938\n","Epoch 111: val_loss improved from 0.19184 to 0.19044, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1904 - accuracy: 0.9938 - val_loss: 0.1904 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 112/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.9940\n","Epoch 112: val_loss improved from 0.19044 to 0.18961, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 15s 260ms/step - loss: 0.1883 - accuracy: 0.9940 - val_loss: 0.1896 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 113/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9938\n","Epoch 113: val_loss did not improve from 0.18961\n","56/56 [==============================] - 14s 246ms/step - loss: 0.1896 - accuracy: 0.9938 - val_loss: 0.1897 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 114/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1871 - accuracy: 0.9937\n","Epoch 114: val_loss improved from 0.18961 to 0.18703, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.1871 - accuracy: 0.9937 - val_loss: 0.1870 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 115/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9940\n","Epoch 115: val_loss improved from 0.18703 to 0.18363, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.1856 - accuracy: 0.9940 - val_loss: 0.1836 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 116/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.9939\n","Epoch 116: val_loss did not improve from 0.18363\n","56/56 [==============================] - 14s 249ms/step - loss: 0.1868 - accuracy: 0.9939 - val_loss: 0.1853 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 117/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1842 - accuracy: 0.9939\n","Epoch 117: val_loss improved from 0.18363 to 0.18260, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1842 - accuracy: 0.9939 - val_loss: 0.1826 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 118/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9941\n","Epoch 118: val_loss improved from 0.18260 to 0.18095, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1852 - accuracy: 0.9941 - val_loss: 0.1810 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 119/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1828 - accuracy: 0.9940\n","Epoch 119: val_loss did not improve from 0.18095\n","56/56 [==============================] - 14s 251ms/step - loss: 0.1828 - accuracy: 0.9940 - val_loss: 0.1813 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 120/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9946\n","Epoch 120: val_loss improved from 0.18095 to 0.17866, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.1820 - accuracy: 0.9946 - val_loss: 0.1787 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 121/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1826 - accuracy: 0.9942\n","Epoch 121: val_loss improved from 0.17866 to 0.17762, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.1826 - accuracy: 0.9942 - val_loss: 0.1776 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 122/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9941\n","Epoch 122: val_loss did not improve from 0.17762\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1799 - accuracy: 0.9941 - val_loss: 0.1798 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 123/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9942\n","Epoch 123: val_loss did not improve from 0.17762\n","56/56 [==============================] - 14s 245ms/step - loss: 0.1799 - accuracy: 0.9942 - val_loss: 0.3006 - val_accuracy: 0.9114 - lr: 1.0000e-04\n","Epoch 124/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.9941\n","Epoch 124: val_loss improved from 0.17762 to 0.17611, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.1785 - accuracy: 0.9941 - val_loss: 0.1761 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 125/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9942\n","Epoch 125: val_loss improved from 0.17611 to 0.17444, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.1777 - accuracy: 0.9942 - val_loss: 0.1744 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 126/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9943\n","Epoch 126: val_loss improved from 0.17444 to 0.17348, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.1777 - accuracy: 0.9943 - val_loss: 0.1735 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 127/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9945\n","Epoch 127: val_loss improved from 0.17348 to 0.17142, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.1744 - accuracy: 0.9945 - val_loss: 0.1714 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 128/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9948\n","Epoch 128: val_loss did not improve from 0.17142\n","56/56 [==============================] - 14s 252ms/step - loss: 0.1712 - accuracy: 0.9948 - val_loss: 0.1736 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 129/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1727 - accuracy: 0.9947\n","Epoch 129: val_loss improved from 0.17142 to 0.17006, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.1727 - accuracy: 0.9947 - val_loss: 0.1701 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 130/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.9947\n","Epoch 130: val_loss improved from 0.17006 to 0.16865, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1714 - accuracy: 0.9947 - val_loss: 0.1686 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 131/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9949\n","Epoch 131: val_loss improved from 0.16865 to 0.16789, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1702 - accuracy: 0.9949 - val_loss: 0.1679 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 132/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9949\n","Epoch 132: val_loss improved from 0.16789 to 0.16707, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 15s 261ms/step - loss: 0.1691 - accuracy: 0.9949 - val_loss: 0.1671 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 133/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1676 - accuracy: 0.9949\n","Epoch 133: val_loss improved from 0.16707 to 0.16621, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1676 - accuracy: 0.9949 - val_loss: 0.1662 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 134/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9949\n","Epoch 134: val_loss improved from 0.16621 to 0.16432, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.1672 - accuracy: 0.9949 - val_loss: 0.1643 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 135/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1658 - accuracy: 0.9949\n","Epoch 135: val_loss improved from 0.16432 to 0.16382, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 15s 260ms/step - loss: 0.1658 - accuracy: 0.9949 - val_loss: 0.1638 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 136/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1676 - accuracy: 0.9947\n","Epoch 136: val_loss improved from 0.16382 to 0.16254, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1676 - accuracy: 0.9947 - val_loss: 0.1625 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 137/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9951\n","Epoch 137: val_loss did not improve from 0.16254\n","56/56 [==============================] - 14s 252ms/step - loss: 0.1641 - accuracy: 0.9951 - val_loss: 0.1627 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 138/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1648 - accuracy: 0.9950\n","Epoch 138: val_loss improved from 0.16254 to 0.16038, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.1648 - accuracy: 0.9950 - val_loss: 0.1604 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 139/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9946\n","Epoch 139: val_loss improved from 0.16038 to 0.15965, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.1632 - accuracy: 0.9946 - val_loss: 0.1596 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 140/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9951\n","Epoch 140: val_loss did not improve from 0.15965\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1624 - accuracy: 0.9951 - val_loss: 0.1599 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 141/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9950\n","Epoch 141: val_loss improved from 0.15965 to 0.15735, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.1621 - accuracy: 0.9950 - val_loss: 0.1574 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 142/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1600 - accuracy: 0.9950\n","Epoch 142: val_loss did not improve from 0.15735\n","56/56 [==============================] - 14s 251ms/step - loss: 0.1600 - accuracy: 0.9950 - val_loss: 0.1598 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 143/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1576 - accuracy: 0.9951\n","Epoch 143: val_loss improved from 0.15735 to 0.15584, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 15s 260ms/step - loss: 0.1576 - accuracy: 0.9951 - val_loss: 0.1558 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 144/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9951\n","Epoch 144: val_loss improved from 0.15584 to 0.15565, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1571 - accuracy: 0.9951 - val_loss: 0.1556 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 145/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9950\n","Epoch 145: val_loss improved from 0.15565 to 0.15425, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.1571 - accuracy: 0.9950 - val_loss: 0.1542 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 146/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1564 - accuracy: 0.9950\n","Epoch 146: val_loss improved from 0.15425 to 0.15271, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.1564 - accuracy: 0.9950 - val_loss: 0.1527 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 147/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9951\n","Epoch 147: val_loss improved from 0.15271 to 0.15252, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.1579 - accuracy: 0.9951 - val_loss: 0.1525 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 148/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9953\n","Epoch 148: val_loss improved from 0.15252 to 0.15180, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.1529 - accuracy: 0.9953 - val_loss: 0.1518 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 149/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1555 - accuracy: 0.9954\n","Epoch 149: val_loss improved from 0.15180 to 0.15092, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.1555 - accuracy: 0.9954 - val_loss: 0.1509 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 150/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.9955\n","Epoch 150: val_loss did not improve from 0.15092\n","56/56 [==============================] - 14s 247ms/step - loss: 0.1525 - accuracy: 0.9955 - val_loss: 0.1509 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 151/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1532 - accuracy: 0.9949\n","Epoch 151: val_loss did not improve from 0.15092\n","56/56 [==============================] - 14s 245ms/step - loss: 0.1532 - accuracy: 0.9949 - val_loss: 0.1535 - val_accuracy: 0.9863 - lr: 1.0000e-04\n","Epoch 152/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.9834\n","Epoch 152: val_loss did not improve from 0.15092\n","56/56 [==============================] - 14s 251ms/step - loss: 0.1699 - accuracy: 0.9834 - val_loss: 0.2883 - val_accuracy: 0.8944 - lr: 1.0000e-04\n","Epoch 153/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1745 - accuracy: 0.9816\n","Epoch 153: val_loss did not improve from 0.15092\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1745 - accuracy: 0.9816 - val_loss: 0.1889 - val_accuracy: 0.9801 - lr: 1.0000e-04\n","Epoch 154/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9875\n","Epoch 154: val_loss did not improve from 0.15092\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1602 - accuracy: 0.9875 - val_loss: 0.1749 - val_accuracy: 0.9793 - lr: 1.0000e-04\n","Epoch 155/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9914\n","Epoch 155: val_loss did not improve from 0.15092\n","56/56 [==============================] - 14s 246ms/step - loss: 0.1540 - accuracy: 0.9914 - val_loss: 0.1602 - val_accuracy: 0.9868 - lr: 1.0000e-04\n","Epoch 156/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9926\n","Epoch 156: val_loss improved from 0.15092 to 0.14886, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1529 - accuracy: 0.9926 - val_loss: 0.1489 - val_accuracy: 0.9911 - lr: 1.0000e-04\n","Epoch 157/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9923\n","Epoch 157: val_loss did not improve from 0.14886\n","56/56 [==============================] - 14s 251ms/step - loss: 0.1508 - accuracy: 0.9923 - val_loss: 0.1502 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 158/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1501 - accuracy: 0.9939\n","Epoch 158: val_loss improved from 0.14886 to 0.14751, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.1501 - accuracy: 0.9939 - val_loss: 0.1475 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 159/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1487 - accuracy: 0.9941\n","Epoch 159: val_loss improved from 0.14751 to 0.14611, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.1487 - accuracy: 0.9941 - val_loss: 0.1461 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 160/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1530 - accuracy: 0.9932\n","Epoch 160: val_loss improved from 0.14611 to 0.14415, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.1530 - accuracy: 0.9932 - val_loss: 0.1441 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 161/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.9942\n","Epoch 161: val_loss did not improve from 0.14415\n","56/56 [==============================] - 14s 251ms/step - loss: 0.1464 - accuracy: 0.9942 - val_loss: 0.1443 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 162/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 0.9946\n","Epoch 162: val_loss improved from 0.14415 to 0.14273, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1470 - accuracy: 0.9946 - val_loss: 0.1427 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 163/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1448 - accuracy: 0.9949\n","Epoch 163: val_loss did not improve from 0.14273\n","56/56 [==============================] - 14s 246ms/step - loss: 0.1448 - accuracy: 0.9949 - val_loss: 0.1428 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 164/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9948\n","Epoch 164: val_loss improved from 0.14273 to 0.14109, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.1440 - accuracy: 0.9948 - val_loss: 0.1411 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 165/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1437 - accuracy: 0.9949\n","Epoch 165: val_loss did not improve from 0.14109\n","56/56 [==============================] - 14s 246ms/step - loss: 0.1437 - accuracy: 0.9949 - val_loss: 0.1413 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 166/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9951\n","Epoch 166: val_loss improved from 0.14109 to 0.14060, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 15s 262ms/step - loss: 0.1402 - accuracy: 0.9951 - val_loss: 0.1406 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 167/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.9953\n","Epoch 167: val_loss improved from 0.14060 to 0.13858, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1410 - accuracy: 0.9953 - val_loss: 0.1386 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 168/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9950\n","Epoch 168: val_loss improved from 0.13858 to 0.13534, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1392 - accuracy: 0.9950 - val_loss: 0.1353 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 169/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1395 - accuracy: 0.9941\n","Epoch 169: val_loss did not improve from 0.13534\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1395 - accuracy: 0.9941 - val_loss: 0.1766 - val_accuracy: 0.9664 - lr: 1.0000e-04\n","Epoch 170/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9945\n","Epoch 170: val_loss did not improve from 0.13534\n","56/56 [==============================] - 14s 252ms/step - loss: 0.1381 - accuracy: 0.9945 - val_loss: 0.1368 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 171/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1376 - accuracy: 0.9945\n","Epoch 171: val_loss improved from 0.13534 to 0.13450, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1376 - accuracy: 0.9945 - val_loss: 0.1345 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 172/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1404 - accuracy: 0.9943\n","Epoch 172: val_loss did not improve from 0.13450\n","56/56 [==============================] - 14s 246ms/step - loss: 0.1404 - accuracy: 0.9943 - val_loss: 0.1381 - val_accuracy: 0.9920 - lr: 1.0000e-04\n","Epoch 173/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.9946\n","Epoch 173: val_loss improved from 0.13450 to 0.13419, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.1357 - accuracy: 0.9946 - val_loss: 0.1342 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 174/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.9943\n","Epoch 174: val_loss did not improve from 0.13419\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1364 - accuracy: 0.9943 - val_loss: 0.1345 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 175/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.9949\n","Epoch 175: val_loss did not improve from 0.13419\n","56/56 [==============================] - 14s 248ms/step - loss: 0.1372 - accuracy: 0.9949 - val_loss: 0.1366 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 176/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9940\n","Epoch 176: val_loss improved from 0.13419 to 0.13089, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1353 - accuracy: 0.9940 - val_loss: 0.1309 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 177/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9925\n","Epoch 177: val_loss did not improve from 0.13089\n","56/56 [==============================] - 14s 248ms/step - loss: 0.1366 - accuracy: 0.9925 - val_loss: 0.1363 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 178/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1378 - accuracy: 0.9922\n","Epoch 178: val_loss did not improve from 0.13089\n","56/56 [==============================] - 14s 246ms/step - loss: 0.1378 - accuracy: 0.9922 - val_loss: 0.1331 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 179/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9943\n","Epoch 179: val_loss did not improve from 0.13089\n","56/56 [==============================] - 14s 248ms/step - loss: 0.1323 - accuracy: 0.9943 - val_loss: 0.1366 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 180/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1310 - accuracy: 0.9948\n","Epoch 180: val_loss did not improve from 0.13089\n","56/56 [==============================] - 14s 246ms/step - loss: 0.1310 - accuracy: 0.9948 - val_loss: 0.1317 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 181/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.9953\n","Epoch 181: val_loss improved from 0.13089 to 0.12926, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1305 - accuracy: 0.9953 - val_loss: 0.1293 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 182/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9946\n","Epoch 182: val_loss improved from 0.12926 to 0.12835, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.1341 - accuracy: 0.9946 - val_loss: 0.1284 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 183/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9952\n","Epoch 183: val_loss improved from 0.12835 to 0.12646, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.1285 - accuracy: 0.9952 - val_loss: 0.1265 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 184/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1302 - accuracy: 0.9941\n","Epoch 184: val_loss improved from 0.12646 to 0.12457, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1302 - accuracy: 0.9941 - val_loss: 0.1246 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 185/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9946\n","Epoch 185: val_loss did not improve from 0.12457\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1293 - accuracy: 0.9946 - val_loss: 0.1261 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 186/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9929\n","Epoch 186: val_loss did not improve from 0.12457\n","56/56 [==============================] - 14s 251ms/step - loss: 0.1341 - accuracy: 0.9929 - val_loss: 0.1273 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 187/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1286 - accuracy: 0.9951\n","Epoch 187: val_loss did not improve from 0.12457\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1286 - accuracy: 0.9951 - val_loss: 0.1249 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 188/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9954\n","Epoch 188: val_loss improved from 0.12457 to 0.12295, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1272 - accuracy: 0.9954 - val_loss: 0.1230 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 189/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9954\n","Epoch 189: val_loss improved from 0.12295 to 0.12257, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.1271 - accuracy: 0.9954 - val_loss: 0.1226 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 190/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9953\n","Epoch 190: val_loss improved from 0.12257 to 0.12125, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.1247 - accuracy: 0.9953 - val_loss: 0.1212 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 191/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1270 - accuracy: 0.9951\n","Epoch 191: val_loss did not improve from 0.12125\n","56/56 [==============================] - 14s 246ms/step - loss: 0.1270 - accuracy: 0.9951 - val_loss: 0.1234 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 192/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9955\n","Epoch 192: val_loss did not improve from 0.12125\n","56/56 [==============================] - 14s 246ms/step - loss: 0.1242 - accuracy: 0.9955 - val_loss: 0.1215 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 193/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 0.9957\n","Epoch 193: val_loss improved from 0.12125 to 0.12010, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1229 - accuracy: 0.9957 - val_loss: 0.1201 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 194/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1218 - accuracy: 0.9957\n","Epoch 194: val_loss did not improve from 0.12010\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1218 - accuracy: 0.9957 - val_loss: 0.1207 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 195/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9958\n","Epoch 195: val_loss did not improve from 0.12010\n","56/56 [==============================] - 14s 248ms/step - loss: 0.1225 - accuracy: 0.9958 - val_loss: 0.1206 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 196/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9959\n","Epoch 196: val_loss improved from 0.12010 to 0.11884, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1201 - accuracy: 0.9959 - val_loss: 0.1188 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 197/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9958\n","Epoch 197: val_loss improved from 0.11884 to 0.11699, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.1190 - accuracy: 0.9958 - val_loss: 0.1170 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 198/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1165 - accuracy: 0.9959\n","Epoch 198: val_loss did not improve from 0.11699\n","56/56 [==============================] - 14s 249ms/step - loss: 0.1165 - accuracy: 0.9959 - val_loss: 0.1178 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 199/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9960\n","Epoch 199: val_loss improved from 0.11699 to 0.11658, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.1200 - accuracy: 0.9960 - val_loss: 0.1166 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 200/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1171 - accuracy: 0.9960\n","Epoch 200: val_loss improved from 0.11658 to 0.11557, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.1171 - accuracy: 0.9960 - val_loss: 0.1156 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 201/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1197 - accuracy: 0.9952\n","Epoch 201: val_loss improved from 0.11557 to 0.11260, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1197 - accuracy: 0.9952 - val_loss: 0.1126 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 202/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9956\n","Epoch 202: val_loss did not improve from 0.11260\n","56/56 [==============================] - 14s 247ms/step - loss: 0.1184 - accuracy: 0.9956 - val_loss: 0.1147 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 203/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9956\n","Epoch 203: val_loss did not improve from 0.11260\n","56/56 [==============================] - 14s 245ms/step - loss: 0.1180 - accuracy: 0.9956 - val_loss: 0.1140 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 204/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1165 - accuracy: 0.9959\n","Epoch 204: val_loss did not improve from 0.11260\n","56/56 [==============================] - 14s 251ms/step - loss: 0.1165 - accuracy: 0.9959 - val_loss: 0.1131 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 205/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1162 - accuracy: 0.9960\n","Epoch 205: val_loss did not improve from 0.11260\n","56/56 [==============================] - 14s 247ms/step - loss: 0.1162 - accuracy: 0.9960 - val_loss: 0.1142 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 206/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9959\n","Epoch 206: val_loss did not improve from 0.11260\n","56/56 [==============================] - 14s 251ms/step - loss: 0.1152 - accuracy: 0.9959 - val_loss: 0.1281 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 207/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9909\n","Epoch 207: val_loss improved from 0.11260 to 0.11203, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.1225 - accuracy: 0.9909 - val_loss: 0.1120 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 208/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1133 - accuracy: 0.9953\n","Epoch 208: val_loss did not improve from 0.11203\n","56/56 [==============================] - 14s 247ms/step - loss: 0.1133 - accuracy: 0.9953 - val_loss: 0.1160 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 209/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9952\n","Epoch 209: val_loss did not improve from 0.11203\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1127 - accuracy: 0.9952 - val_loss: 0.1153 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 210/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1125 - accuracy: 0.9950\n","Epoch 210: val_loss did not improve from 0.11203\n","56/56 [==============================] - 14s 246ms/step - loss: 0.1125 - accuracy: 0.9950 - val_loss: 0.1142 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 211/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1120 - accuracy: 0.9954\n","Epoch 211: val_loss improved from 0.11203 to 0.11088, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.1120 - accuracy: 0.9954 - val_loss: 0.1109 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 212/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.9883\n","Epoch 212: val_loss did not improve from 0.11088\n","56/56 [==============================] - 14s 246ms/step - loss: 0.1254 - accuracy: 0.9883 - val_loss: 0.1174 - val_accuracy: 0.9822 - lr: 1.0000e-04\n","Epoch 213/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9925\n","Epoch 213: val_loss did not improve from 0.11088\n","56/56 [==============================] - 14s 247ms/step - loss: 0.1152 - accuracy: 0.9925 - val_loss: 0.1141 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 214/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9945\n","Epoch 214: val_loss did not improve from 0.11088\n","56/56 [==============================] - 14s 246ms/step - loss: 0.1115 - accuracy: 0.9945 - val_loss: 0.1134 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 215/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9948\n","Epoch 215: val_loss improved from 0.11088 to 0.10879, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.1127 - accuracy: 0.9948 - val_loss: 0.1088 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 216/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9946\n","Epoch 216: val_loss did not improve from 0.10879\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1114 - accuracy: 0.9946 - val_loss: 0.1125 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 217/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9955\n","Epoch 217: val_loss did not improve from 0.10879\n","56/56 [==============================] - 14s 247ms/step - loss: 0.1100 - accuracy: 0.9955 - val_loss: 0.1106 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 218/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9957\n","Epoch 218: val_loss improved from 0.10879 to 0.10792, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.1082 - accuracy: 0.9957 - val_loss: 0.1079 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 219/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9959\n","Epoch 219: val_loss improved from 0.10792 to 0.10734, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.1076 - accuracy: 0.9959 - val_loss: 0.1073 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 220/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9959\n","Epoch 220: val_loss improved from 0.10734 to 0.10636, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.1080 - accuracy: 0.9959 - val_loss: 0.1064 - val_accuracy: 0.9942 - lr: 1.0000e-04\n","Epoch 221/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 0.9960\n","Epoch 221: val_loss improved from 0.10636 to 0.10591, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.1064 - accuracy: 0.9960 - val_loss: 0.1059 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 222/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.9962\n","Epoch 222: val_loss improved from 0.10591 to 0.10424, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1034 - accuracy: 0.9962 - val_loss: 0.1042 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 223/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9961\n","Epoch 223: val_loss improved from 0.10424 to 0.10241, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1044 - accuracy: 0.9961 - val_loss: 0.1024 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 224/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9957\n","Epoch 224: val_loss improved from 0.10241 to 0.10026, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.1047 - accuracy: 0.9957 - val_loss: 0.1003 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 225/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9955\n","Epoch 225: val_loss did not improve from 0.10026\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1027 - accuracy: 0.9955 - val_loss: 0.1017 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 226/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9954\n","Epoch 226: val_loss improved from 0.10026 to 0.09976, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.1068 - accuracy: 0.9954 - val_loss: 0.0998 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 227/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9960\n","Epoch 227: val_loss did not improve from 0.09976\n","56/56 [==============================] - 14s 247ms/step - loss: 0.1008 - accuracy: 0.9960 - val_loss: 0.1004 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 228/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9962\n","Epoch 228: val_loss did not improve from 0.09976\n","56/56 [==============================] - 14s 251ms/step - loss: 0.1037 - accuracy: 0.9962 - val_loss: 0.1008 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 229/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1000 - accuracy: 0.9962\n","Epoch 229: val_loss improved from 0.09976 to 0.09882, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1000 - accuracy: 0.9962 - val_loss: 0.0988 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 230/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9963\n","Epoch 230: val_loss did not improve from 0.09882\n","56/56 [==============================] - 14s 250ms/step - loss: 0.1010 - accuracy: 0.9963 - val_loss: 0.0989 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 231/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1031 - accuracy: 0.9962\n","Epoch 231: val_loss did not improve from 0.09882\n","56/56 [==============================] - 14s 248ms/step - loss: 0.1031 - accuracy: 0.9962 - val_loss: 0.0989 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 232/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1007 - accuracy: 0.9964\n","Epoch 232: val_loss improved from 0.09882 to 0.09858, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.1007 - accuracy: 0.9964 - val_loss: 0.0986 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 233/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9964\n","Epoch 233: val_loss improved from 0.09858 to 0.09768, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.1014 - accuracy: 0.9964 - val_loss: 0.0977 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 234/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9964\n","Epoch 234: val_loss improved from 0.09768 to 0.09716, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0986 - accuracy: 0.9964 - val_loss: 0.0972 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 235/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9964\n","Epoch 235: val_loss improved from 0.09716 to 0.09501, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0980 - accuracy: 0.9964 - val_loss: 0.0950 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 236/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9965\n","Epoch 236: val_loss did not improve from 0.09501\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0993 - accuracy: 0.9965 - val_loss: 0.0962 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 237/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0976 - accuracy: 0.9966\n","Epoch 237: val_loss did not improve from 0.09501\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0976 - accuracy: 0.9966 - val_loss: 0.0952 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 238/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9965\n","Epoch 238: val_loss improved from 0.09501 to 0.09371, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.0971 - accuracy: 0.9965 - val_loss: 0.0937 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 239/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.9965\n","Epoch 239: val_loss did not improve from 0.09371\n","56/56 [==============================] - 14s 245ms/step - loss: 0.0949 - accuracy: 0.9965 - val_loss: 0.0942 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 240/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9966\n","Epoch 240: val_loss improved from 0.09371 to 0.09332, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0944 - accuracy: 0.9966 - val_loss: 0.0933 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 241/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9965\n","Epoch 241: val_loss improved from 0.09332 to 0.09222, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0957 - accuracy: 0.9965 - val_loss: 0.0922 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 242/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9965\n","Epoch 242: val_loss improved from 0.09222 to 0.09112, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.0980 - accuracy: 0.9965 - val_loss: 0.0911 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 243/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9967\n","Epoch 243: val_loss did not improve from 0.09112\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0938 - accuracy: 0.9967 - val_loss: 0.0914 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 244/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9966\n","Epoch 244: val_loss did not improve from 0.09112\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0933 - accuracy: 0.9966 - val_loss: 0.0912 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 245/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9966\n","Epoch 245: val_loss improved from 0.09112 to 0.09011, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0940 - accuracy: 0.9966 - val_loss: 0.0901 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 246/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9966\n","Epoch 246: val_loss did not improve from 0.09011\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0944 - accuracy: 0.9966 - val_loss: 0.0915 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 247/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9966\n","Epoch 247: val_loss improved from 0.09011 to 0.09005, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0930 - accuracy: 0.9966 - val_loss: 0.0901 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 248/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9967\n","Epoch 248: val_loss improved from 0.09005 to 0.08999, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0919 - accuracy: 0.9967 - val_loss: 0.0900 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 249/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9967\n","Epoch 249: val_loss did not improve from 0.08999\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0930 - accuracy: 0.9967 - val_loss: 0.0908 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 250/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9966\n","Epoch 250: val_loss improved from 0.08999 to 0.08902, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0928 - accuracy: 0.9966 - val_loss: 0.0890 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 251/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9967\n","Epoch 251: val_loss improved from 0.08902 to 0.08841, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.0889 - accuracy: 0.9967 - val_loss: 0.0884 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 252/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9966\n","Epoch 252: val_loss improved from 0.08841 to 0.08816, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0909 - accuracy: 0.9966 - val_loss: 0.0882 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 253/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9965\n","Epoch 253: val_loss improved from 0.08816 to 0.08693, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0904 - accuracy: 0.9965 - val_loss: 0.0869 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 254/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9967\n","Epoch 254: val_loss improved from 0.08693 to 0.08684, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0878 - accuracy: 0.9967 - val_loss: 0.0868 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 255/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9968\n","Epoch 255: val_loss improved from 0.08684 to 0.08650, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 256ms/step - loss: 0.0915 - accuracy: 0.9968 - val_loss: 0.0865 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 256/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9968\n","Epoch 256: val_loss improved from 0.08650 to 0.08539, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0873 - accuracy: 0.9968 - val_loss: 0.0854 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 257/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9968\n","Epoch 257: val_loss improved from 0.08539 to 0.08497, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.0907 - accuracy: 0.9968 - val_loss: 0.0850 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 258/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9968\n","Epoch 258: val_loss did not improve from 0.08497\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0879 - accuracy: 0.9968 - val_loss: 0.0858 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 259/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9970\n","Epoch 259: val_loss did not improve from 0.08497\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0829 - accuracy: 0.9970 - val_loss: 0.0856 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 260/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9959\n","Epoch 260: val_loss improved from 0.08497 to 0.08257, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.0918 - accuracy: 0.9959 - val_loss: 0.0826 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 261/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9932\n","Epoch 261: val_loss did not improve from 0.08257\n","56/56 [==============================] - 14s 245ms/step - loss: 0.0909 - accuracy: 0.9932 - val_loss: 0.0936 - val_accuracy: 0.9883 - lr: 1.0000e-04\n","Epoch 262/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9928\n","Epoch 262: val_loss did not improve from 0.08257\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0937 - accuracy: 0.9928 - val_loss: 0.0931 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 263/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9952\n","Epoch 263: val_loss did not improve from 0.08257\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0864 - accuracy: 0.9952 - val_loss: 0.0855 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 264/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9959\n","Epoch 264: val_loss did not improve from 0.08257\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0877 - accuracy: 0.9959 - val_loss: 0.0851 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 265/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0848 - accuracy: 0.9964\n","Epoch 265: val_loss did not improve from 0.08257\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0848 - accuracy: 0.9964 - val_loss: 0.0854 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 266/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9965\n","Epoch 266: val_loss did not improve from 0.08257\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0869 - accuracy: 0.9965 - val_loss: 0.0835 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 267/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9966\n","Epoch 267: val_loss did not improve from 0.08257\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0840 - accuracy: 0.9966 - val_loss: 0.0834 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 268/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9966\n","Epoch 268: val_loss improved from 0.08257 to 0.08174, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0835 - accuracy: 0.9966 - val_loss: 0.0817 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 269/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9968\n","Epoch 269: val_loss improved from 0.08174 to 0.08106, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.0822 - accuracy: 0.9968 - val_loss: 0.0811 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 270/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9966\n","Epoch 270: val_loss did not improve from 0.08106\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0849 - accuracy: 0.9966 - val_loss: 0.0853 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 271/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9959\n","Epoch 271: val_loss did not improve from 0.08106\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0875 - accuracy: 0.9959 - val_loss: 0.0830 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 272/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9953\n","Epoch 272: val_loss did not improve from 0.08106\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0847 - accuracy: 0.9953 - val_loss: 0.0819 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 273/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9916\n","Epoch 273: val_loss did not improve from 0.08106\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0958 - accuracy: 0.9916 - val_loss: 0.0900 - val_accuracy: 0.9868 - lr: 1.0000e-04\n","Epoch 274/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9945\n","Epoch 274: val_loss did not improve from 0.08106\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0857 - accuracy: 0.9945 - val_loss: 0.0821 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 275/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9958\n","Epoch 275: val_loss did not improve from 0.08106\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0824 - accuracy: 0.9958 - val_loss: 0.0842 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 276/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9963\n","Epoch 276: val_loss improved from 0.08106 to 0.07953, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0815 - accuracy: 0.9963 - val_loss: 0.0795 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 277/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9953\n","Epoch 277: val_loss improved from 0.07953 to 0.07909, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0847 - accuracy: 0.9953 - val_loss: 0.0791 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 278/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9961\n","Epoch 278: val_loss did not improve from 0.07909\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0813 - accuracy: 0.9961 - val_loss: 0.0808 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 279/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9956\n","Epoch 279: val_loss did not improve from 0.07909\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0841 - accuracy: 0.9956 - val_loss: 0.1007 - val_accuracy: 0.9843 - lr: 1.0000e-04\n","Epoch 280/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9943\n","Epoch 280: val_loss improved from 0.07909 to 0.07876, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.0841 - accuracy: 0.9943 - val_loss: 0.0788 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 281/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9944\n","Epoch 281: val_loss did not improve from 0.07876\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0839 - accuracy: 0.9944 - val_loss: 0.0795 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 282/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9956\n","Epoch 282: val_loss did not improve from 0.07876\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0806 - accuracy: 0.9956 - val_loss: 0.0820 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 283/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9963\n","Epoch 283: val_loss improved from 0.07876 to 0.07870, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0776 - accuracy: 0.9963 - val_loss: 0.0787 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 284/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9965\n","Epoch 284: val_loss improved from 0.07870 to 0.07727, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0755 - accuracy: 0.9965 - val_loss: 0.0773 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 285/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9967\n","Epoch 285: val_loss improved from 0.07727 to 0.07664, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0762 - accuracy: 0.9967 - val_loss: 0.0766 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 286/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9968\n","Epoch 286: val_loss improved from 0.07664 to 0.07545, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.0774 - accuracy: 0.9968 - val_loss: 0.0755 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 287/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9968\n","Epoch 287: val_loss improved from 0.07545 to 0.07427, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0774 - accuracy: 0.9968 - val_loss: 0.0743 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 288/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9968\n","Epoch 288: val_loss improved from 0.07427 to 0.07368, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0764 - accuracy: 0.9968 - val_loss: 0.0737 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 289/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9969\n","Epoch 289: val_loss did not improve from 0.07368\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0766 - accuracy: 0.9969 - val_loss: 0.0738 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 290/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9969\n","Epoch 290: val_loss improved from 0.07368 to 0.07367, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0779 - accuracy: 0.9969 - val_loss: 0.0737 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 291/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9970\n","Epoch 291: val_loss improved from 0.07367 to 0.07280, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.0741 - accuracy: 0.9970 - val_loss: 0.0728 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 292/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9970\n","Epoch 292: val_loss improved from 0.07280 to 0.07258, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0744 - accuracy: 0.9970 - val_loss: 0.0726 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 293/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9970\n","Epoch 293: val_loss improved from 0.07258 to 0.07162, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0754 - accuracy: 0.9970 - val_loss: 0.0716 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 294/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9971\n","Epoch 294: val_loss did not improve from 0.07162\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0718 - accuracy: 0.9971 - val_loss: 0.0720 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 295/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9971\n","Epoch 295: val_loss improved from 0.07162 to 0.07087, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.0727 - accuracy: 0.9971 - val_loss: 0.0709 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 296/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9972\n","Epoch 296: val_loss improved from 0.07087 to 0.07048, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0720 - accuracy: 0.9972 - val_loss: 0.0705 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 297/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9971\n","Epoch 297: val_loss improved from 0.07048 to 0.07025, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0740 - accuracy: 0.9971 - val_loss: 0.0702 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 298/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9968\n","Epoch 298: val_loss improved from 0.07025 to 0.06974, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 15s 260ms/step - loss: 0.0757 - accuracy: 0.9968 - val_loss: 0.0697 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 299/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9972\n","Epoch 299: val_loss did not improve from 0.06974\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0708 - accuracy: 0.9972 - val_loss: 0.0705 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 300/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9971\n","Epoch 300: val_loss improved from 0.06974 to 0.06719, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.0719 - accuracy: 0.9971 - val_loss: 0.0672 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 301/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9972\n","Epoch 301: val_loss did not improve from 0.06719\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0722 - accuracy: 0.9972 - val_loss: 0.0692 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 302/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9971\n","Epoch 302: val_loss did not improve from 0.06719\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0707 - accuracy: 0.9971 - val_loss: 0.0681 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 303/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9960\n","Epoch 303: val_loss did not improve from 0.06719\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0735 - accuracy: 0.9960 - val_loss: 0.1104 - val_accuracy: 0.9749 - lr: 1.0000e-04\n","Epoch 304/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9919\n","Epoch 304: val_loss did not improve from 0.06719\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0799 - accuracy: 0.9919 - val_loss: 0.0810 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 305/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9908\n","Epoch 305: val_loss did not improve from 0.06719\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0820 - accuracy: 0.9908 - val_loss: 0.0704 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 306/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9950\n","Epoch 306: val_loss did not improve from 0.06719\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0738 - accuracy: 0.9950 - val_loss: 0.0722 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 307/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9955\n","Epoch 307: val_loss did not improve from 0.06719\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0749 - accuracy: 0.9955 - val_loss: 0.0696 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 308/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9958\n","Epoch 308: val_loss did not improve from 0.06719\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0717 - accuracy: 0.9958 - val_loss: 0.0690 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 309/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9940\n","Epoch 309: val_loss did not improve from 0.06719\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0742 - accuracy: 0.9940 - val_loss: 0.0907 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 310/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9958\n","Epoch 310: val_loss improved from 0.06719 to 0.06693, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0704 - accuracy: 0.9958 - val_loss: 0.0669 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 311/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9964\n","Epoch 311: val_loss did not improve from 0.06693\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0705 - accuracy: 0.9964 - val_loss: 0.0684 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 312/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9967\n","Epoch 312: val_loss improved from 0.06693 to 0.06671, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0693 - accuracy: 0.9967 - val_loss: 0.0667 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 313/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9965\n","Epoch 313: val_loss improved from 0.06671 to 0.06566, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0689 - accuracy: 0.9965 - val_loss: 0.0657 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 314/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9968\n","Epoch 314: val_loss did not improve from 0.06566\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0685 - accuracy: 0.9968 - val_loss: 0.0657 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 315/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9968\n","Epoch 315: val_loss improved from 0.06566 to 0.06535, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0677 - accuracy: 0.9968 - val_loss: 0.0654 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 316/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9970\n","Epoch 316: val_loss did not improve from 0.06535\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0658 - accuracy: 0.9970 - val_loss: 0.0664 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 317/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9969\n","Epoch 317: val_loss did not improve from 0.06535\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0661 - accuracy: 0.9969 - val_loss: 0.0655 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 318/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9971\n","Epoch 318: val_loss improved from 0.06535 to 0.06386, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.0684 - accuracy: 0.9971 - val_loss: 0.0639 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 319/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9970\n","Epoch 319: val_loss improved from 0.06386 to 0.06360, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.0684 - accuracy: 0.9970 - val_loss: 0.0636 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 320/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9970\n","Epoch 320: val_loss improved from 0.06360 to 0.06308, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.0685 - accuracy: 0.9970 - val_loss: 0.0631 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 321/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9972\n","Epoch 321: val_loss did not improve from 0.06308\n","56/56 [==============================] - 14s 245ms/step - loss: 0.0637 - accuracy: 0.9972 - val_loss: 0.0635 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 322/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9971\n","Epoch 322: val_loss improved from 0.06308 to 0.06297, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0662 - accuracy: 0.9971 - val_loss: 0.0630 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 323/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9972\n","Epoch 323: val_loss improved from 0.06297 to 0.06264, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0668 - accuracy: 0.9972 - val_loss: 0.0626 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 324/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9963\n","Epoch 324: val_loss did not improve from 0.06264\n","56/56 [==============================] - 14s 248ms/step - loss: 0.0673 - accuracy: 0.9963 - val_loss: 0.0644 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 325/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9971\n","Epoch 325: val_loss did not improve from 0.06264\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0634 - accuracy: 0.9971 - val_loss: 0.0636 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 326/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0635 - accuracy: 0.9968\n","Epoch 326: val_loss did not improve from 0.06264\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0635 - accuracy: 0.9968 - val_loss: 0.0996 - val_accuracy: 0.9813 - lr: 1.0000e-04\n","Epoch 327/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9968\n","Epoch 327: val_loss did not improve from 0.06264\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0633 - accuracy: 0.9968 - val_loss: 0.0634 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 328/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9961\n","Epoch 328: val_loss improved from 0.06264 to 0.06218, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0644 - accuracy: 0.9961 - val_loss: 0.0622 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 329/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9953\n","Epoch 329: val_loss did not improve from 0.06218\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0685 - accuracy: 0.9953 - val_loss: 0.0681 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 330/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9960\n","Epoch 330: val_loss did not improve from 0.06218\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0659 - accuracy: 0.9960 - val_loss: 0.0628 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 331/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9969\n","Epoch 331: val_loss did not improve from 0.06218\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0617 - accuracy: 0.9969 - val_loss: 0.0634 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 332/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9972\n","Epoch 332: val_loss improved from 0.06218 to 0.06212, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0604 - accuracy: 0.9972 - val_loss: 0.0621 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 333/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9972\n","Epoch 333: val_loss improved from 0.06212 to 0.06075, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.0604 - accuracy: 0.9972 - val_loss: 0.0607 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 334/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9972\n","Epoch 334: val_loss improved from 0.06075 to 0.06004, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0603 - accuracy: 0.9972 - val_loss: 0.0600 - val_accuracy: 0.9955 - lr: 1.0000e-04\n","Epoch 335/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9972\n","Epoch 335: val_loss improved from 0.06004 to 0.05808, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.0617 - accuracy: 0.9972 - val_loss: 0.0581 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 336/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9971\n","Epoch 336: val_loss did not improve from 0.05808\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0614 - accuracy: 0.9971 - val_loss: 0.0585 - val_accuracy: 0.9958 - lr: 1.0000e-04\n","Epoch 337/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9973\n","Epoch 337: val_loss did not improve from 0.05808\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0591 - accuracy: 0.9973 - val_loss: 0.0582 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 338/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9973\n","Epoch 338: val_loss improved from 0.05808 to 0.05748, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0586 - accuracy: 0.9973 - val_loss: 0.0575 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 339/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9974\n","Epoch 339: val_loss did not improve from 0.05748\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0597 - accuracy: 0.9974 - val_loss: 0.0585 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 340/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9974\n","Epoch 340: val_loss improved from 0.05748 to 0.05585, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0617 - accuracy: 0.9974 - val_loss: 0.0559 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 341/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9974\n","Epoch 341: val_loss did not improve from 0.05585\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0588 - accuracy: 0.9974 - val_loss: 0.0569 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 342/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9975\n","Epoch 342: val_loss did not improve from 0.05585\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0591 - accuracy: 0.9975 - val_loss: 0.0576 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 343/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9974\n","Epoch 343: val_loss did not improve from 0.05585\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0583 - accuracy: 0.9974 - val_loss: 0.0572 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 344/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9973\n","Epoch 344: val_loss did not improve from 0.05585\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0588 - accuracy: 0.9973 - val_loss: 0.0568 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 345/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9974\n","Epoch 345: val_loss improved from 0.05585 to 0.05568, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0604 - accuracy: 0.9974 - val_loss: 0.0557 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 346/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9975\n","Epoch 346: val_loss did not improve from 0.05568\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0559 - accuracy: 0.9975 - val_loss: 0.0563 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 347/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9975\n","Epoch 347: val_loss did not improve from 0.05568\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0575 - accuracy: 0.9975 - val_loss: 0.0559 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 348/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9974\n","Epoch 348: val_loss did not improve from 0.05568\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0586 - accuracy: 0.9974 - val_loss: 0.0557 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 349/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9975\n","Epoch 349: val_loss improved from 0.05568 to 0.05492, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0565 - accuracy: 0.9975 - val_loss: 0.0549 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 350/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9976\n","Epoch 350: val_loss did not improve from 0.05492\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0555 - accuracy: 0.9976 - val_loss: 0.0551 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 351/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9975\n","Epoch 351: val_loss improved from 0.05492 to 0.05398, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.0574 - accuracy: 0.9975 - val_loss: 0.0540 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 352/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9976\n","Epoch 352: val_loss did not improve from 0.05398\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0554 - accuracy: 0.9976 - val_loss: 0.0546 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 353/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9976\n","Epoch 353: val_loss improved from 0.05398 to 0.05384, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0541 - accuracy: 0.9976 - val_loss: 0.0538 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 354/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9976\n","Epoch 354: val_loss improved from 0.05384 to 0.05339, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0547 - accuracy: 0.9976 - val_loss: 0.0534 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 355/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9976\n","Epoch 355: val_loss did not improve from 0.05339\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0560 - accuracy: 0.9976 - val_loss: 0.0537 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 356/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9976\n","Epoch 356: val_loss did not improve from 0.05339\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0555 - accuracy: 0.9976 - val_loss: 0.0548 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 357/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9976\n","Epoch 357: val_loss improved from 0.05339 to 0.05289, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0559 - accuracy: 0.9976 - val_loss: 0.0529 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 358/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9975\n","Epoch 358: val_loss did not improve from 0.05289\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0571 - accuracy: 0.9975 - val_loss: 0.0530 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 359/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9976\n","Epoch 359: val_loss did not improve from 0.05289\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0547 - accuracy: 0.9976 - val_loss: 0.0531 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 360/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9976\n","Epoch 360: val_loss improved from 0.05289 to 0.05256, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 15s 260ms/step - loss: 0.0539 - accuracy: 0.9976 - val_loss: 0.0526 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 361/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9976\n","Epoch 361: val_loss did not improve from 0.05256\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0562 - accuracy: 0.9976 - val_loss: 0.0529 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 362/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9977\n","Epoch 362: val_loss improved from 0.05256 to 0.05232, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0547 - accuracy: 0.9977 - val_loss: 0.0523 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 363/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9978\n","Epoch 363: val_loss did not improve from 0.05232\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0515 - accuracy: 0.9978 - val_loss: 0.0524 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 364/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9972\n","Epoch 364: val_loss did not improve from 0.05232\n","56/56 [==============================] - 14s 245ms/step - loss: 0.0548 - accuracy: 0.9972 - val_loss: 0.0544 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 365/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9931\n","Epoch 365: val_loss did not improve from 0.05232\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0619 - accuracy: 0.9931 - val_loss: 0.0591 - val_accuracy: 0.9891 - lr: 1.0000e-04\n","Epoch 366/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9961\n","Epoch 366: val_loss did not improve from 0.05232\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0541 - accuracy: 0.9961 - val_loss: 0.0553 - val_accuracy: 0.9951 - lr: 1.0000e-04\n","Epoch 367/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9970\n","Epoch 367: val_loss did not improve from 0.05232\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0533 - accuracy: 0.9970 - val_loss: 0.0548 - val_accuracy: 0.9957 - lr: 1.0000e-04\n","Epoch 368/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9973\n","Epoch 368: val_loss did not improve from 0.05232\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0551 - accuracy: 0.9973 - val_loss: 0.0528 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 369/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9974\n","Epoch 369: val_loss improved from 0.05232 to 0.05145, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.0524 - accuracy: 0.9974 - val_loss: 0.0515 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 370/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9971\n","Epoch 370: val_loss improved from 0.05145 to 0.05030, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0530 - accuracy: 0.9971 - val_loss: 0.0503 - val_accuracy: 0.9954 - lr: 1.0000e-04\n","Epoch 371/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9971\n","Epoch 371: val_loss did not improve from 0.05030\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0532 - accuracy: 0.9971 - val_loss: 0.0509 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 372/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9975\n","Epoch 372: val_loss improved from 0.05030 to 0.05005, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0506 - accuracy: 0.9975 - val_loss: 0.0501 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 373/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9975\n","Epoch 373: val_loss did not improve from 0.05005\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0504 - accuracy: 0.9975 - val_loss: 0.0501 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 374/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9976\n","Epoch 374: val_loss did not improve from 0.05005\n","56/56 [==============================] - 14s 248ms/step - loss: 0.0489 - accuracy: 0.9976 - val_loss: 0.0501 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 375/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9976\n","Epoch 375: val_loss improved from 0.05005 to 0.04833, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0502 - accuracy: 0.9976 - val_loss: 0.0483 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 376/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9975\n","Epoch 376: val_loss did not improve from 0.04833\n","56/56 [==============================] - 14s 248ms/step - loss: 0.0535 - accuracy: 0.9975 - val_loss: 0.0495 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 377/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9977\n","Epoch 377: val_loss improved from 0.04833 to 0.04826, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0478 - accuracy: 0.9977 - val_loss: 0.0483 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 378/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9977\n","Epoch 378: val_loss improved from 0.04826 to 0.04788, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0495 - accuracy: 0.9977 - val_loss: 0.0479 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 379/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9977\n","Epoch 379: val_loss did not improve from 0.04788\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0484 - accuracy: 0.9977 - val_loss: 0.0484 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 380/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9978\n","Epoch 380: val_loss improved from 0.04788 to 0.04682, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.0503 - accuracy: 0.9978 - val_loss: 0.0468 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 381/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9978\n","Epoch 381: val_loss did not improve from 0.04682\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0501 - accuracy: 0.9978 - val_loss: 0.0472 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 382/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9978\n","Epoch 382: val_loss improved from 0.04682 to 0.04631, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 256ms/step - loss: 0.0502 - accuracy: 0.9978 - val_loss: 0.0463 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 383/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9978\n","Epoch 383: val_loss did not improve from 0.04631\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0489 - accuracy: 0.9978 - val_loss: 0.0471 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 384/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9978\n","Epoch 384: val_loss improved from 0.04631 to 0.04612, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0480 - accuracy: 0.9978 - val_loss: 0.0461 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 385/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9978\n","Epoch 385: val_loss did not improve from 0.04612\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0491 - accuracy: 0.9978 - val_loss: 0.0470 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 386/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9979\n","Epoch 386: val_loss did not improve from 0.04612\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0480 - accuracy: 0.9979 - val_loss: 0.0470 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 387/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9979\n","Epoch 387: val_loss improved from 0.04612 to 0.04485, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0477 - accuracy: 0.9979 - val_loss: 0.0448 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 388/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9978\n","Epoch 388: val_loss did not improve from 0.04485\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0485 - accuracy: 0.9978 - val_loss: 0.0466 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 389/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9977\n","Epoch 389: val_loss did not improve from 0.04485\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0475 - accuracy: 0.9977 - val_loss: 0.0458 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 390/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9978\n","Epoch 390: val_loss did not improve from 0.04485\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0508 - accuracy: 0.9978 - val_loss: 0.0459 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 391/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9980\n","Epoch 391: val_loss did not improve from 0.04485\n","56/56 [==============================] - 14s 248ms/step - loss: 0.0465 - accuracy: 0.9980 - val_loss: 0.0464 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 392/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9978\n","Epoch 392: val_loss improved from 0.04485 to 0.04374, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0471 - accuracy: 0.9978 - val_loss: 0.0437 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 393/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9955\n","Epoch 393: val_loss did not improve from 0.04374\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0514 - accuracy: 0.9955 - val_loss: 0.0486 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 394/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9952\n","Epoch 394: val_loss did not improve from 0.04374\n","56/56 [==============================] - 14s 248ms/step - loss: 0.0522 - accuracy: 0.9952 - val_loss: 0.0561 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 395/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9963\n","Epoch 395: val_loss did not improve from 0.04374\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0520 - accuracy: 0.9963 - val_loss: 0.0484 - val_accuracy: 0.9956 - lr: 1.0000e-04\n","Epoch 396/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9973\n","Epoch 396: val_loss did not improve from 0.04374\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0488 - accuracy: 0.9973 - val_loss: 0.0467 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 397/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9976\n","Epoch 397: val_loss did not improve from 0.04374\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0459 - accuracy: 0.9976 - val_loss: 0.0464 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 398/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9977\n","Epoch 398: val_loss did not improve from 0.04374\n","56/56 [==============================] - 14s 248ms/step - loss: 0.0464 - accuracy: 0.9977 - val_loss: 0.0450 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 399/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9977\n","Epoch 399: val_loss did not improve from 0.04374\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0457 - accuracy: 0.9977 - val_loss: 0.0442 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 400/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9978\n","Epoch 400: val_loss improved from 0.04374 to 0.04347, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 15s 260ms/step - loss: 0.0443 - accuracy: 0.9978 - val_loss: 0.0435 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 401/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9978\n","Epoch 401: val_loss did not improve from 0.04347\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0442 - accuracy: 0.9978 - val_loss: 0.0437 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 402/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9979\n","Epoch 402: val_loss improved from 0.04347 to 0.04298, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0438 - accuracy: 0.9979 - val_loss: 0.0430 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 403/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9980\n","Epoch 403: val_loss did not improve from 0.04298\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0443 - accuracy: 0.9980 - val_loss: 0.0438 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 404/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9975\n","Epoch 404: val_loss improved from 0.04298 to 0.04224, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0447 - accuracy: 0.9975 - val_loss: 0.0422 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 405/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9975\n","Epoch 405: val_loss did not improve from 0.04224\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0457 - accuracy: 0.9975 - val_loss: 0.0436 - val_accuracy: 0.9959 - lr: 1.0000e-04\n","Epoch 406/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9977\n","Epoch 406: val_loss did not improve from 0.04224\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0440 - accuracy: 0.9977 - val_loss: 0.0434 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 407/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9978\n","Epoch 407: val_loss improved from 0.04224 to 0.04175, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0456 - accuracy: 0.9978 - val_loss: 0.0418 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 408/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9979\n","Epoch 408: val_loss did not improve from 0.04175\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0429 - accuracy: 0.9979 - val_loss: 0.0430 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 409/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9980\n","Epoch 409: val_loss did not improve from 0.04175\n","56/56 [==============================] - 14s 249ms/step - loss: 0.0426 - accuracy: 0.9980 - val_loss: 0.0419 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 410/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9980\n","Epoch 410: val_loss improved from 0.04175 to 0.04172, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0437 - accuracy: 0.9980 - val_loss: 0.0417 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 411/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9980\n","Epoch 411: val_loss improved from 0.04172 to 0.04141, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0439 - accuracy: 0.9980 - val_loss: 0.0414 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 412/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9980\n","Epoch 412: val_loss did not improve from 0.04141\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0459 - accuracy: 0.9980 - val_loss: 0.0416 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 413/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9980\n","Epoch 413: val_loss did not improve from 0.04141\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0417 - accuracy: 0.9980 - val_loss: 0.0415 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 414/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9980\n","Epoch 414: val_loss improved from 0.04141 to 0.04117, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0430 - accuracy: 0.9980 - val_loss: 0.0412 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 415/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9980\n","Epoch 415: val_loss did not improve from 0.04117\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0418 - accuracy: 0.9980 - val_loss: 0.0413 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 416/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9981\n","Epoch 416: val_loss did not improve from 0.04117\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0409 - accuracy: 0.9981 - val_loss: 0.0413 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 417/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9981\n","Epoch 417: val_loss improved from 0.04117 to 0.04074, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0415 - accuracy: 0.9981 - val_loss: 0.0407 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 418/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9981\n","Epoch 418: val_loss improved from 0.04074 to 0.04000, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0404 - accuracy: 0.9981 - val_loss: 0.0400 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 419/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9981\n","Epoch 419: val_loss did not improve from 0.04000\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0418 - accuracy: 0.9981 - val_loss: 0.0401 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 420/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9982\n","Epoch 420: val_loss improved from 0.04000 to 0.03995, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0404 - accuracy: 0.9982 - val_loss: 0.0399 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 421/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9979\n","Epoch 421: val_loss improved from 0.03995 to 0.03948, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.0424 - accuracy: 0.9979 - val_loss: 0.0395 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 422/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9980\n","Epoch 422: val_loss did not improve from 0.03948\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0415 - accuracy: 0.9980 - val_loss: 0.0403 - val_accuracy: 0.9964 - lr: 1.0000e-04\n","Epoch 423/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9981\n","Epoch 423: val_loss did not improve from 0.03948\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0397 - accuracy: 0.9981 - val_loss: 0.0405 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 424/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9981\n","Epoch 424: val_loss did not improve from 0.03948\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0417 - accuracy: 0.9981 - val_loss: 0.0398 - val_accuracy: 0.9963 - lr: 1.0000e-04\n","Epoch 425/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9982\n","Epoch 425: val_loss did not improve from 0.03948\n","56/56 [==============================] - 14s 248ms/step - loss: 0.0411 - accuracy: 0.9982 - val_loss: 0.0400 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 426/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9982\n","Epoch 426: val_loss improved from 0.03948 to 0.03901, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0397 - accuracy: 0.9982 - val_loss: 0.0390 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 427/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9981\n","Epoch 427: val_loss improved from 0.03901 to 0.03890, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.0424 - accuracy: 0.9981 - val_loss: 0.0389 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 428/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9983\n","Epoch 428: val_loss improved from 0.03890 to 0.03887, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0391 - accuracy: 0.9983 - val_loss: 0.0389 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 429/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9982\n","Epoch 429: val_loss improved from 0.03887 to 0.03822, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 255ms/step - loss: 0.0394 - accuracy: 0.9982 - val_loss: 0.0382 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 430/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9983\n","Epoch 430: val_loss did not improve from 0.03822\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0384 - accuracy: 0.9983 - val_loss: 0.0386 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 431/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9982\n","Epoch 431: val_loss improved from 0.03822 to 0.03807, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0395 - accuracy: 0.9982 - val_loss: 0.0381 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 432/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9983\n","Epoch 432: val_loss improved from 0.03807 to 0.03787, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0377 - accuracy: 0.9983 - val_loss: 0.0379 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 433/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9983\n","Epoch 433: val_loss improved from 0.03787 to 0.03772, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0401 - accuracy: 0.9983 - val_loss: 0.0377 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 434/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9983\n","Epoch 434: val_loss did not improve from 0.03772\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0402 - accuracy: 0.9983 - val_loss: 0.0380 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 435/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9983\n","Epoch 435: val_loss did not improve from 0.03772\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0391 - accuracy: 0.9983 - val_loss: 0.0384 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 436/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9983\n","Epoch 436: val_loss did not improve from 0.03772\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0370 - accuracy: 0.9983 - val_loss: 0.0378 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 437/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9983\n","Epoch 437: val_loss did not improve from 0.03772\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0367 - accuracy: 0.9983 - val_loss: 0.0380 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 438/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9984\n","Epoch 438: val_loss improved from 0.03772 to 0.03685, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 15s 261ms/step - loss: 0.0367 - accuracy: 0.9984 - val_loss: 0.0368 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 439/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9983\n","Epoch 439: val_loss improved from 0.03685 to 0.03643, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0384 - accuracy: 0.9983 - val_loss: 0.0364 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 440/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9983\n","Epoch 440: val_loss did not improve from 0.03643\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0388 - accuracy: 0.9983 - val_loss: 0.0372 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 441/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9983\n","Epoch 441: val_loss did not improve from 0.03643\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0360 - accuracy: 0.9983 - val_loss: 0.0370 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 442/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9975\n","Epoch 442: val_loss did not improve from 0.03643\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0366 - accuracy: 0.9975 - val_loss: 0.0378 - val_accuracy: 0.9953 - lr: 1.0000e-04\n","Epoch 443/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9971\n","Epoch 443: val_loss did not improve from 0.03643\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0397 - accuracy: 0.9971 - val_loss: 0.0408 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 444/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9979\n","Epoch 444: val_loss did not improve from 0.03643\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0385 - accuracy: 0.9979 - val_loss: 0.0384 - val_accuracy: 0.9965 - lr: 1.0000e-04\n","Epoch 445/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9982\n","Epoch 445: val_loss did not improve from 0.03643\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0385 - accuracy: 0.9982 - val_loss: 0.0372 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 446/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9983\n","Epoch 446: val_loss improved from 0.03643 to 0.03594, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0360 - accuracy: 0.9983 - val_loss: 0.0359 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 447/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9983\n","Epoch 447: val_loss improved from 0.03594 to 0.03588, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0373 - accuracy: 0.9983 - val_loss: 0.0359 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 448/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9984\n","Epoch 448: val_loss improved from 0.03588 to 0.03577, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0358 - accuracy: 0.9984 - val_loss: 0.0358 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 449/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9984\n","Epoch 449: val_loss improved from 0.03577 to 0.03527, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0354 - accuracy: 0.9984 - val_loss: 0.0353 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 450/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9984\n","Epoch 450: val_loss improved from 0.03527 to 0.03527, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.0361 - accuracy: 0.9984 - val_loss: 0.0353 - val_accuracy: 0.9966 - lr: 1.0000e-04\n","Epoch 451/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9983\n","Epoch 451: val_loss improved from 0.03527 to 0.03435, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0352 - accuracy: 0.9983 - val_loss: 0.0344 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 452/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9984\n","Epoch 452: val_loss did not improve from 0.03435\n","56/56 [==============================] - 14s 253ms/step - loss: 0.0348 - accuracy: 0.9984 - val_loss: 0.0355 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 453/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9984\n","Epoch 453: val_loss did not improve from 0.03435\n","56/56 [==============================] - 14s 245ms/step - loss: 0.0352 - accuracy: 0.9984 - val_loss: 0.0351 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 454/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9984\n","Epoch 454: val_loss improved from 0.03435 to 0.03421, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 258ms/step - loss: 0.0341 - accuracy: 0.9984 - val_loss: 0.0342 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 455/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9984\n","Epoch 455: val_loss did not improve from 0.03421\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0365 - accuracy: 0.9984 - val_loss: 0.0359 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 456/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9985\n","Epoch 456: val_loss did not improve from 0.03421\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0333 - accuracy: 0.9985 - val_loss: 0.0349 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 457/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9985\n","Epoch 457: val_loss did not improve from 0.03421\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0346 - accuracy: 0.9985 - val_loss: 0.0357 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 458/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9985\n","Epoch 458: val_loss improved from 0.03421 to 0.03374, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 257ms/step - loss: 0.0334 - accuracy: 0.9985 - val_loss: 0.0337 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 459/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9986\n","Epoch 459: val_loss did not improve from 0.03374\n","56/56 [==============================] - 14s 248ms/step - loss: 0.0333 - accuracy: 0.9986 - val_loss: 0.0340 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 460/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9985\n","Epoch 460: val_loss improved from 0.03374 to 0.03357, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0339 - accuracy: 0.9985 - val_loss: 0.0336 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 461/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9985\n","Epoch 461: val_loss did not improve from 0.03357\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0339 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 462/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9984\n","Epoch 462: val_loss did not improve from 0.03357\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0350 - accuracy: 0.9984 - val_loss: 0.0352 - val_accuracy: 0.9960 - lr: 1.0000e-04\n","Epoch 463/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9983\n","Epoch 463: val_loss improved from 0.03357 to 0.03354, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 259ms/step - loss: 0.0340 - accuracy: 0.9983 - val_loss: 0.0335 - val_accuracy: 0.9968 - lr: 1.0000e-04\n","Epoch 464/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9984\n","Epoch 464: val_loss did not improve from 0.03354\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0347 - accuracy: 0.9984 - val_loss: 0.0339 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 465/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9985\n","Epoch 465: val_loss did not improve from 0.03354\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0329 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 466/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9986\n","Epoch 466: val_loss improved from 0.03354 to 0.03331, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0317 - accuracy: 0.9986 - val_loss: 0.0333 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 467/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9986\n","Epoch 467: val_loss improved from 0.03331 to 0.03274, saving model to unet_hybrid.hdf5\n","56/56 [==============================] - 14s 254ms/step - loss: 0.0332 - accuracy: 0.9986 - val_loss: 0.0327 - val_accuracy: 0.9970 - lr: 1.0000e-04\n","Epoch 468/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9986\n","Epoch 468: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0328 - accuracy: 0.9986 - val_loss: 0.0339 - val_accuracy: 0.9969 - lr: 1.0000e-04\n","Epoch 469/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9986\n","Epoch 469: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0325 - accuracy: 0.9986 - val_loss: 0.0337 - val_accuracy: 0.9967 - lr: 1.0000e-04\n","Epoch 470/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9909\n","Epoch 470: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 248ms/step - loss: 0.0515 - accuracy: 0.9909 - val_loss: 0.0520 - val_accuracy: 0.9863 - lr: 1.0000e-04\n","Epoch 471/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9942\n","Epoch 471: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0417 - accuracy: 0.9942 - val_loss: 0.0467 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 472/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9964\n","Epoch 472: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 248ms/step - loss: 0.0358 - accuracy: 0.9964 - val_loss: 0.0372 - val_accuracy: 0.9952 - lr: 1.0000e-04\n","Epoch 473/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9952\n","Epoch 473: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0395 - accuracy: 0.9952 - val_loss: 0.0584 - val_accuracy: 0.9847 - lr: 1.0000e-04\n","Epoch 474/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9965\n","Epoch 474: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 247ms/step - loss: 0.0365 - accuracy: 0.9965 - val_loss: 0.0386 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 475/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9974\n","Epoch 475: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0334 - accuracy: 0.9974 - val_loss: 0.0363 - val_accuracy: 0.9961 - lr: 1.0000e-04\n","Epoch 476/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9972\n","Epoch 476: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 250ms/step - loss: 0.0347 - accuracy: 0.9972 - val_loss: 0.0386 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 477/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9977\n","Epoch 477: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n","\n","Epoch 477: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 248ms/step - loss: 0.0320 - accuracy: 0.9977 - val_loss: 0.0343 - val_accuracy: 0.9962 - lr: 1.0000e-04\n","Epoch 478/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9978\n","Epoch 478: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0344 - accuracy: 0.9978 - val_loss: 0.0335 - val_accuracy: 0.9964 - lr: 1.0000e-05\n","Epoch 479/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9980\n","Epoch 479: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 251ms/step - loss: 0.0336 - accuracy: 0.9980 - val_loss: 0.0331 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 480/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9979\n","Epoch 480: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 246ms/step - loss: 0.0342 - accuracy: 0.9979 - val_loss: 0.0329 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 481/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9980\n","Epoch 481: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 252ms/step - loss: 0.0326 - accuracy: 0.9980 - val_loss: 0.0328 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 482/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9981\n","Epoch 482: val_loss did not improve from 0.03274\n","56/56 [==============================] - 14s 245ms/step - loss: 0.0330 - accuracy: 0.9981 - val_loss: 0.0327 - val_accuracy: 0.9965 - lr: 1.0000e-05\n","Epoch 482: early stopping\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fa56e038c10>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from keras.callbacks import ReduceLROnPlateau\n","reduce_lr=ReduceLROnPlateau(monitor='val_loss',\n","                         factor=0.1,\n","                         patience=10,\n","                         verbose=1,\n","                         mode='auto',\n","                         min_delta=0.00003,\n","                         cooldown=0,\n","                         min_lr=0)\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15,verbose=1,mode='min')\n","save_model= ModelCheckpoint('unet_hybrid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","unet.fit(images, masks, validation_data=(val_images,val_masks), batch_size=16, epochs=1000,verbose=1,shuffle=True,callbacks=[reduce_lr,save_model,early_stop])"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:27:02.115977Z","iopub.status.busy":"2023-04-03T11:27:02.114903Z","iopub.status.idle":"2023-04-03T11:27:02.123804Z","shell.execute_reply":"2023-04-03T11:27:02.122773Z","shell.execute_reply.started":"2023-04-03T11:27:02.115938Z"},"trusted":true},"outputs":[],"source":["np.save('unet_hybrid-history.npy',unet.history.history)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:27:04.642597Z","iopub.status.busy":"2023-04-03T11:27:04.641950Z","iopub.status.idle":"2023-04-03T11:27:04.649006Z","shell.execute_reply":"2023-04-03T11:27:04.647655Z","shell.execute_reply.started":"2023-04-03T11:27:04.642558Z"},"trusted":true},"outputs":[],"source":["model_history = np.load('unet_hybrid-history.npy', allow_pickle='TRUE').item()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:27:10.171519Z","iopub.status.busy":"2023-04-03T11:27:10.171142Z","iopub.status.idle":"2023-04-03T11:27:48.979110Z","shell.execute_reply":"2023-04-03T11:27:48.977988Z","shell.execute_reply.started":"2023-04-03T11:27:10.171486Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<matplotlib.legend.Legend at 0x7fa56660e750>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#@title Plot accuracy and loss \n","from matplotlib import pyplot as plt\n","## Accuracy\n","plt.plot(model_history['accuracy'])\n","#  plot(x(1:5:100),y2(1:5:100),'-ro','markersize',3)\n","plt.plot(model_history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')\n","plt.show()\n","\n","## Loss\n","plt.plot(model_history['loss'])\n","plt.plot(model_history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:29:03.466302Z","iopub.status.busy":"2023-04-03T11:29:03.465512Z","iopub.status.idle":"2023-04-03T11:29:03.476416Z","shell.execute_reply":"2023-04-03T11:29:03.475422Z","shell.execute_reply.started":"2023-04-03T11:29:03.466264Z"},"trusted":true},"outputs":[],"source":["#@title Returns an image or array plot of mask prediction\n","\n","def reconstruct_image(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","  # Standardise between 0-1\n","  reconstruction = reconstruction/np.max(reconstruction)\n","\n","  # Round to 0-1, binary pixel-by-pixel classification \n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  # Plot reconstructed mask (prediction)\n","  plt.imshow(reconstruction) \n","'''\n","  Returns array of mask prediction, given model and image\n","'''\n","def reconstruct_array(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","\n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  return reconstruction # Returns array"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:29:07.177675Z","iopub.status.busy":"2023-04-03T11:29:07.177230Z","iopub.status.idle":"2023-04-03T11:29:07.198448Z","shell.execute_reply":"2023-04-03T11:29:07.197345Z","shell.execute_reply.started":"2023-04-03T11:29:07.177639Z"},"trusted":true},"outputs":[],"source":["#@title Metric functions for evaluation\n","\n","def score_eval(model, image, mask): # Gives score of mask vs prediction\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","    return accuracy_score(mask.flatten(), reconstruction)\n","\n","  else: # If a list of images input, find accuracy for each\n","    scores = []\n","    for i in range(len(image)):\n","      reconstruction = model.predict(image[i].reshape(1, 256, 256, 10))\n","      reconstruction = np.round(reconstruction).flatten()\n","      scores.append(accuracy_score(mask[i].flatten(), reconstruction))\n","\n","    return scores\n","\n","def recall_eval(model, image, mask): # Find recall score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return recall_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    recall = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        recall.append(recall_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return recall\n","\n","def precision_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return precision_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    precision = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        precision.append(precision_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return precision\n","\n","def iou_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return jaccard_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    iou = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        iou.append(jaccard_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return iou\n","\n","def f1_score_eval(model, image, mask): # Find F1-score\n","    prec = np.mean(precision_eval(model, image, mask))\n","    rec = np.mean(recall_eval(model, image, mask))\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def f1_score_eval_basic(precision, recall):\n","    prec = np.mean(precision)\n","    rec = np.mean(recall)\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def produce_mask(image): # Outputs rounded image (binary)\n","  return np.round(image)\n"," \n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:29:11.327753Z","iopub.status.busy":"2023-04-03T11:29:11.327166Z","iopub.status.idle":"2023-04-03T11:30:44.804791Z","shell.execute_reply":"2023-04-03T11:30:44.803776Z","shell.execute_reply.started":"2023-04-03T11:29:11.327718Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 738ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 50ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 51ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 57ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 32ms/step\n"]}],"source":["\n","score = (score_eval(unet, test_images, test_masks))\n","\n","precision = (precision_eval(unet, test_images, test_masks))\n","\n","recall = (recall_eval(unet, test_images, test_masks))\n","\n","iou = (iou_eval(unet, test_images, test_masks))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:31:17.334128Z","iopub.status.busy":"2023-04-03T11:31:17.333088Z","iopub.status.idle":"2023-04-03T11:31:17.339837Z","shell.execute_reply":"2023-04-03T11:31:17.338728Z","shell.execute_reply.started":"2023-04-03T11:31:17.334082Z"},"trusted":true},"outputs":[],"source":["f1_score = (f1_score_eval_basic(precision, recall))"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-04-03T11:31:22.639532Z","iopub.status.busy":"2023-04-03T11:31:22.638621Z","iopub.status.idle":"2023-04-03T11:31:22.647169Z","shell.execute_reply":"2023-04-03T11:31:22.646091Z","shell.execute_reply.started":"2023-04-03T11:31:22.639494Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model accuracy:  0.9858446049510985 0.012829588235729855\n","model precision:  0.9622277119763596 0.051625791206861754\n","model recall:  0.9783590487403081 0.03276676545727119\n","model F1-score:  0.9702263336199299\n","model iou:  0.9427292670738204 0.0620185867696936\n"]}],"source":["print('model accuracy: ', np.mean(score), np.std(score))\n","print('model precision: ', np.mean(precision), np.std(precision))\n","print('model recall: ', np.mean(recall), np.std(recall))\n","print('model F1-score: ', np.mean(f1_score))\n","print('model iou: ', np.mean(iou), np.std(iou))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
