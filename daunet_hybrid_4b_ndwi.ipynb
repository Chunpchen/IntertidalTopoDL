{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-06T14:58:21.396608Z","iopub.status.busy":"2023-04-06T14:58:21.395979Z","iopub.status.idle":"2023-04-06T14:59:00.719682Z","shell.execute_reply":"2023-04-06T14:59:00.718515Z","shell.execute_reply.started":"2023-04-06T14:58:21.396554Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/davej23/image-segmentation-keras.git\n","  Cloning https://github.com/davej23/image-segmentation-keras.git to /tmp/pip-req-build-4wkv8nx4\n","  Running command git clone --filter=blob:none --quiet https://github.com/davej23/image-segmentation-keras.git /tmp/pip-req-build-4wkv8nx4\n","  Resolved https://github.com/davej23/image-segmentation-keras.git to commit e01b0a8d5859854cd9d259a618829889166439f5\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting rarfile\n","  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n","Collecting segmentation-models\n","  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n","Collecting rioxarray\n","  Downloading rioxarray-0.9.1.tar.gz (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting keras-applications<=1.0.8,>=1.0.7\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting efficientnet==1.0.0\n","  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n","Collecting image-classifiers==1.0.0\n","  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n","Collecting h5py<=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: Keras>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (2.11.0)\n","Collecting imageio==2.5.0\n","  Downloading imageio-2.5.0-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (0.4.0)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.5.4.60)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-segmentation==0.3.0) (4.64.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (1.21.6)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio==2.5.0->keras-segmentation==0.3.0) (9.4.0)\n","Requirement already satisfied: pyproj>=2.2 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (3.1.0)\n","Requirement already satisfied: rasterio in /opt/conda/lib/python3.7/site-packages (from rioxarray) (1.2.10)\n","Requirement already satisfied: xarray>=0.17 in /opt/conda/lib/python3.7/site-packages (from rioxarray) (0.20.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from rioxarray) (23.0)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py<=2.10.0->keras-segmentation==0.3.0) (1.16.0)\n","Requirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.8.0)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (3.5.3)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->keras-segmentation==0.3.0) (1.7.3)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from pyproj>=2.2->rioxarray) (2022.12.7)\n","Requirement already satisfied: pandas>=1.1 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (1.3.5)\n","Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.4.0)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from xarray>=0.17->rioxarray) (4.11.4)\n","Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (0.7.2)\n","Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (8.1.3)\n","Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.4.7)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (59.8.0)\n","Requirement already satisfied: click-plugins in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (1.1.1)\n","Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (22.2.0)\n","Requirement already satisfied: affine in /opt/conda/lib/python3.7/site-packages (from rasterio->rioxarray) (2.4.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1->xarray>=0.17->rioxarray) (2023.2)\n","Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.11.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.0)\n","Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n","Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio->rioxarray) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->xarray>=0.17->rioxarray) (3.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (4.38.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->keras-segmentation==0.3.0) (1.4.4)\n","Building wheels for collected packages: keras-segmentation, rioxarray\n","  Building wheel for keras-segmentation (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for keras-segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34377 sha256=6e63111f4e8b4d6cc795588cd9d5d3a2fb31a207c77097a318fc22338f2e6547\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-m0cje_3h/wheels/f4/fb/07/8f81ceb3d9fe936f5e4dcd1a64cbc489e42e6e7f9c2f166785\n","  Building wheel for rioxarray (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rioxarray: filename=rioxarray-0.9.1-py3-none-any.whl size=54590 sha256=27f734e595a3457bd422ddde5909f86bfa52895b2d555b33a488d203732b63f2\n","  Stored in directory: /root/.cache/pip/wheels/03/b2/26/2e2cc1797ac99cc070d2cae87c340bd3429bbb583c90b1c780\n","Successfully built keras-segmentation rioxarray\n","Installing collected packages: rarfile, imageio, h5py, keras-applications, image-classifiers, efficientnet, segmentation-models, keras-segmentation, rioxarray\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.25.0\n","    Uninstalling imageio-2.25.0:\n","      Successfully uninstalled imageio-2.25.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n","tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed efficientnet-1.0.0 h5py-2.10.0 image-classifiers-1.0.0 imageio-2.5.0 keras-applications-1.0.8 keras-segmentation-0.3.0 rarfile-4.0 rioxarray-0.9.1 segmentation-models-1.0.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#@title import packages\n","import keras\n","import numpy as np\n","import os\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.losses import *\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint\n","import sys\n","\n","!pip install rarfile segmentation-models git+https://github.com/davej23/image-segmentation-keras.git rioxarray\n","from rarfile import RarFile\n","from sklearn.metrics import *\n","import rioxarray as rxr"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T14:59:32.119254Z","iopub.status.busy":"2023-04-06T14:59:32.118354Z","iopub.status.idle":"2023-04-06T15:01:12.694947Z","shell.execute_reply":"2023-04-06T15:01:12.693876Z","shell.execute_reply.started":"2023-04-06T14:59:32.119211Z"},"trusted":true},"outputs":[],"source":["base_dir = r\"/content/gdrive/MyDrive/mudtrain/\"\n","#@title Read training images and normalise\n","training_images_list = os.listdir(r\"{}train/images/\".format(base_dir))\n","training_masks_list = []\n","training_images = []\n","training_originalimages = []\n","for n in training_images_list:\n","  training_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}train/images/{}\".format(base_dir,n))))\n","  training_originalimages.append(a)\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  training_images.append(a)\n","\n","## Training masks\n","training_masks = []\n","for n in training_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}train/labels/{}\".format(base_dir,n))))\n","  training_masks.append(a)\n","\n","## Validation images\n","validation_images_list = os.listdir(r\"{}val/images/\".format(base_dir))\n","validation_masks_list = []\n","validation_images = []\n","validation_originalimages = []\n","for n in validation_images_list:\n","  validation_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}val/images/{}\".format(base_dir,n))))\n","  validation_originalimages.append(a)\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  validation_images.append(a)\n","\n","## Validation masks\n","validation_masks = []\n","for n in validation_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}val/labels/{}\".format(base_dir,n))))\n","  validation_masks.append(a)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:02:30.374879Z","iopub.status.busy":"2023-04-06T15:02:30.374181Z","iopub.status.idle":"2023-04-06T15:02:31.578710Z","shell.execute_reply":"2023-04-06T15:02:31.577502Z","shell.execute_reply.started":"2023-04-06T15:02:30.374842Z"},"trusted":true},"outputs":[],"source":["#@title Pre-process data, reshaping and transposing\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].astype('float32')\n","  training_images[i] = training_images[i].T\n","    \n","  training_originalimages[i] = training_originalimages[i].astype('float32')\n","  training_originalimages[i] = training_originalimages[i].T\n","\n","for i in range(len(training_masks)):\n","  training_masks[i] = training_masks[i].reshape(1,256,256)\n","  training_masks[i] = training_masks[i].T\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].astype('float32')\n","  validation_images[i] = validation_images[i].T\n","    \n","  validation_originalimages[i] = validation_originalimages[i].astype('float32')\n","  validation_originalimages[i] = validation_originalimages[i].T\n","\n","for i in range(len(validation_masks)):\n","  validation_masks[i] = validation_masks[i].reshape(1,256,256)\n","  validation_masks[i] = validation_masks[i].T\n","\n","\n","for i in range(len(training_images)):\n","  training_images[i] = training_images[i].reshape(256,256,10)\n","  training_originalimages[i] = training_originalimages[i].reshape(256,256,10)\n","\n","for i in range(len(validation_images)):\n","  validation_images[i] = validation_images[i].reshape(256,256,10)\n","  validation_originalimages[i] = validation_originalimages[i].reshape(256,256,10)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:03:15.566074Z","iopub.status.busy":"2023-04-06T15:03:15.565697Z","iopub.status.idle":"2023-04-06T15:03:21.680493Z","shell.execute_reply":"2023-04-06T15:03:21.679216Z","shell.execute_reply.started":"2023-04-06T15:03:15.566042Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 10)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["images=np.vstack([training_images])\n","val_images=np.vstack([validation_images])\n","images.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:02:40.266772Z","iopub.status.busy":"2023-04-06T15:02:40.265802Z","iopub.status.idle":"2023-04-06T15:02:48.446940Z","shell.execute_reply":"2023-04-06T15:02:48.445781Z","shell.execute_reply.started":"2023-04-06T15:02:40.266733Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 10)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["originalimages=np.vstack([training_originalimages])\n","val_originalimages=np.vstack([validation_originalimages])\n","originalimages.shape"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:03:59.394387Z","iopub.status.busy":"2023-04-06T15:03:59.394005Z","iopub.status.idle":"2023-04-06T15:03:59.555225Z","shell.execute_reply":"2023-04-06T15:03:59.554078Z","shell.execute_reply.started":"2023-04-06T15:03:59.394352Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 1)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["masks=np.vstack([training_masks])\n","val_masks=np.vstack([validation_masks])\n","val_masks.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:02:53.832943Z","iopub.status.busy":"2023-04-06T15:02:53.832573Z","iopub.status.idle":"2023-04-06T15:02:54.432594Z","shell.execute_reply":"2023-04-06T15:02:54.431488Z","shell.execute_reply.started":"2023-04-06T15:02:53.832910Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# calculate ndwi\n","epsilon=1e-8 #small value to avoid division by zero errors\n","ndwi=(originalimages[...,1]-originalimages[...,6])/(originalimages[...,1]+originalimages[...,6]+epsilon)\n","ndwi.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:02:57.771363Z","iopub.status.busy":"2023-04-06T15:02:57.770926Z","iopub.status.idle":"2023-04-06T15:02:57.918495Z","shell.execute_reply":"2023-04-06T15:02:57.917108Z","shell.execute_reply.started":"2023-04-06T15:02:57.771328Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# calculate ndwi\n","valndwi=(val_originalimages[...,1]-val_originalimages[...,6])/(val_originalimages[...,1]+val_originalimages[...,6]+epsilon)\n","valndwi.shape"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:03:06.583950Z","iopub.status.busy":"2023-04-06T15:03:06.583575Z","iopub.status.idle":"2023-04-06T15:03:06.703983Z","shell.execute_reply":"2023-04-06T15:03:06.702880Z","shell.execute_reply.started":"2023-04-06T15:03:06.583917Z"},"trusted":true},"outputs":[],"source":["import gc\n","del originalimages,training_originalimages,test_originalimages,validation_originalimages"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:03:35.918825Z","iopub.status.busy":"2023-04-06T15:03:35.918404Z","iopub.status.idle":"2023-04-06T15:03:36.900139Z","shell.execute_reply":"2023-04-06T15:03:36.898915Z","shell.execute_reply.started":"2023-04-06T15:03:35.918790Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 4)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["bands=[0,1,2,6] # Blue, Green, Red, and NIR band\n","images_4b=images[...,bands]\n","images_4b.shape"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:03:39.417297Z","iopub.status.busy":"2023-04-06T15:03:39.416927Z","iopub.status.idle":"2023-04-06T15:03:39.659457Z","shell.execute_reply":"2023-04-06T15:03:39.658140Z","shell.execute_reply.started":"2023-04-06T15:03:39.417265Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 4)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["val_images_4b=val_images[...,bands]\n","val_images_4b.shape"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:03:54.118566Z","iopub.status.busy":"2023-04-06T15:03:54.117517Z","iopub.status.idle":"2023-04-06T15:03:54.202982Z","shell.execute_reply":"2023-04-06T15:03:54.201881Z","shell.execute_reply.started":"2023-04-06T15:03:54.118515Z"},"trusted":true},"outputs":[],"source":["del images,val_images"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:03:47.422468Z","iopub.status.busy":"2023-04-06T15:03:47.422056Z","iopub.status.idle":"2023-04-06T15:03:48.349064Z","shell.execute_reply":"2023-04-06T15:03:48.348095Z","shell.execute_reply.started":"2023-04-06T15:03:47.422434Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(889, 256, 256, 5)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["newimages_4b_ndwi=np.concatenate([images_4b,np.expand_dims(ndwi,axis=-1)],axis=-1)\n","newimages_4b_ndwi.shape"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:03:50.854371Z","iopub.status.busy":"2023-04-06T15:03:50.853683Z","iopub.status.idle":"2023-04-06T15:03:51.437498Z","shell.execute_reply":"2023-04-06T15:03:51.436498Z","shell.execute_reply.started":"2023-04-06T15:03:50.854333Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(223, 256, 256, 5)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["newvalimages_4b_ndwi=np.concatenate([val_images_4b,np.expand_dims(valndwi,axis=-1)],axis=-1)\n","newvalimages_4b_ndwi.shape"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:04:08.732172Z","iopub.status.busy":"2023-04-06T15:04:08.731186Z","iopub.status.idle":"2023-04-06T15:04:08.740373Z","shell.execute_reply":"2023-04-06T15:04:08.739178Z","shell.execute_reply.started":"2023-04-06T15:04:08.732131Z"},"trusted":true},"outputs":[],"source":["del images_4b,val_images_4b"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:04:11.421499Z","iopub.status.busy":"2023-04-06T15:04:11.421098Z","iopub.status.idle":"2023-04-06T15:04:11.665453Z","shell.execute_reply":"2023-04-06T15:04:11.664363Z","shell.execute_reply.started":"2023-04-06T15:04:11.421444Z"},"trusted":true},"outputs":[{"data":{"text/plain":["1136"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del training_images,validation_images,training_masks,validation_masks,training_images_list,validation_images_list,\n","training_masks_list,validation_masks_list\n","gc.collect()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T11:03:29.573400Z","iopub.status.busy":"2023-04-06T11:03:29.572767Z","iopub.status.idle":"2023-04-06T11:03:29.612007Z","shell.execute_reply":"2023-04-06T11:03:29.610718Z","shell.execute_reply.started":"2023-04-06T11:03:29.573359Z"},"trusted":true},"outputs":[],"source":["del images,val_images"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:04:19.778160Z","iopub.status.busy":"2023-04-06T15:04:19.777142Z","iopub.status.idle":"2023-04-06T15:04:19.790982Z","shell.execute_reply":"2023-04-06T15:04:19.789851Z","shell.execute_reply.started":"2023-04-06T15:04:19.778121Z"},"trusted":true},"outputs":[],"source":["#@title boundary loss\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import models\n","\n","#Shape of semantic segmentation mask\n","OUTPUT_SHAPE = (256, 256, 1)\n","def boundary_loss(y_true, y_pred):\n","\n","    \"\"\"\n","    Paper Implemented : https://arxiv.org/abs/1905.07852\n","    Using Binary Segmentation mask, generates boundary mask on fly and claculates boundary loss.\n","    :param y_true:\n","    :param y_pred:\n","    :return:\n","    \"\"\"\n","    y_true=tf.cast(y_true,tf.float32)\n","    y_pred=tf.cast(y_pred,tf.float32)\n","    \n","    y_pred_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd = y_pred_bd - (1 - y_pred)\n","    y_true_bd = y_true_bd - (1 - y_true)\n","\n","    y_pred_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_pred)\n","    y_true_bd_ext = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', input_shape=OUTPUT_SHAPE)(1 - y_true)\n","    y_pred_bd_ext = y_pred_bd_ext - (1 - y_pred)\n","    y_true_bd_ext = y_true_bd_ext - (1 - y_true)\n","\n","    P = K.sum(y_pred_bd * y_true_bd_ext) / K.sum(y_pred_bd) + 1e-7\n","    R = K.sum(y_true_bd * y_pred_bd_ext) / K.sum(y_true_bd) + 1e-7\n","    F1_Score = 2 * P * R / (P + R + 1e-7)\n","    loss = K.mean(1 - F1_Score)\n","    \n","    return loss"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:04:23.364375Z","iopub.status.busy":"2023-04-06T15:04:23.363390Z","iopub.status.idle":"2023-04-06T15:04:25.707267Z","shell.execute_reply":"2023-04-06T15:04:25.706170Z","shell.execute_reply.started":"2023-04-06T15:04:23.364337Z"},"trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, Callback\n","class AlphaScheduler(Callback):\n","  def init(self, alpha, update_fn):\n","    self.alpha = alpha\n","    self.update_fn = update_fn\n","  def on_epoch_end(self, epoch, logs=None):\n","    updated_alpha = self.update_fn(K.get_value(self.alpha))\n","\n","alpha = K.variable(1, dtype='float32')\n","\n","def update_alpha(value):\n","  return np.clip(value - 0.005, 0.005, 1)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:04:27.580980Z","iopub.status.busy":"2023-04-06T15:04:27.579862Z","iopub.status.idle":"2023-04-06T15:04:27.587259Z","shell.execute_reply":"2023-04-06T15:04:27.586091Z","shell.execute_reply.started":"2023-04-06T15:04:27.580936Z"},"trusted":true},"outputs":[],"source":["def gl_sl_wrapper(alpha):\n","    def gl_sl(y_true, y_pred):\n","        return alpha*keras.losses.binary_crossentropy(y_true, y_pred) +  (1-alpha)* boundary_loss(y_true, y_pred)\n","    return gl_sl"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:04:31.396021Z","iopub.status.busy":"2023-04-06T15:04:31.395628Z","iopub.status.idle":"2023-04-06T15:04:31.420998Z","shell.execute_reply":"2023-04-06T15:04:31.419689Z","shell.execute_reply.started":"2023-04-06T15:04:31.395987Z"},"trusted":true},"outputs":[],"source":[" \n","def spatial_pool(x, mode, ratio=4):\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n"," \n","    if channel_axis == -1:\n","        batch, height, width, channels = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels//2\n","        input_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        input_x = Reshape((width*height, channel))(input_x)\n"," \n","        context_mask = Conv2D(1, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        context_mask = Reshape((width*height, 1))(context_mask)\n","        context_mask = Softmax(axis=1)(context_mask)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1], transpose_a=True))([input_x, context_mask])\n","        context = Permute((2, 1))(context)\n","        context = Reshape((1, 1, channel))(context)\n"," \n","    else:\n","        batch, channels, height, width = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels // 2\n","        input_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False,\n","                         kernel_initializer='he_normal')(x)\n","        input_x = Reshape((channel, width * height))(input_x)\n"," \n","        context_mask = Conv2D(1, kernel_size=1, strides=1, padding='same', use_bias=False,\n","                              kernel_initializer='he_normal')(x)\n","        context_mask = Reshape((width * height, 1))(context_mask)\n","        context_mask = Softmax(axis=1)(context_mask)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1]))([input_x, context_mask])\n","        context = Reshape((channel, 1, 1))(context)\n"," \n","    if mode == 'p':\n","        context = Conv2D(channels, kernel_size=1, strides=1, padding='same')(context)\n","    else:\n","        context = Conv2D(channel // ratio, kernel_size=1, strides=1, padding='same')(context)\n","        context = LayerNormalization()(context) # pip install keras-layer-normalization\n","        context = Conv2D(channels, kernel_size=1, strides=1, padding='same')(context)\n"," \n","    mask_ch = Activation('sigmoid')(context)\n","    return Multiply()([x, mask_ch])\n"," \n","def channel_pool(x):\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n","    if channel_axis == -1:\n","        batch, height, width, channels = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels//2\n","        g_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        avg_x = GlobalAvgPool2D()(g_x)\n","        avg_x = Softmax()(avg_x)\n","        avg_x = Reshape((channel, 1))(avg_x)\n"," \n","        theta_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        theta_x = Reshape((height*width, channel))(theta_x)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1]))([theta_x, avg_x])\n","        context = Reshape((height*width,))(context)\n","        mask_sp = Activation('sigmoid')(context)\n","        mask_sp = Reshape((height, width, 1))(mask_sp)\n","    else:\n","        batch, channels, height, width = K.int_shape(x)\n","        assert channels % 2 == 0\n","        channel = channels//2\n","        g_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        avg_x = GlobalAvgPool2D()(g_x)\n","        avg_x = Softmax()(avg_x)\n","        avg_x = Reshape((1, channel))(avg_x)\n"," \n","        theta_x = Conv2D(channel, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","        theta_x = Reshape((channel, height*width))(theta_x)\n","        context = Lambda(lambda x: tf.matmul(x[0], x[1]))([avg_x, theta_x])\n","        context = Reshape((height*width,))(context)\n","        mask_sp = Activation('sigmoid')(context)\n","        mask_sp = Reshape((1, height, width))(mask_sp)\n"," \n","    return Multiply()([x, mask_sp])\n","\n","def PSA(x, mode='p'):\n","    context_channel = spatial_pool(x, mode)\n","    if mode == 'p':\n","        context_spatial = channel_pool(x)\n","        out = Add()([context_spatial, context_channel])\n","    elif mode == 's':\n","        out = channel_pool(context_channel)\n","    else:\n","        out = x\n","    return out"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:04:38.743720Z","iopub.status.busy":"2023-04-06T15:04:38.743110Z","iopub.status.idle":"2023-04-06T15:04:39.094196Z","shell.execute_reply":"2023-04-06T15:04:39.093160Z","shell.execute_reply.started":"2023-04-06T15:04:38.743679Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import models, layers, regularizers\n","from tensorflow.keras import backend as K\n","\n","#convolutional block\n","def conv_block(x, kernelsize, filters, dropout, batchnorm=True): \n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    if dropout > 0:\n","        conv = layers.Dropout(dropout)(conv)\n","    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(conv)\n","    if batchnorm is True:\n","        conv = layers.BatchNormalization(axis=3)(conv)\n","    conv = layers.Activation(\"relu\")(conv)\n","    return conv\n","\n","def daunet(input_shape, dropout=0, batchnorm=True):    \n","    \n","    filters = [32,64, 128, 256,512]\n","    kernelsize = 3\n","    upsample_size = 2\n","    \n","    inputs = layers.Input(input_shape)    \n","\n","    # Downsampling layers\n","    dn_1 = conv_block(inputs, kernelsize, filters[0], dropout, batchnorm)\n","    c1=PSA(dn_1)\n","    pool_1 = layers.MaxPooling2D(pool_size=(2,2))(dn_1)\n","    \n","    dn_2 = conv_block(pool_1, kernelsize, filters[1], dropout, batchnorm)\n","    c2=PSA(dn_2)\n","    pool_2 = layers.MaxPooling2D(pool_size=(2,2))(dn_2)\n","    \n","    dn_3 = conv_block(pool_2, kernelsize, filters[2], dropout, batchnorm)\n","    c3=PSA(dn_3)\n","    pool_3 = layers.MaxPooling2D(pool_size=(2,2))(dn_3)\n","    \n","    dn_4 = conv_block(pool_3, kernelsize, filters[3], dropout, batchnorm)\n","    c4=PSA(dn_4)\n","    pool_4 = layers.MaxPooling2D(pool_size=(2,2))(dn_4)\n","    \n","    dn_5 = conv_block(pool_4, kernelsize, filters[4], dropout, batchnorm)\n","\n","    # Upsampling layers   \n","    up_5 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(dn_5)\n","    up_5 = layers.concatenate([up_5, c4], axis=3)\n","    up_conv_5 = conv_block(up_5, kernelsize, filters[3], dropout, batchnorm)\n","    \n","    up_4 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_5)\n","    up_4 = layers.concatenate([up_4, c3], axis=3)\n","    up_conv_4 = conv_block(up_4, kernelsize, filters[2], dropout, batchnorm)\n","       \n","    up_3 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_4)\n","    up_3 = layers.concatenate([up_3, c2], axis=3)\n","    up_conv_3 = conv_block(up_3, kernelsize, filters[1], dropout, batchnorm)\n","    \n","    up_2 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_3)\n","    up_2 = layers.concatenate([up_2, c1], axis=3)\n","    up_conv_2 = conv_block(up_2, kernelsize, filters[0], dropout, batchnorm)    \n","   \n","    conv_final = layers.Conv2D(1, kernel_size=(1,1))(up_conv_2)\n","    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n","    outputs = layers.Activation('sigmoid')(conv_final)  \n","\n","    model = models.Model(inputs=[inputs], outputs=[outputs])     \n","    return model"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:04:47.634699Z","iopub.status.busy":"2023-04-06T15:04:47.634286Z","iopub.status.idle":"2023-04-06T15:04:49.097831Z","shell.execute_reply":"2023-04-06T15:04:49.097010Z","shell.execute_reply.started":"2023-04-06T15:04:47.634663Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 256, 256, 5  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 256, 256, 32  1472        ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 256, 256, 32  128        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," activation (Activation)        (None, 256, 256, 32  0           ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 256, 256, 32  9248        ['activation[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 256, 256, 32  128        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_1 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 128, 128, 32  0           ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 128, 128, 64  18496       ['max_pooling2d[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 128, 128, 64  256        ['conv2d_7[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_4 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_2[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 128, 128, 64  36928       ['activation_4[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 128, 128, 64  256        ['conv2d_8[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_5 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_3[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)  0           ['activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 64, 64, 128)  73856       ['max_pooling2d_1[0][0]']        \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_14[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_8 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_8[0][0]']           \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_15[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_9 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0          ['activation_9[0][0]']           \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 32, 32, 256)  295168      ['max_pooling2d_2[0][0]']        \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_21[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_12 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_12[0][0]']          \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_22[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_13 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," conv2d_26 (Conv2D)             (None, 32, 32, 128)  32768       ['activation_13[0][0]']          \n","                                                                                                  \n"," conv2d_24 (Conv2D)             (None, 32, 32, 1)    256         ['activation_13[0][0]']          \n","                                                                                                  \n"," global_average_pooling2d_3 (Gl  (None, 128)         0           ['conv2d_26[0][0]']              \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 32, 32, 128)  32768       ['activation_13[0][0]']          \n","                                                                                                  \n"," reshape_22 (Reshape)           (None, 1024, 1)      0           ['conv2d_24[0][0]']              \n","                                                                                                  \n"," max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0          ['activation_13[0][0]']          \n","                                                                                                  \n"," conv2d_27 (Conv2D)             (None, 32, 32, 128)  32768       ['activation_13[0][0]']          \n","                                                                                                  \n"," softmax_7 (Softmax)            (None, 128)          0           ['global_average_pooling2d_3[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," reshape_21 (Reshape)           (None, 1024, 128)    0           ['conv2d_23[0][0]']              \n","                                                                                                  \n"," softmax_6 (Softmax)            (None, 1024, 1)      0           ['reshape_22[0][0]']             \n","                                                                                                  \n"," conv2d_28 (Conv2D)             (None, 16, 16, 512)  1180160     ['max_pooling2d_3[0][0]']        \n","                                                                                                  \n"," reshape_25 (Reshape)           (None, 1024, 128)    0           ['conv2d_27[0][0]']              \n","                                                                                                  \n"," reshape_24 (Reshape)           (None, 128, 1)       0           ['softmax_7[0][0]']              \n","                                                                                                  \n"," lambda_6 (Lambda)              (None, 128, 1)       0           ['reshape_21[0][0]',             \n","                                                                  'softmax_6[0][0]']              \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_28[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," lambda_7 (Lambda)              (None, 1024, 1)      0           ['reshape_25[0][0]',             \n","                                                                  'reshape_24[0][0]']             \n","                                                                                                  \n"," permute_3 (Permute)            (None, 1, 128)       0           ['lambda_6[0][0]']               \n","                                                                                                  \n"," activation_16 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," reshape_26 (Reshape)           (None, 1024)         0           ['lambda_7[0][0]']               \n","                                                                                                  \n"," reshape_23 (Reshape)           (None, 1, 1, 128)    0           ['permute_3[0][0]']              \n","                                                                                                  \n"," conv2d_29 (Conv2D)             (None, 16, 16, 512)  2359808     ['activation_16[0][0]']          \n","                                                                                                  \n"," activation_15 (Activation)     (None, 1024)         0           ['reshape_26[0][0]']             \n","                                                                                                  \n"," conv2d_25 (Conv2D)             (None, 1, 1, 256)    33024       ['reshape_23[0][0]']             \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_29[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," reshape_27 (Reshape)           (None, 32, 32, 1)    0           ['activation_15[0][0]']          \n","                                                                                                  \n"," activation_14 (Activation)     (None, 1, 1, 256)    0           ['conv2d_25[0][0]']              \n","                                                                                                  \n"," activation_17 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," multiply_7 (Multiply)          (None, 32, 32, 256)  0           ['activation_13[0][0]',          \n","                                                                  'reshape_27[0][0]']             \n","                                                                                                  \n"," multiply_6 (Multiply)          (None, 32, 32, 256)  0           ['activation_13[0][0]',          \n","                                                                  'activation_14[0][0]']          \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 64, 64, 64)   8192        ['activation_9[0][0]']           \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 64, 64, 1)    128         ['activation_9[0][0]']           \n","                                                                                                  \n"," up_sampling2d (UpSampling2D)   (None, 32, 32, 512)  0           ['activation_17[0][0]']          \n","                                                                                                  \n"," add_3 (Add)                    (None, 32, 32, 256)  0           ['multiply_7[0][0]',             \n","                                                                  'multiply_6[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d_2 (Gl  (None, 64)          0           ['conv2d_19[0][0]']              \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 64, 64, 64)   8192        ['activation_9[0][0]']           \n","                                                                                                  \n"," reshape_15 (Reshape)           (None, 4096, 1)      0           ['conv2d_17[0][0]']              \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 32, 32, 768)  0           ['up_sampling2d[0][0]',          \n","                                                                  'add_3[0][0]']                  \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 64, 64, 64)   8192        ['activation_9[0][0]']           \n","                                                                                                  \n"," softmax_5 (Softmax)            (None, 64)           0           ['global_average_pooling2d_2[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," reshape_14 (Reshape)           (None, 4096, 64)     0           ['conv2d_16[0][0]']              \n","                                                                                                  \n"," softmax_4 (Softmax)            (None, 4096, 1)      0           ['reshape_15[0][0]']             \n","                                                                                                  \n"," conv2d_30 (Conv2D)             (None, 32, 32, 256)  1769728     ['concatenate[0][0]']            \n","                                                                                                  \n"," reshape_18 (Reshape)           (None, 4096, 64)     0           ['conv2d_20[0][0]']              \n","                                                                                                  \n"," reshape_17 (Reshape)           (None, 64, 1)        0           ['softmax_5[0][0]']              \n","                                                                                                  \n"," lambda_4 (Lambda)              (None, 64, 1)        0           ['reshape_14[0][0]',             \n","                                                                  'softmax_4[0][0]']              \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_30[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," lambda_5 (Lambda)              (None, 4096, 1)      0           ['reshape_18[0][0]',             \n","                                                                  'reshape_17[0][0]']             \n","                                                                                                  \n"," permute_2 (Permute)            (None, 1, 64)        0           ['lambda_4[0][0]']               \n","                                                                                                  \n"," activation_18 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_10[0][0]'] \n","                                                                                                  \n"," reshape_19 (Reshape)           (None, 4096)         0           ['lambda_5[0][0]']               \n","                                                                                                  \n"," reshape_16 (Reshape)           (None, 1, 1, 64)     0           ['permute_2[0][0]']              \n","                                                                                                  \n"," conv2d_31 (Conv2D)             (None, 32, 32, 256)  590080      ['activation_18[0][0]']          \n","                                                                                                  \n"," activation_11 (Activation)     (None, 4096)         0           ['reshape_19[0][0]']             \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 1, 1, 128)    8320        ['reshape_16[0][0]']             \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_31[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," reshape_20 (Reshape)           (None, 64, 64, 1)    0           ['activation_11[0][0]']          \n","                                                                                                  \n"," activation_10 (Activation)     (None, 1, 1, 128)    0           ['conv2d_18[0][0]']              \n","                                                                                                  \n"," activation_19 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," multiply_5 (Multiply)          (None, 64, 64, 128)  0           ['activation_9[0][0]',           \n","                                                                  'reshape_20[0][0]']             \n","                                                                                                  \n"," multiply_4 (Multiply)          (None, 64, 64, 128)  0           ['activation_9[0][0]',           \n","                                                                  'activation_10[0][0]']          \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 128, 128, 32  2048        ['activation_5[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 128, 128, 1)  64          ['activation_5[0][0]']           \n","                                                                                                  \n"," up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0          ['activation_19[0][0]']          \n","                                                                                                  \n"," add_2 (Add)                    (None, 64, 64, 128)  0           ['multiply_5[0][0]',             \n","                                                                  'multiply_4[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d_1 (Gl  (None, 32)          0           ['conv2d_12[0][0]']              \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 128, 128, 32  2048        ['activation_5[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," reshape_8 (Reshape)            (None, 16384, 1)     0           ['conv2d_10[0][0]']              \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 64, 64, 384)  0           ['up_sampling2d_1[0][0]',        \n","                                                                  'add_2[0][0]']                  \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 128, 128, 32  2048        ['activation_5[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," softmax_3 (Softmax)            (None, 32)           0           ['global_average_pooling2d_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," reshape_7 (Reshape)            (None, 16384, 32)    0           ['conv2d_9[0][0]']               \n","                                                                                                  \n"," softmax_2 (Softmax)            (None, 16384, 1)     0           ['reshape_8[0][0]']              \n","                                                                                                  \n"," conv2d_32 (Conv2D)             (None, 64, 64, 128)  442496      ['concatenate_1[0][0]']          \n","                                                                                                  \n"," reshape_11 (Reshape)           (None, 16384, 32)    0           ['conv2d_13[0][0]']              \n","                                                                                                  \n"," reshape_10 (Reshape)           (None, 32, 1)        0           ['softmax_3[0][0]']              \n","                                                                                                  \n"," lambda_2 (Lambda)              (None, 32, 1)        0           ['reshape_7[0][0]',              \n","                                                                  'softmax_2[0][0]']              \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 64, 64, 128)  512        ['conv2d_32[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," lambda_3 (Lambda)              (None, 16384, 1)     0           ['reshape_11[0][0]',             \n","                                                                  'reshape_10[0][0]']             \n","                                                                                                  \n"," permute_1 (Permute)            (None, 1, 32)        0           ['lambda_2[0][0]']               \n","                                                                                                  \n"," activation_20 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," reshape_12 (Reshape)           (None, 16384)        0           ['lambda_3[0][0]']               \n","                                                                                                  \n"," reshape_9 (Reshape)            (None, 1, 1, 32)     0           ['permute_1[0][0]']              \n","                                                                                                  \n"," conv2d_33 (Conv2D)             (None, 64, 64, 128)  147584      ['activation_20[0][0]']          \n","                                                                                                  \n"," activation_7 (Activation)      (None, 16384)        0           ['reshape_12[0][0]']             \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 1, 1, 64)     2112        ['reshape_9[0][0]']              \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 64, 64, 128)  512        ['conv2d_33[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," reshape_13 (Reshape)           (None, 128, 128, 1)  0           ['activation_7[0][0]']           \n","                                                                                                  \n"," activation_6 (Activation)      (None, 1, 1, 64)     0           ['conv2d_11[0][0]']              \n","                                                                                                  \n"," activation_21 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," multiply_3 (Multiply)          (None, 128, 128, 64  0           ['activation_5[0][0]',           \n","                                )                                 'reshape_13[0][0]']             \n","                                                                                                  \n"," multiply_2 (Multiply)          (None, 128, 128, 64  0           ['activation_5[0][0]',           \n","                                )                                 'activation_6[0][0]']           \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 256, 256, 16  512         ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 256, 256, 1)  32          ['activation_1[0][0]']           \n","                                                                                                  \n"," up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 12  0          ['activation_21[0][0]']          \n","                                8)                                                                \n","                                                                                                  \n"," add_1 (Add)                    (None, 128, 128, 64  0           ['multiply_3[0][0]',             \n","                                )                                 'multiply_2[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d (Glob  (None, 16)          0           ['conv2d_5[0][0]']               \n"," alAveragePooling2D)                                                                              \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 256, 256, 16  512         ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," reshape_1 (Reshape)            (None, 65536, 1)     0           ['conv2d_3[0][0]']               \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 128, 128, 19  0           ['up_sampling2d_2[0][0]',        \n","                                2)                                'add_1[0][0]']                  \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 256, 256, 16  512         ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," softmax_1 (Softmax)            (None, 16)           0           ['global_average_pooling2d[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," reshape (Reshape)              (None, 65536, 16)    0           ['conv2d_2[0][0]']               \n","                                                                                                  \n"," softmax (Softmax)              (None, 65536, 1)     0           ['reshape_1[0][0]']              \n","                                                                                                  \n"," conv2d_34 (Conv2D)             (None, 128, 128, 64  110656      ['concatenate_2[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," reshape_4 (Reshape)            (None, 65536, 16)    0           ['conv2d_6[0][0]']               \n","                                                                                                  \n"," reshape_3 (Reshape)            (None, 16, 1)        0           ['softmax_1[0][0]']              \n","                                                                                                  \n"," lambda (Lambda)                (None, 16, 1)        0           ['reshape[0][0]',                \n","                                                                  'softmax[0][0]']                \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 128, 128, 64  256        ['conv2d_34[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," lambda_1 (Lambda)              (None, 65536, 1)     0           ['reshape_4[0][0]',              \n","                                                                  'reshape_3[0][0]']              \n","                                                                                                  \n"," permute (Permute)              (None, 1, 16)        0           ['lambda[0][0]']                 \n","                                                                                                  \n"," activation_22 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_14[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," reshape_5 (Reshape)            (None, 65536)        0           ['lambda_1[0][0]']               \n","                                                                                                  \n"," reshape_2 (Reshape)            (None, 1, 1, 16)     0           ['permute[0][0]']                \n","                                                                                                  \n"," conv2d_35 (Conv2D)             (None, 128, 128, 64  36928       ['activation_22[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," activation_3 (Activation)      (None, 65536)        0           ['reshape_5[0][0]']              \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 1, 1, 32)     544         ['reshape_2[0][0]']              \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 128, 128, 64  256        ['conv2d_35[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," reshape_6 (Reshape)            (None, 256, 256, 1)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," activation_2 (Activation)      (None, 1, 1, 32)     0           ['conv2d_4[0][0]']               \n","                                                                                                  \n"," activation_23 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_15[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," multiply_1 (Multiply)          (None, 256, 256, 32  0           ['activation_1[0][0]',           \n","                                )                                 'reshape_6[0][0]']              \n","                                                                                                  \n"," multiply (Multiply)            (None, 256, 256, 32  0           ['activation_1[0][0]',           \n","                                )                                 'activation_2[0][0]']           \n","                                                                                                  \n"," up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64  0          ['activation_23[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," add (Add)                      (None, 256, 256, 32  0           ['multiply_1[0][0]',             \n","                                )                                 'multiply[0][0]']               \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 256, 256, 96  0           ['up_sampling2d_3[0][0]',        \n","                                )                                 'add[0][0]']                    \n","                                                                                                  \n"," conv2d_36 (Conv2D)             (None, 256, 256, 32  27680       ['concatenate_3[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 256, 256, 32  128        ['conv2d_36[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_24 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_16[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_37 (Conv2D)             (None, 256, 256, 32  9248        ['activation_24[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 256, 256, 32  128        ['conv2d_37[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_25 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_17[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_38 (Conv2D)             (None, 256, 256, 1)  33          ['activation_25[0][0]']          \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 256, 256, 1)  4          ['conv2d_38[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_26 (Activation)     (None, 256, 256, 1)  0           ['batch_normalization_18[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 8,034,053\n","Trainable params: 8,028,163\n","Non-trainable params: 5,890\n","__________________________________________________________________________________________________\n"]}],"source":["from keras import metrics\n","unet2= daunet(input_shape=(256,256,5))\n","unet2.compile(optimizer = adam_v2.Adam(learning_rate = 1e-4), loss =gl_sl_wrapper(alpha), metrics = ['accuracy'])\n","unet2.summary()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T15:05:37.250424Z","iopub.status.busy":"2023-04-06T15:05:37.249605Z","iopub.status.idle":"2023-04-06T16:19:55.643163Z","shell.execute_reply":"2023-04-06T16:19:55.641977Z","shell.execute_reply.started":"2023-04-06T15:05:37.250385Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.4540 - accuracy: 0.8741\n","Epoch 1: val_loss improved from inf to 0.62072, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 32s 365ms/step - loss: 0.4540 - accuracy: 0.8741 - val_loss: 0.6207 - val_accuracy: 0.7660 - lr: 1.0000e-04\n","Epoch 2/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.4003 - accuracy: 0.9378\n","Epoch 2: val_loss improved from 0.62072 to 0.56176, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.4003 - accuracy: 0.9378 - val_loss: 0.5618 - val_accuracy: 0.8024 - lr: 1.0000e-04\n","Epoch 3/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3884 - accuracy: 0.9522\n","Epoch 3: val_loss improved from 0.56176 to 0.49339, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3884 - accuracy: 0.9522 - val_loss: 0.4934 - val_accuracy: 0.9054 - lr: 1.0000e-04\n","Epoch 4/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3820 - accuracy: 0.9572\n","Epoch 4: val_loss improved from 0.49339 to 0.49281, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.3820 - accuracy: 0.9572 - val_loss: 0.4928 - val_accuracy: 0.8500 - lr: 1.0000e-04\n","Epoch 5/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3719 - accuracy: 0.9674\n","Epoch 5: val_loss did not improve from 0.49281\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3719 - accuracy: 0.9674 - val_loss: 0.5071 - val_accuracy: 0.8608 - lr: 1.0000e-04\n","Epoch 6/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3683 - accuracy: 0.9695\n","Epoch 6: val_loss improved from 0.49281 to 0.42407, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3683 - accuracy: 0.9695 - val_loss: 0.4241 - val_accuracy: 0.8984 - lr: 1.0000e-04\n","Epoch 7/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3684 - accuracy: 0.9664\n","Epoch 7: val_loss improved from 0.42407 to 0.39770, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3684 - accuracy: 0.9664 - val_loss: 0.3977 - val_accuracy: 0.9445 - lr: 1.0000e-04\n","Epoch 8/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3658 - accuracy: 0.9693\n","Epoch 8: val_loss improved from 0.39770 to 0.38179, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.3658 - accuracy: 0.9693 - val_loss: 0.3818 - val_accuracy: 0.9602 - lr: 1.0000e-04\n","Epoch 9/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3607 - accuracy: 0.9715\n","Epoch 9: val_loss improved from 0.38179 to 0.36298, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.3607 - accuracy: 0.9715 - val_loss: 0.3630 - val_accuracy: 0.9735 - lr: 1.0000e-04\n","Epoch 10/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3595 - accuracy: 0.9722\n","Epoch 10: val_loss improved from 0.36298 to 0.35747, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.3595 - accuracy: 0.9722 - val_loss: 0.3575 - val_accuracy: 0.9779 - lr: 1.0000e-04\n","Epoch 11/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3557 - accuracy: 0.9721\n","Epoch 11: val_loss improved from 0.35747 to 0.35211, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3557 - accuracy: 0.9721 - val_loss: 0.3521 - val_accuracy: 0.9770 - lr: 1.0000e-04\n","Epoch 12/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3496 - accuracy: 0.9762\n","Epoch 12: val_loss improved from 0.35211 to 0.34315, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3496 - accuracy: 0.9762 - val_loss: 0.3432 - val_accuracy: 0.9825 - lr: 1.0000e-04\n","Epoch 13/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3483 - accuracy: 0.9765\n","Epoch 13: val_loss did not improve from 0.34315\n","56/56 [==============================] - 16s 278ms/step - loss: 0.3483 - accuracy: 0.9765 - val_loss: 0.3515 - val_accuracy: 0.9777 - lr: 1.0000e-04\n","Epoch 14/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3527 - accuracy: 0.9711\n","Epoch 14: val_loss improved from 0.34315 to 0.33844, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.3527 - accuracy: 0.9711 - val_loss: 0.3384 - val_accuracy: 0.9809 - lr: 1.0000e-04\n","Epoch 15/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3415 - accuracy: 0.9799\n","Epoch 15: val_loss did not improve from 0.33844\n","56/56 [==============================] - 15s 274ms/step - loss: 0.3415 - accuracy: 0.9799 - val_loss: 0.3403 - val_accuracy: 0.9823 - lr: 1.0000e-04\n","Epoch 16/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.9783\n","Epoch 16: val_loss improved from 0.33844 to 0.33438, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.3409 - accuracy: 0.9783 - val_loss: 0.3344 - val_accuracy: 0.9827 - lr: 1.0000e-04\n","Epoch 17/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3428 - accuracy: 0.9743\n","Epoch 17: val_loss did not improve from 0.33438\n","56/56 [==============================] - 15s 274ms/step - loss: 0.3428 - accuracy: 0.9743 - val_loss: 0.3412 - val_accuracy: 0.9759 - lr: 1.0000e-04\n","Epoch 18/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3366 - accuracy: 0.9789\n","Epoch 18: val_loss improved from 0.33438 to 0.33337, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3366 - accuracy: 0.9789 - val_loss: 0.3334 - val_accuracy: 0.9814 - lr: 1.0000e-04\n","Epoch 19/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3338 - accuracy: 0.9794\n","Epoch 19: val_loss did not improve from 0.33337\n","56/56 [==============================] - 16s 277ms/step - loss: 0.3338 - accuracy: 0.9794 - val_loss: 0.3351 - val_accuracy: 0.9803 - lr: 1.0000e-04\n","Epoch 20/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3353 - accuracy: 0.9762\n","Epoch 20: val_loss improved from 0.33337 to 0.32579, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3353 - accuracy: 0.9762 - val_loss: 0.3258 - val_accuracy: 0.9828 - lr: 1.0000e-04\n","Epoch 21/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.9781\n","Epoch 21: val_loss did not improve from 0.32579\n","56/56 [==============================] - 15s 274ms/step - loss: 0.3306 - accuracy: 0.9781 - val_loss: 0.3281 - val_accuracy: 0.9814 - lr: 1.0000e-04\n","Epoch 22/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3308 - accuracy: 0.9774\n","Epoch 22: val_loss did not improve from 0.32579\n","56/56 [==============================] - 15s 277ms/step - loss: 0.3308 - accuracy: 0.9774 - val_loss: 0.3297 - val_accuracy: 0.9772 - lr: 1.0000e-04\n","Epoch 23/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3244 - accuracy: 0.9819\n","Epoch 23: val_loss improved from 0.32579 to 0.32282, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.3244 - accuracy: 0.9819 - val_loss: 0.3228 - val_accuracy: 0.9836 - lr: 1.0000e-04\n","Epoch 24/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.9834\n","Epoch 24: val_loss did not improve from 0.32282\n","56/56 [==============================] - 15s 274ms/step - loss: 0.3226 - accuracy: 0.9834 - val_loss: 0.3233 - val_accuracy: 0.9835 - lr: 1.0000e-04\n","Epoch 25/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3218 - accuracy: 0.9811\n","Epoch 25: val_loss improved from 0.32282 to 0.31693, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.3218 - accuracy: 0.9811 - val_loss: 0.3169 - val_accuracy: 0.9834 - lr: 1.0000e-04\n","Epoch 26/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3210 - accuracy: 0.9812\n","Epoch 26: val_loss improved from 0.31693 to 0.31504, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.3210 - accuracy: 0.9812 - val_loss: 0.3150 - val_accuracy: 0.9840 - lr: 1.0000e-04\n","Epoch 27/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3150 - accuracy: 0.9842\n","Epoch 27: val_loss did not improve from 0.31504\n","56/56 [==============================] - 15s 274ms/step - loss: 0.3150 - accuracy: 0.9842 - val_loss: 0.3170 - val_accuracy: 0.9852 - lr: 1.0000e-04\n","Epoch 28/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3156 - accuracy: 0.9814\n","Epoch 28: val_loss did not improve from 0.31504\n","56/56 [==============================] - 15s 275ms/step - loss: 0.3156 - accuracy: 0.9814 - val_loss: 0.3166 - val_accuracy: 0.9832 - lr: 1.0000e-04\n","Epoch 29/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.9832\n","Epoch 29: val_loss improved from 0.31504 to 0.31223, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.3118 - accuracy: 0.9832 - val_loss: 0.3122 - val_accuracy: 0.9839 - lr: 1.0000e-04\n","Epoch 30/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3099 - accuracy: 0.9850\n","Epoch 30: val_loss improved from 0.31223 to 0.30878, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.3099 - accuracy: 0.9850 - val_loss: 0.3088 - val_accuracy: 0.9849 - lr: 1.0000e-04\n","Epoch 31/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.9839\n","Epoch 31: val_loss did not improve from 0.30878\n","56/56 [==============================] - 15s 274ms/step - loss: 0.3098 - accuracy: 0.9839 - val_loss: 0.3098 - val_accuracy: 0.9828 - lr: 1.0000e-04\n","Epoch 32/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3071 - accuracy: 0.9844\n","Epoch 32: val_loss improved from 0.30878 to 0.30734, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 291ms/step - loss: 0.3071 - accuracy: 0.9844 - val_loss: 0.3073 - val_accuracy: 0.9858 - lr: 1.0000e-04\n","Epoch 33/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.9867\n","Epoch 33: val_loss improved from 0.30734 to 0.30346, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.3035 - accuracy: 0.9867 - val_loss: 0.3035 - val_accuracy: 0.9858 - lr: 1.0000e-04\n","Epoch 34/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3000 - accuracy: 0.9877\n","Epoch 34: val_loss improved from 0.30346 to 0.30267, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.3000 - accuracy: 0.9877 - val_loss: 0.3027 - val_accuracy: 0.9855 - lr: 1.0000e-04\n","Epoch 35/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.9857\n","Epoch 35: val_loss improved from 0.30267 to 0.29871, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 288ms/step - loss: 0.3011 - accuracy: 0.9857 - val_loss: 0.2987 - val_accuracy: 0.9852 - lr: 1.0000e-04\n","Epoch 36/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.9782\n","Epoch 36: val_loss did not improve from 0.29871\n","56/56 [==============================] - 15s 277ms/step - loss: 0.3052 - accuracy: 0.9782 - val_loss: 0.3147 - val_accuracy: 0.9728 - lr: 1.0000e-04\n","Epoch 37/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2980 - accuracy: 0.9839\n","Epoch 37: val_loss did not improve from 0.29871\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2980 - accuracy: 0.9839 - val_loss: 0.2995 - val_accuracy: 0.9812 - lr: 1.0000e-04\n","Epoch 38/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.9850\n","Epoch 38: val_loss improved from 0.29871 to 0.29288, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2976 - accuracy: 0.9850 - val_loss: 0.2929 - val_accuracy: 0.9852 - lr: 1.0000e-04\n","Epoch 39/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2928 - accuracy: 0.9876\n","Epoch 39: val_loss did not improve from 0.29288\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2928 - accuracy: 0.9876 - val_loss: 0.2933 - val_accuracy: 0.9857 - lr: 1.0000e-04\n","Epoch 40/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.9875\n","Epoch 40: val_loss did not improve from 0.29288\n","56/56 [==============================] - 15s 273ms/step - loss: 0.2923 - accuracy: 0.9875 - val_loss: 0.2935 - val_accuracy: 0.9852 - lr: 1.0000e-04\n","Epoch 41/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.9825\n","Epoch 41: val_loss did not improve from 0.29288\n","56/56 [==============================] - 16s 277ms/step - loss: 0.2960 - accuracy: 0.9825 - val_loss: 0.2948 - val_accuracy: 0.9849 - lr: 1.0000e-04\n","Epoch 42/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9862\n","Epoch 42: val_loss improved from 0.29288 to 0.28934, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2881 - accuracy: 0.9862 - val_loss: 0.2893 - val_accuracy: 0.9863 - lr: 1.0000e-04\n","Epoch 43/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.9857\n","Epoch 43: val_loss improved from 0.28934 to 0.28475, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2865 - accuracy: 0.9857 - val_loss: 0.2848 - val_accuracy: 0.9864 - lr: 1.0000e-04\n","Epoch 44/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.9880\n","Epoch 44: val_loss did not improve from 0.28475\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2833 - accuracy: 0.9880 - val_loss: 0.2867 - val_accuracy: 0.9859 - lr: 1.0000e-04\n","Epoch 45/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2828 - accuracy: 0.9874\n","Epoch 45: val_loss improved from 0.28475 to 0.28213, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2828 - accuracy: 0.9874 - val_loss: 0.2821 - val_accuracy: 0.9854 - lr: 1.0000e-04\n","Epoch 46/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2806 - accuracy: 0.9867\n","Epoch 46: val_loss improved from 0.28213 to 0.28060, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2806 - accuracy: 0.9867 - val_loss: 0.2806 - val_accuracy: 0.9865 - lr: 1.0000e-04\n","Epoch 47/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2790 - accuracy: 0.9880\n","Epoch 47: val_loss improved from 0.28060 to 0.27954, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2790 - accuracy: 0.9880 - val_loss: 0.2795 - val_accuracy: 0.9861 - lr: 1.0000e-04\n","Epoch 48/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2766 - accuracy: 0.9878\n","Epoch 48: val_loss improved from 0.27954 to 0.27515, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2766 - accuracy: 0.9878 - val_loss: 0.2752 - val_accuracy: 0.9873 - lr: 1.0000e-04\n","Epoch 49/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.9880\n","Epoch 49: val_loss did not improve from 0.27515\n","56/56 [==============================] - 16s 277ms/step - loss: 0.2755 - accuracy: 0.9880 - val_loss: 0.2768 - val_accuracy: 0.9870 - lr: 1.0000e-04\n","Epoch 50/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.9869\n","Epoch 50: val_loss improved from 0.27515 to 0.27409, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2751 - accuracy: 0.9869 - val_loss: 0.2741 - val_accuracy: 0.9862 - lr: 1.0000e-04\n","Epoch 51/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.9878\n","Epoch 51: val_loss improved from 0.27409 to 0.27107, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2736 - accuracy: 0.9878 - val_loss: 0.2711 - val_accuracy: 0.9866 - lr: 1.0000e-04\n","Epoch 52/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.9880\n","Epoch 52: val_loss improved from 0.27107 to 0.27070, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 291ms/step - loss: 0.2717 - accuracy: 0.9880 - val_loss: 0.2707 - val_accuracy: 0.9867 - lr: 1.0000e-04\n","Epoch 53/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.9896\n","Epoch 53: val_loss improved from 0.27070 to 0.26923, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2665 - accuracy: 0.9896 - val_loss: 0.2692 - val_accuracy: 0.9880 - lr: 1.0000e-04\n","Epoch 54/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2656 - accuracy: 0.9894\n","Epoch 54: val_loss improved from 0.26923 to 0.26861, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2656 - accuracy: 0.9894 - val_loss: 0.2686 - val_accuracy: 0.9877 - lr: 1.0000e-04\n","Epoch 55/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.9897\n","Epoch 55: val_loss improved from 0.26861 to 0.26781, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2639 - accuracy: 0.9897 - val_loss: 0.2678 - val_accuracy: 0.9868 - lr: 1.0000e-04\n","Epoch 56/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.9898\n","Epoch 56: val_loss improved from 0.26781 to 0.26240, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2629 - accuracy: 0.9898 - val_loss: 0.2624 - val_accuracy: 0.9893 - lr: 1.0000e-04\n","Epoch 57/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 0.9895\n","Epoch 57: val_loss did not improve from 0.26240\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2627 - accuracy: 0.9895 - val_loss: 0.2635 - val_accuracy: 0.9881 - lr: 1.0000e-04\n","Epoch 58/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.9863\n","Epoch 58: val_loss improved from 0.26240 to 0.25372, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2655 - accuracy: 0.9863 - val_loss: 0.2537 - val_accuracy: 0.9880 - lr: 1.0000e-04\n","Epoch 59/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2637 - accuracy: 0.9880\n","Epoch 59: val_loss did not improve from 0.25372\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2637 - accuracy: 0.9880 - val_loss: 0.2628 - val_accuracy: 0.9854 - lr: 1.0000e-04\n","Epoch 60/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.9871\n","Epoch 60: val_loss did not improve from 0.25372\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2602 - accuracy: 0.9871 - val_loss: 0.2560 - val_accuracy: 0.9875 - lr: 1.0000e-04\n","Epoch 61/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.9816\n","Epoch 61: val_loss did not improve from 0.25372\n","56/56 [==============================] - 16s 278ms/step - loss: 0.2639 - accuracy: 0.9816 - val_loss: 0.2556 - val_accuracy: 0.9842 - lr: 1.0000e-04\n","Epoch 62/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2631 - accuracy: 0.9802\n","Epoch 62: val_loss improved from 0.25372 to 0.25304, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2631 - accuracy: 0.9802 - val_loss: 0.2530 - val_accuracy: 0.9821 - lr: 1.0000e-04\n","Epoch 63/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.9851\n","Epoch 63: val_loss improved from 0.25304 to 0.24712, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2575 - accuracy: 0.9851 - val_loss: 0.2471 - val_accuracy: 0.9866 - lr: 1.0000e-04\n","Epoch 64/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.9871\n","Epoch 64: val_loss did not improve from 0.24712\n","56/56 [==============================] - 16s 278ms/step - loss: 0.2557 - accuracy: 0.9871 - val_loss: 0.2520 - val_accuracy: 0.9877 - lr: 1.0000e-04\n","Epoch 65/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.9890\n","Epoch 65: val_loss did not improve from 0.24712\n","56/56 [==============================] - 16s 278ms/step - loss: 0.2511 - accuracy: 0.9890 - val_loss: 0.2543 - val_accuracy: 0.9873 - lr: 1.0000e-04\n","Epoch 66/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.9860\n","Epoch 66: val_loss did not improve from 0.24712\n","56/56 [==============================] - 16s 278ms/step - loss: 0.2537 - accuracy: 0.9860 - val_loss: 0.2553 - val_accuracy: 0.9852 - lr: 1.0000e-04\n","Epoch 67/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.9840\n","Epoch 67: val_loss did not improve from 0.24712\n","56/56 [==============================] - 16s 278ms/step - loss: 0.2542 - accuracy: 0.9840 - val_loss: 0.2487 - val_accuracy: 0.9837 - lr: 1.0000e-04\n","Epoch 68/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9869\n","Epoch 68: val_loss did not improve from 0.24712\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2495 - accuracy: 0.9869 - val_loss: 0.2485 - val_accuracy: 0.9850 - lr: 1.0000e-04\n","Epoch 69/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.9900\n","Epoch 69: val_loss did not improve from 0.24712\n","56/56 [==============================] - 16s 277ms/step - loss: 0.2467 - accuracy: 0.9900 - val_loss: 0.2503 - val_accuracy: 0.9872 - lr: 1.0000e-04\n","Epoch 70/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9903\n","Epoch 70: val_loss did not improve from 0.24712\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2448 - accuracy: 0.9903 - val_loss: 0.2482 - val_accuracy: 0.9880 - lr: 1.0000e-04\n","Epoch 71/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.9904\n","Epoch 71: val_loss improved from 0.24712 to 0.24263, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2417 - accuracy: 0.9904 - val_loss: 0.2426 - val_accuracy: 0.9891 - lr: 1.0000e-04\n","Epoch 72/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9909\n","Epoch 72: val_loss improved from 0.24263 to 0.24159, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2394 - accuracy: 0.9909 - val_loss: 0.2416 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 73/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2401 - accuracy: 0.9906\n","Epoch 73: val_loss improved from 0.24159 to 0.23806, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2401 - accuracy: 0.9906 - val_loss: 0.2381 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 74/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2373 - accuracy: 0.9906\n","Epoch 74: val_loss improved from 0.23806 to 0.23618, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2373 - accuracy: 0.9906 - val_loss: 0.2362 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 75/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.9893\n","Epoch 75: val_loss did not improve from 0.23618\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2375 - accuracy: 0.9893 - val_loss: 0.2372 - val_accuracy: 0.9886 - lr: 1.0000e-04\n","Epoch 76/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2357 - accuracy: 0.9906\n","Epoch 76: val_loss improved from 0.23618 to 0.23456, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2357 - accuracy: 0.9906 - val_loss: 0.2346 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 77/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.9904\n","Epoch 77: val_loss improved from 0.23456 to 0.23425, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2335 - accuracy: 0.9904 - val_loss: 0.2343 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 78/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2311 - accuracy: 0.9912\n","Epoch 78: val_loss improved from 0.23425 to 0.23193, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2311 - accuracy: 0.9912 - val_loss: 0.2319 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 79/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.9912\n","Epoch 79: val_loss improved from 0.23193 to 0.23106, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2302 - accuracy: 0.9912 - val_loss: 0.2311 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 80/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2268 - accuracy: 0.9918\n","Epoch 80: val_loss improved from 0.23106 to 0.22889, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2268 - accuracy: 0.9918 - val_loss: 0.2289 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 81/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9909\n","Epoch 81: val_loss improved from 0.22889 to 0.22751, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.2297 - accuracy: 0.9909 - val_loss: 0.2275 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 82/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9918\n","Epoch 82: val_loss improved from 0.22751 to 0.22618, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.2249 - accuracy: 0.9918 - val_loss: 0.2262 - val_accuracy: 0.9906 - lr: 1.0000e-04\n","Epoch 83/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.9915\n","Epoch 83: val_loss improved from 0.22618 to 0.22563, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2255 - accuracy: 0.9915 - val_loss: 0.2256 - val_accuracy: 0.9896 - lr: 1.0000e-04\n","Epoch 84/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2225 - accuracy: 0.9912\n","Epoch 84: val_loss improved from 0.22563 to 0.22507, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2225 - accuracy: 0.9912 - val_loss: 0.2251 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 85/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2233 - accuracy: 0.9905\n","Epoch 85: val_loss improved from 0.22507 to 0.22191, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 288ms/step - loss: 0.2233 - accuracy: 0.9905 - val_loss: 0.2219 - val_accuracy: 0.9902 - lr: 1.0000e-04\n","Epoch 86/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2208 - accuracy: 0.9922\n","Epoch 86: val_loss did not improve from 0.22191\n","56/56 [==============================] - 16s 278ms/step - loss: 0.2208 - accuracy: 0.9922 - val_loss: 0.2228 - val_accuracy: 0.9892 - lr: 1.0000e-04\n","Epoch 87/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 0.9922\n","Epoch 87: val_loss improved from 0.22191 to 0.22029, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2188 - accuracy: 0.9922 - val_loss: 0.2203 - val_accuracy: 0.9903 - lr: 1.0000e-04\n","Epoch 88/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2180 - accuracy: 0.9924\n","Epoch 88: val_loss improved from 0.22029 to 0.21860, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 288ms/step - loss: 0.2180 - accuracy: 0.9924 - val_loss: 0.2186 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 89/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.9927\n","Epoch 89: val_loss improved from 0.21860 to 0.21652, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2167 - accuracy: 0.9927 - val_loss: 0.2165 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 90/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2140 - accuracy: 0.9927\n","Epoch 90: val_loss did not improve from 0.21652\n","56/56 [==============================] - 16s 278ms/step - loss: 0.2140 - accuracy: 0.9927 - val_loss: 0.2168 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 91/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2139 - accuracy: 0.9929\n","Epoch 91: val_loss improved from 0.21652 to 0.21629, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2139 - accuracy: 0.9929 - val_loss: 0.2163 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 92/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2121 - accuracy: 0.9929\n","Epoch 92: val_loss improved from 0.21629 to 0.21289, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 288ms/step - loss: 0.2121 - accuracy: 0.9929 - val_loss: 0.2129 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 93/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2133 - accuracy: 0.9921\n","Epoch 93: val_loss improved from 0.21289 to 0.21003, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2133 - accuracy: 0.9921 - val_loss: 0.2100 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 94/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2113 - accuracy: 0.9923\n","Epoch 94: val_loss did not improve from 0.21003\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2113 - accuracy: 0.9923 - val_loss: 0.2124 - val_accuracy: 0.9895 - lr: 1.0000e-04\n","Epoch 95/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2098 - accuracy: 0.9911\n","Epoch 95: val_loss did not improve from 0.21003\n","56/56 [==============================] - 15s 273ms/step - loss: 0.2098 - accuracy: 0.9911 - val_loss: 0.2108 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 96/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.9914\n","Epoch 96: val_loss improved from 0.21003 to 0.20720, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2079 - accuracy: 0.9914 - val_loss: 0.2072 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 97/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2095 - accuracy: 0.9902\n","Epoch 97: val_loss did not improve from 0.20720\n","56/56 [==============================] - 15s 277ms/step - loss: 0.2095 - accuracy: 0.9902 - val_loss: 0.2073 - val_accuracy: 0.9885 - lr: 1.0000e-04\n","Epoch 98/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2072 - accuracy: 0.9904\n","Epoch 98: val_loss did not improve from 0.20720\n","56/56 [==============================] - 15s 274ms/step - loss: 0.2072 - accuracy: 0.9904 - val_loss: 0.2278 - val_accuracy: 0.9724 - lr: 1.0000e-04\n","Epoch 99/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9871\n","Epoch 99: val_loss improved from 0.20720 to 0.20675, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2104 - accuracy: 0.9871 - val_loss: 0.2067 - val_accuracy: 0.9876 - lr: 1.0000e-04\n","Epoch 100/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2070 - accuracy: 0.9908\n","Epoch 100: val_loss did not improve from 0.20675\n","56/56 [==============================] - 15s 275ms/step - loss: 0.2070 - accuracy: 0.9908 - val_loss: 0.2083 - val_accuracy: 0.9877 - lr: 1.0000e-04\n","Epoch 101/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2033 - accuracy: 0.9919\n","Epoch 101: val_loss improved from 0.20675 to 0.20357, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2033 - accuracy: 0.9919 - val_loss: 0.2036 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 102/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2013 - accuracy: 0.9924\n","Epoch 102: val_loss improved from 0.20357 to 0.20228, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.2013 - accuracy: 0.9924 - val_loss: 0.2023 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 103/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2001 - accuracy: 0.9923\n","Epoch 103: val_loss improved from 0.20228 to 0.19993, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.2001 - accuracy: 0.9923 - val_loss: 0.1999 - val_accuracy: 0.9910 - lr: 1.0000e-04\n","Epoch 104/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9924\n","Epoch 104: val_loss improved from 0.19993 to 0.19911, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.2005 - accuracy: 0.9924 - val_loss: 0.1991 - val_accuracy: 0.9900 - lr: 1.0000e-04\n","Epoch 105/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.9926\n","Epoch 105: val_loss improved from 0.19911 to 0.19763, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.2009 - accuracy: 0.9926 - val_loss: 0.1976 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 106/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9923\n","Epoch 106: val_loss improved from 0.19763 to 0.19416, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.2003 - accuracy: 0.9923 - val_loss: 0.1942 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 107/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1954 - accuracy: 0.9930\n","Epoch 107: val_loss did not improve from 0.19416\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1954 - accuracy: 0.9930 - val_loss: 0.1946 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 108/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.9932\n","Epoch 108: val_loss improved from 0.19416 to 0.19320, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1949 - accuracy: 0.9932 - val_loss: 0.1932 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 109/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9919\n","Epoch 109: val_loss did not improve from 0.19320\n","56/56 [==============================] - 15s 273ms/step - loss: 0.1961 - accuracy: 0.9919 - val_loss: 0.1946 - val_accuracy: 0.9901 - lr: 1.0000e-04\n","Epoch 110/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9929\n","Epoch 110: val_loss did not improve from 0.19320\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1934 - accuracy: 0.9929 - val_loss: 0.1955 - val_accuracy: 0.9898 - lr: 1.0000e-04\n","Epoch 111/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9933\n","Epoch 111: val_loss improved from 0.19320 to 0.19056, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1918 - accuracy: 0.9933 - val_loss: 0.1906 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 112/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9935\n","Epoch 112: val_loss improved from 0.19056 to 0.19049, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1898 - accuracy: 0.9935 - val_loss: 0.1905 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 113/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1881 - accuracy: 0.9935\n","Epoch 113: val_loss improved from 0.19049 to 0.18869, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1881 - accuracy: 0.9935 - val_loss: 0.1887 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 114/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1866 - accuracy: 0.9936\n","Epoch 114: val_loss improved from 0.18869 to 0.18737, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1866 - accuracy: 0.9936 - val_loss: 0.1874 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 115/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9938\n","Epoch 115: val_loss improved from 0.18737 to 0.18569, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1852 - accuracy: 0.9938 - val_loss: 0.1857 - val_accuracy: 0.9922 - lr: 1.0000e-04\n","Epoch 116/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1867 - accuracy: 0.9932\n","Epoch 116: val_loss improved from 0.18569 to 0.18491, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1867 - accuracy: 0.9932 - val_loss: 0.1849 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 117/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9931\n","Epoch 117: val_loss improved from 0.18491 to 0.18288, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1852 - accuracy: 0.9931 - val_loss: 0.1829 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 118/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9926\n","Epoch 118: val_loss did not improve from 0.18288\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1862 - accuracy: 0.9926 - val_loss: 0.1852 - val_accuracy: 0.9897 - lr: 1.0000e-04\n","Epoch 119/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1822 - accuracy: 0.9931\n","Epoch 119: val_loss did not improve from 0.18288\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1822 - accuracy: 0.9931 - val_loss: 0.1842 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 120/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1831 - accuracy: 0.9931\n","Epoch 120: val_loss did not improve from 0.18288\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1831 - accuracy: 0.9931 - val_loss: 0.1847 - val_accuracy: 0.9904 - lr: 1.0000e-04\n","Epoch 121/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1822 - accuracy: 0.9934\n","Epoch 121: val_loss improved from 0.18288 to 0.18199, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1822 - accuracy: 0.9934 - val_loss: 0.1820 - val_accuracy: 0.9907 - lr: 1.0000e-04\n","Epoch 122/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9937\n","Epoch 122: val_loss improved from 0.18199 to 0.18008, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1790 - accuracy: 0.9937 - val_loss: 0.1801 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 123/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9940\n","Epoch 123: val_loss did not improve from 0.18008\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1810 - accuracy: 0.9940 - val_loss: 0.1805 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 124/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1783 - accuracy: 0.9939\n","Epoch 124: val_loss improved from 0.18008 to 0.17860, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1783 - accuracy: 0.9939 - val_loss: 0.1786 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 125/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1834 - accuracy: 0.9899\n","Epoch 125: val_loss did not improve from 0.17860\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1834 - accuracy: 0.9899 - val_loss: 0.1881 - val_accuracy: 0.9792 - lr: 1.0000e-04\n","Epoch 126/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.9876\n","Epoch 126: val_loss improved from 0.17860 to 0.17362, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1851 - accuracy: 0.9876 - val_loss: 0.1736 - val_accuracy: 0.9876 - lr: 1.0000e-04\n","Epoch 127/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1791 - accuracy: 0.9903\n","Epoch 127: val_loss did not improve from 0.17362\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1791 - accuracy: 0.9903 - val_loss: 0.1761 - val_accuracy: 0.9890 - lr: 1.0000e-04\n","Epoch 128/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1780 - accuracy: 0.9922\n","Epoch 128: val_loss did not improve from 0.17362\n","56/56 [==============================] - 16s 277ms/step - loss: 0.1780 - accuracy: 0.9922 - val_loss: 0.1751 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 129/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.9934\n","Epoch 129: val_loss did not improve from 0.17362\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1754 - accuracy: 0.9934 - val_loss: 0.1781 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 130/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.9936\n","Epoch 130: val_loss did not improve from 0.17362\n","56/56 [==============================] - 15s 275ms/step - loss: 0.1716 - accuracy: 0.9936 - val_loss: 0.1766 - val_accuracy: 0.9916 - lr: 1.0000e-04\n","Epoch 131/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9938\n","Epoch 131: val_loss improved from 0.17362 to 0.17050, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1691 - accuracy: 0.9938 - val_loss: 0.1705 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 132/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9941\n","Epoch 132: val_loss improved from 0.17050 to 0.16947, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1679 - accuracy: 0.9941 - val_loss: 0.1695 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 133/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.9942\n","Epoch 133: val_loss improved from 0.16947 to 0.16807, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1699 - accuracy: 0.9942 - val_loss: 0.1681 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 134/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1671 - accuracy: 0.9942\n","Epoch 134: val_loss improved from 0.16807 to 0.16680, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1671 - accuracy: 0.9942 - val_loss: 0.1668 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 135/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1662 - accuracy: 0.9940\n","Epoch 135: val_loss improved from 0.16680 to 0.16541, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1662 - accuracy: 0.9940 - val_loss: 0.1654 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 136/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1666 - accuracy: 0.9940\n","Epoch 136: val_loss improved from 0.16541 to 0.16499, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1666 - accuracy: 0.9940 - val_loss: 0.1650 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 137/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9938\n","Epoch 137: val_loss improved from 0.16499 to 0.16262, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1683 - accuracy: 0.9938 - val_loss: 0.1626 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 138/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1637 - accuracy: 0.9933\n","Epoch 138: val_loss did not improve from 0.16262\n","56/56 [==============================] - 16s 277ms/step - loss: 0.1637 - accuracy: 0.9933 - val_loss: 0.1637 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 139/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9940\n","Epoch 139: val_loss did not improve from 0.16262\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1657 - accuracy: 0.9940 - val_loss: 0.1652 - val_accuracy: 0.9923 - lr: 1.0000e-04\n","Epoch 140/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9939\n","Epoch 140: val_loss did not improve from 0.16262\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1621 - accuracy: 0.9939 - val_loss: 0.1632 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 141/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9945\n","Epoch 141: val_loss improved from 0.16262 to 0.16078, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1615 - accuracy: 0.9945 - val_loss: 0.1608 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 142/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9947\n","Epoch 142: val_loss improved from 0.16078 to 0.16033, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1610 - accuracy: 0.9947 - val_loss: 0.1603 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 143/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9945\n","Epoch 143: val_loss did not improve from 0.16033\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1616 - accuracy: 0.9945 - val_loss: 0.1607 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 144/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1574 - accuracy: 0.9949\n","Epoch 144: val_loss improved from 0.16033 to 0.15736, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1574 - accuracy: 0.9949 - val_loss: 0.1574 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 145/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9946\n","Epoch 145: val_loss did not improve from 0.15736\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1590 - accuracy: 0.9946 - val_loss: 0.1581 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 146/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.9949\n","Epoch 146: val_loss improved from 0.15736 to 0.15599, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1565 - accuracy: 0.9949 - val_loss: 0.1560 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 147/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9947\n","Epoch 147: val_loss did not improve from 0.15599\n","56/56 [==============================] - 15s 273ms/step - loss: 0.1628 - accuracy: 0.9947 - val_loss: 0.1562 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 148/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1560 - accuracy: 0.9948\n","Epoch 148: val_loss improved from 0.15599 to 0.15349, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1560 - accuracy: 0.9948 - val_loss: 0.1535 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 149/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9945\n","Epoch 149: val_loss did not improve from 0.15349\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1579 - accuracy: 0.9945 - val_loss: 0.1542 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 150/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1583 - accuracy: 0.9923\n","Epoch 150: val_loss did not improve from 0.15349\n","56/56 [==============================] - 16s 277ms/step - loss: 0.1583 - accuracy: 0.9923 - val_loss: 0.1558 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 151/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1533 - accuracy: 0.9940\n","Epoch 151: val_loss did not improve from 0.15349\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1533 - accuracy: 0.9940 - val_loss: 0.1553 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 152/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9947\n","Epoch 152: val_loss improved from 0.15349 to 0.15340, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1508 - accuracy: 0.9947 - val_loss: 0.1534 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 153/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1507 - accuracy: 0.9947\n","Epoch 153: val_loss improved from 0.15340 to 0.15249, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1507 - accuracy: 0.9947 - val_loss: 0.1525 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 154/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.9944\n","Epoch 154: val_loss improved from 0.15249 to 0.14952, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1517 - accuracy: 0.9944 - val_loss: 0.1495 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 155/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9948\n","Epoch 155: val_loss improved from 0.14952 to 0.14867, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1483 - accuracy: 0.9948 - val_loss: 0.1487 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 156/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9950\n","Epoch 156: val_loss improved from 0.14867 to 0.14828, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1483 - accuracy: 0.9950 - val_loss: 0.1483 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 157/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.9952\n","Epoch 157: val_loss improved from 0.14828 to 0.14795, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1467 - accuracy: 0.9952 - val_loss: 0.1480 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 158/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9951\n","Epoch 158: val_loss improved from 0.14795 to 0.14639, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1483 - accuracy: 0.9951 - val_loss: 0.1464 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 159/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 0.9930\n","Epoch 159: val_loss did not improve from 0.14639\n","56/56 [==============================] - 15s 273ms/step - loss: 0.1497 - accuracy: 0.9930 - val_loss: 0.1579 - val_accuracy: 0.9826 - lr: 1.0000e-04\n","Epoch 160/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9924\n","Epoch 160: val_loss did not improve from 0.14639\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1515 - accuracy: 0.9924 - val_loss: 0.1474 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 161/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1449 - accuracy: 0.9942\n","Epoch 161: val_loss improved from 0.14639 to 0.14536, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1449 - accuracy: 0.9942 - val_loss: 0.1454 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 162/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9930\n","Epoch 162: val_loss improved from 0.14536 to 0.14041, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1483 - accuracy: 0.9930 - val_loss: 0.1404 - val_accuracy: 0.9917 - lr: 1.0000e-04\n","Epoch 163/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1442 - accuracy: 0.9941\n","Epoch 163: val_loss did not improve from 0.14041\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1442 - accuracy: 0.9941 - val_loss: 0.1448 - val_accuracy: 0.9914 - lr: 1.0000e-04\n","Epoch 164/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.9932\n","Epoch 164: val_loss did not improve from 0.14041\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1473 - accuracy: 0.9932 - val_loss: 0.1438 - val_accuracy: 0.9918 - lr: 1.0000e-04\n","Epoch 165/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9909\n","Epoch 165: val_loss did not improve from 0.14041\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1480 - accuracy: 0.9909 - val_loss: 0.1433 - val_accuracy: 0.9884 - lr: 1.0000e-04\n","Epoch 166/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.9897\n","Epoch 166: val_loss did not improve from 0.14041\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1500 - accuracy: 0.9897 - val_loss: 0.1458 - val_accuracy: 0.9905 - lr: 1.0000e-04\n","Epoch 167/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.9935\n","Epoch 167: val_loss did not improve from 0.14041\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1419 - accuracy: 0.9935 - val_loss: 0.1440 - val_accuracy: 0.9899 - lr: 1.0000e-04\n","Epoch 168/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1429 - accuracy: 0.9938\n","Epoch 168: val_loss did not improve from 0.14041\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1429 - accuracy: 0.9938 - val_loss: 0.1429 - val_accuracy: 0.9912 - lr: 1.0000e-04\n","Epoch 169/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9943\n","Epoch 169: val_loss did not improve from 0.14041\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1402 - accuracy: 0.9943 - val_loss: 0.1418 - val_accuracy: 0.9913 - lr: 1.0000e-04\n","Epoch 170/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1408 - accuracy: 0.9943\n","Epoch 170: val_loss improved from 0.14041 to 0.13940, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 285ms/step - loss: 0.1408 - accuracy: 0.9943 - val_loss: 0.1394 - val_accuracy: 0.9921 - lr: 1.0000e-04\n","Epoch 171/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9948\n","Epoch 171: val_loss improved from 0.13940 to 0.13838, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1367 - accuracy: 0.9948 - val_loss: 0.1384 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 172/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1404 - accuracy: 0.9950\n","Epoch 172: val_loss improved from 0.13838 to 0.13792, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 290ms/step - loss: 0.1404 - accuracy: 0.9950 - val_loss: 0.1379 - val_accuracy: 0.9927 - lr: 1.0000e-04\n","Epoch 173/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9950\n","Epoch 173: val_loss improved from 0.13792 to 0.13583, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1354 - accuracy: 0.9950 - val_loss: 0.1358 - val_accuracy: 0.9928 - lr: 1.0000e-04\n","Epoch 174/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.9952\n","Epoch 174: val_loss improved from 0.13583 to 0.13542, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1349 - accuracy: 0.9952 - val_loss: 0.1354 - val_accuracy: 0.9932 - lr: 1.0000e-04\n","Epoch 175/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9953\n","Epoch 175: val_loss improved from 0.13542 to 0.13385, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1335 - accuracy: 0.9953 - val_loss: 0.1338 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 176/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1338 - accuracy: 0.9954\n","Epoch 176: val_loss improved from 0.13385 to 0.13301, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1338 - accuracy: 0.9954 - val_loss: 0.1330 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 177/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9954\n","Epoch 177: val_loss improved from 0.13301 to 0.13177, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1315 - accuracy: 0.9954 - val_loss: 0.1318 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 178/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9955\n","Epoch 178: val_loss did not improve from 0.13177\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1316 - accuracy: 0.9955 - val_loss: 0.1318 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 179/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9955\n","Epoch 179: val_loss improved from 0.13177 to 0.13021, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1330 - accuracy: 0.9955 - val_loss: 0.1302 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 180/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.9955\n","Epoch 180: val_loss did not improve from 0.13021\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1308 - accuracy: 0.9955 - val_loss: 0.1303 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 181/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.9955\n","Epoch 181: val_loss improved from 0.13021 to 0.12859, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1295 - accuracy: 0.9955 - val_loss: 0.1286 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 182/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9957\n","Epoch 182: val_loss improved from 0.12859 to 0.12793, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 289ms/step - loss: 0.1293 - accuracy: 0.9957 - val_loss: 0.1279 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 183/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9956\n","Epoch 183: val_loss improved from 0.12793 to 0.12763, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1293 - accuracy: 0.9956 - val_loss: 0.1276 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 184/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.9956\n","Epoch 184: val_loss improved from 0.12763 to 0.12706, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1273 - accuracy: 0.9956 - val_loss: 0.1271 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 185/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9956\n","Epoch 185: val_loss improved from 0.12706 to 0.12688, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1263 - accuracy: 0.9956 - val_loss: 0.1269 - val_accuracy: 0.9931 - lr: 1.0000e-04\n","Epoch 186/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9956\n","Epoch 186: val_loss improved from 0.12688 to 0.12493, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1258 - accuracy: 0.9956 - val_loss: 0.1249 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 187/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9957\n","Epoch 187: val_loss improved from 0.12493 to 0.12468, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1260 - accuracy: 0.9957 - val_loss: 0.1247 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 188/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1245 - accuracy: 0.9957\n","Epoch 188: val_loss improved from 0.12468 to 0.12445, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1245 - accuracy: 0.9957 - val_loss: 0.1245 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 189/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9957\n","Epoch 189: val_loss improved from 0.12445 to 0.12303, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1252 - accuracy: 0.9957 - val_loss: 0.1230 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 190/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1222 - accuracy: 0.9959\n","Epoch 190: val_loss did not improve from 0.12303\n","56/56 [==============================] - 16s 277ms/step - loss: 0.1222 - accuracy: 0.9959 - val_loss: 0.1235 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 191/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9959\n","Epoch 191: val_loss improved from 0.12303 to 0.12197, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.1253 - accuracy: 0.9959 - val_loss: 0.1220 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 192/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9959\n","Epoch 192: val_loss improved from 0.12197 to 0.12178, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1235 - accuracy: 0.9959 - val_loss: 0.1218 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 193/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1226 - accuracy: 0.9959\n","Epoch 193: val_loss improved from 0.12178 to 0.12130, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1226 - accuracy: 0.9959 - val_loss: 0.1213 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 194/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.9960\n","Epoch 194: val_loss improved from 0.12130 to 0.12059, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1214 - accuracy: 0.9960 - val_loss: 0.1206 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 195/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1199 - accuracy: 0.9959\n","Epoch 195: val_loss improved from 0.12059 to 0.12014, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1199 - accuracy: 0.9959 - val_loss: 0.1201 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 196/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9959\n","Epoch 196: val_loss improved from 0.12014 to 0.11947, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 290ms/step - loss: 0.1195 - accuracy: 0.9959 - val_loss: 0.1195 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 197/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1192 - accuracy: 0.9960\n","Epoch 197: val_loss improved from 0.11947 to 0.11861, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1192 - accuracy: 0.9960 - val_loss: 0.1186 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 198/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9960\n","Epoch 198: val_loss improved from 0.11861 to 0.11757, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1180 - accuracy: 0.9960 - val_loss: 0.1176 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 199/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9960\n","Epoch 199: val_loss improved from 0.11757 to 0.11663, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1164 - accuracy: 0.9960 - val_loss: 0.1166 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 200/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9959\n","Epoch 200: val_loss improved from 0.11663 to 0.11571, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1202 - accuracy: 0.9959 - val_loss: 0.1157 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 201/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.9960\n","Epoch 201: val_loss improved from 0.11571 to 0.11487, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1143 - accuracy: 0.9960 - val_loss: 0.1149 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 202/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9961\n","Epoch 202: val_loss improved from 0.11487 to 0.11474, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1156 - accuracy: 0.9961 - val_loss: 0.1147 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 203/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1171 - accuracy: 0.9962\n","Epoch 203: val_loss did not improve from 0.11474\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1171 - accuracy: 0.9962 - val_loss: 0.1159 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 204/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1139 - accuracy: 0.9963\n","Epoch 204: val_loss improved from 0.11474 to 0.11434, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1139 - accuracy: 0.9963 - val_loss: 0.1143 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 205/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 0.9962\n","Epoch 205: val_loss improved from 0.11434 to 0.11383, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1149 - accuracy: 0.9962 - val_loss: 0.1138 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 206/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9962\n","Epoch 206: val_loss improved from 0.11383 to 0.11297, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1134 - accuracy: 0.9962 - val_loss: 0.1130 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 207/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9962\n","Epoch 207: val_loss improved from 0.11297 to 0.11168, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.1147 - accuracy: 0.9962 - val_loss: 0.1117 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 208/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.9962\n","Epoch 208: val_loss improved from 0.11168 to 0.11003, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1143 - accuracy: 0.9962 - val_loss: 0.1100 - val_accuracy: 0.9936 - lr: 1.0000e-04\n","Epoch 209/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9962\n","Epoch 209: val_loss did not improve from 0.11003\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1121 - accuracy: 0.9962 - val_loss: 0.1108 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 210/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9958\n","Epoch 210: val_loss did not improve from 0.11003\n","56/56 [==============================] - 16s 277ms/step - loss: 0.1126 - accuracy: 0.9958 - val_loss: 0.1104 - val_accuracy: 0.9930 - lr: 1.0000e-04\n","Epoch 211/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9958\n","Epoch 211: val_loss did not improve from 0.11003\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1119 - accuracy: 0.9958 - val_loss: 0.1183 - val_accuracy: 0.9888 - lr: 1.0000e-04\n","Epoch 212/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9958\n","Epoch 212: val_loss did not improve from 0.11003\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1096 - accuracy: 0.9958 - val_loss: 0.1104 - val_accuracy: 0.9933 - lr: 1.0000e-04\n","Epoch 213/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9962\n","Epoch 213: val_loss improved from 0.11003 to 0.10978, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1096 - accuracy: 0.9962 - val_loss: 0.1098 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 214/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1072 - accuracy: 0.9963\n","Epoch 214: val_loss improved from 0.10978 to 0.10844, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1072 - accuracy: 0.9963 - val_loss: 0.1084 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 215/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9964\n","Epoch 215: val_loss improved from 0.10844 to 0.10791, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1093 - accuracy: 0.9964 - val_loss: 0.1079 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 216/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9964\n","Epoch 216: val_loss improved from 0.10791 to 0.10724, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1076 - accuracy: 0.9964 - val_loss: 0.1072 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 217/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9963\n","Epoch 217: val_loss did not improve from 0.10724\n","56/56 [==============================] - 16s 277ms/step - loss: 0.1073 - accuracy: 0.9963 - val_loss: 0.1079 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 218/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9964\n","Epoch 218: val_loss improved from 0.10724 to 0.10520, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1057 - accuracy: 0.9964 - val_loss: 0.1052 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 219/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9964\n","Epoch 219: val_loss improved from 0.10520 to 0.10432, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1039 - accuracy: 0.9964 - val_loss: 0.1043 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 220/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9962\n","Epoch 220: val_loss improved from 0.10432 to 0.10374, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.1075 - accuracy: 0.9962 - val_loss: 0.1037 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 221/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9965\n","Epoch 221: val_loss did not improve from 0.10374\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1030 - accuracy: 0.9965 - val_loss: 0.1059 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 222/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9966\n","Epoch 222: val_loss did not improve from 0.10374\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1050 - accuracy: 0.9966 - val_loss: 0.1039 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 223/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9964\n","Epoch 223: val_loss improved from 0.10374 to 0.10274, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1047 - accuracy: 0.9964 - val_loss: 0.1027 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 224/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9930\n","Epoch 224: val_loss did not improve from 0.10274\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1099 - accuracy: 0.9930 - val_loss: 0.1546 - val_accuracy: 0.9636 - lr: 1.0000e-04\n","Epoch 225/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9853\n","Epoch 225: val_loss did not improve from 0.10274\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1242 - accuracy: 0.9853 - val_loss: 0.1449 - val_accuracy: 0.9675 - lr: 1.0000e-04\n","Epoch 226/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.9851\n","Epoch 226: val_loss did not improve from 0.10274\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1217 - accuracy: 0.9851 - val_loss: 0.1064 - val_accuracy: 0.9867 - lr: 1.0000e-04\n","Epoch 227/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9874\n","Epoch 227: val_loss did not improve from 0.10274\n","56/56 [==============================] - 16s 277ms/step - loss: 0.1172 - accuracy: 0.9874 - val_loss: 0.1112 - val_accuracy: 0.9865 - lr: 1.0000e-04\n","Epoch 228/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9930\n","Epoch 228: val_loss did not improve from 0.10274\n","56/56 [==============================] - 16s 278ms/step - loss: 0.1062 - accuracy: 0.9930 - val_loss: 0.1086 - val_accuracy: 0.9908 - lr: 1.0000e-04\n","Epoch 229/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9940\n","Epoch 229: val_loss did not improve from 0.10274\n","56/56 [==============================] - 15s 277ms/step - loss: 0.1037 - accuracy: 0.9940 - val_loss: 0.1086 - val_accuracy: 0.9924 - lr: 1.0000e-04\n","Epoch 230/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9950\n","Epoch 230: val_loss did not improve from 0.10274\n","56/56 [==============================] - 15s 274ms/step - loss: 0.1021 - accuracy: 0.9950 - val_loss: 0.1065 - val_accuracy: 0.9935 - lr: 1.0000e-04\n","Epoch 231/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9954\n","Epoch 231: val_loss did not improve from 0.10274\n","56/56 [==============================] - 16s 277ms/step - loss: 0.1013 - accuracy: 0.9954 - val_loss: 0.1032 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 232/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9957\n","Epoch 232: val_loss improved from 0.10274 to 0.10261, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.1012 - accuracy: 0.9957 - val_loss: 0.1026 - val_accuracy: 0.9938 - lr: 1.0000e-04\n","Epoch 233/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9959\n","Epoch 233: val_loss improved from 0.10261 to 0.10068, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0997 - accuracy: 0.9959 - val_loss: 0.1007 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 234/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9960\n","Epoch 234: val_loss improved from 0.10068 to 0.09896, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0984 - accuracy: 0.9960 - val_loss: 0.0990 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 235/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9961\n","Epoch 235: val_loss improved from 0.09896 to 0.09883, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0984 - accuracy: 0.9961 - val_loss: 0.0988 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 236/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9961\n","Epoch 236: val_loss improved from 0.09883 to 0.09790, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0997 - accuracy: 0.9961 - val_loss: 0.0979 - val_accuracy: 0.9943 - lr: 1.0000e-04\n","Epoch 237/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9963\n","Epoch 237: val_loss improved from 0.09790 to 0.09648, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0960 - accuracy: 0.9963 - val_loss: 0.0965 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 238/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9964\n","Epoch 238: val_loss improved from 0.09648 to 0.09630, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0950 - accuracy: 0.9964 - val_loss: 0.0963 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 239/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.9964\n","Epoch 239: val_loss improved from 0.09630 to 0.09563, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0966 - accuracy: 0.9964 - val_loss: 0.0956 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 240/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9964\n","Epoch 240: val_loss improved from 0.09563 to 0.09499, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0948 - accuracy: 0.9964 - val_loss: 0.0950 - val_accuracy: 0.9946 - lr: 1.0000e-04\n","Epoch 241/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9965\n","Epoch 241: val_loss improved from 0.09499 to 0.09443, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0946 - accuracy: 0.9965 - val_loss: 0.0944 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 242/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9965\n","Epoch 242: val_loss improved from 0.09443 to 0.09374, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0950 - accuracy: 0.9965 - val_loss: 0.0937 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 243/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9966\n","Epoch 243: val_loss improved from 0.09374 to 0.09301, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 282ms/step - loss: 0.0931 - accuracy: 0.9966 - val_loss: 0.0930 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 244/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9966\n","Epoch 244: val_loss improved from 0.09301 to 0.09229, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0930 - accuracy: 0.9966 - val_loss: 0.0923 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 245/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9965\n","Epoch 245: val_loss improved from 0.09229 to 0.09194, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0934 - accuracy: 0.9965 - val_loss: 0.0919 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 246/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9966\n","Epoch 246: val_loss improved from 0.09194 to 0.09072, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0901 - accuracy: 0.9966 - val_loss: 0.0907 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 247/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9966\n","Epoch 247: val_loss did not improve from 0.09072\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0923 - accuracy: 0.9966 - val_loss: 0.0910 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 248/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9967\n","Epoch 248: val_loss improved from 0.09072 to 0.09065, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 287ms/step - loss: 0.0922 - accuracy: 0.9967 - val_loss: 0.0907 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 249/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0932 - accuracy: 0.9967\n","Epoch 249: val_loss improved from 0.09065 to 0.08984, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0932 - accuracy: 0.9967 - val_loss: 0.0898 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 250/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9967\n","Epoch 250: val_loss improved from 0.08984 to 0.08894, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0907 - accuracy: 0.9967 - val_loss: 0.0889 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 251/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9968\n","Epoch 251: val_loss did not improve from 0.08894\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0917 - accuracy: 0.9968 - val_loss: 0.0890 - val_accuracy: 0.9950 - lr: 1.0000e-04\n","Epoch 252/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9966\n","Epoch 252: val_loss improved from 0.08894 to 0.08820, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 284ms/step - loss: 0.0910 - accuracy: 0.9966 - val_loss: 0.0882 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 253/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9966\n","Epoch 253: val_loss improved from 0.08820 to 0.08817, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0916 - accuracy: 0.9966 - val_loss: 0.0882 - val_accuracy: 0.9949 - lr: 1.0000e-04\n","Epoch 254/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9967\n","Epoch 254: val_loss did not improve from 0.08817\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0905 - accuracy: 0.9967 - val_loss: 0.0899 - val_accuracy: 0.9939 - lr: 1.0000e-04\n","Epoch 255/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9954\n","Epoch 255: val_loss did not improve from 0.08817\n","56/56 [==============================] - 15s 273ms/step - loss: 0.0928 - accuracy: 0.9954 - val_loss: 0.1011 - val_accuracy: 0.9889 - lr: 1.0000e-04\n","Epoch 256/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9947\n","Epoch 256: val_loss did not improve from 0.08817\n","56/56 [==============================] - 16s 278ms/step - loss: 0.0920 - accuracy: 0.9947 - val_loss: 0.0913 - val_accuracy: 0.9915 - lr: 1.0000e-04\n","Epoch 257/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9957\n","Epoch 257: val_loss did not improve from 0.08817\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0889 - accuracy: 0.9957 - val_loss: 0.0890 - val_accuracy: 0.9934 - lr: 1.0000e-04\n","Epoch 258/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9961\n","Epoch 258: val_loss did not improve from 0.08817\n","56/56 [==============================] - 16s 278ms/step - loss: 0.0863 - accuracy: 0.9961 - val_loss: 0.1026 - val_accuracy: 0.9860 - lr: 1.0000e-04\n","Epoch 259/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0924 - accuracy: 0.9940\n","Epoch 259: val_loss improved from 0.08817 to 0.08520, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0924 - accuracy: 0.9940 - val_loss: 0.0852 - val_accuracy: 0.9929 - lr: 1.0000e-04\n","Epoch 260/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9958\n","Epoch 260: val_loss did not improve from 0.08520\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0883 - accuracy: 0.9958 - val_loss: 0.0888 - val_accuracy: 0.9937 - lr: 1.0000e-04\n","Epoch 261/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9965\n","Epoch 261: val_loss did not improve from 0.08520\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0854 - accuracy: 0.9965 - val_loss: 0.0886 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 262/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9966\n","Epoch 262: val_loss did not improve from 0.08520\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0874 - accuracy: 0.9966 - val_loss: 0.0867 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 263/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9967\n","Epoch 263: val_loss did not improve from 0.08520\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0855 - accuracy: 0.9967 - val_loss: 0.0853 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 264/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9967\n","Epoch 264: val_loss improved from 0.08520 to 0.08416, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 283ms/step - loss: 0.0898 - accuracy: 0.9967 - val_loss: 0.0842 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 265/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9968\n","Epoch 265: val_loss did not improve from 0.08416\n","56/56 [==============================] - 15s 273ms/step - loss: 0.0847 - accuracy: 0.9968 - val_loss: 0.0843 - val_accuracy: 0.9948 - lr: 1.0000e-04\n","Epoch 266/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9962\n","Epoch 266: val_loss did not improve from 0.08416\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0855 - accuracy: 0.9962 - val_loss: 0.0845 - val_accuracy: 0.9947 - lr: 1.0000e-04\n","Epoch 267/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9933\n","Epoch 267: val_loss improved from 0.08416 to 0.07845, saving model to daunet_hybird_4b_ndwi.hdf5\n","56/56 [==============================] - 16s 286ms/step - loss: 0.0886 - accuracy: 0.9933 - val_loss: 0.0784 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 268/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9944\n","Epoch 268: val_loss did not improve from 0.07845\n","56/56 [==============================] - 16s 278ms/step - loss: 0.0886 - accuracy: 0.9944 - val_loss: 0.0847 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 269/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9961\n","Epoch 269: val_loss did not improve from 0.07845\n","56/56 [==============================] - 16s 277ms/step - loss: 0.0820 - accuracy: 0.9961 - val_loss: 0.0848 - val_accuracy: 0.9941 - lr: 1.0000e-04\n","Epoch 270/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9965\n","Epoch 270: val_loss did not improve from 0.07845\n","56/56 [==============================] - 16s 277ms/step - loss: 0.0850 - accuracy: 0.9965 - val_loss: 0.0855 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 271/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9966\n","Epoch 271: val_loss did not improve from 0.07845\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0813 - accuracy: 0.9966 - val_loss: 0.0844 - val_accuracy: 0.9945 - lr: 1.0000e-04\n","Epoch 272/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9965\n","Epoch 272: val_loss did not improve from 0.07845\n","56/56 [==============================] - 16s 277ms/step - loss: 0.0834 - accuracy: 0.9965 - val_loss: 0.0837 - val_accuracy: 0.9940 - lr: 1.0000e-04\n","Epoch 273/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9966\n","Epoch 273: val_loss did not improve from 0.07845\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0838 - accuracy: 0.9966 - val_loss: 0.0810 - val_accuracy: 0.9944 - lr: 1.0000e-04\n","Epoch 274/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9964\n","Epoch 274: val_loss did not improve from 0.07845\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0830 - accuracy: 0.9964 - val_loss: 0.0848 - val_accuracy: 0.9926 - lr: 1.0000e-04\n","Epoch 275/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9953\n","Epoch 275: val_loss did not improve from 0.07845\n","56/56 [==============================] - 15s 277ms/step - loss: 0.0837 - accuracy: 0.9953 - val_loss: 0.0848 - val_accuracy: 0.9919 - lr: 1.0000e-04\n","Epoch 276/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9960\n","Epoch 276: val_loss did not improve from 0.07845\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0811 - accuracy: 0.9960 - val_loss: 0.0853 - val_accuracy: 0.9925 - lr: 1.0000e-04\n","Epoch 277/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9896\n","Epoch 277: val_loss did not improve from 0.07845\n","\n","Epoch 277: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n","56/56 [==============================] - 16s 277ms/step - loss: 0.0951 - accuracy: 0.9896 - val_loss: 0.0785 - val_accuracy: 0.9909 - lr: 1.0000e-04\n","Epoch 278/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9946\n","Epoch 278: val_loss did not improve from 0.07845\n","56/56 [==============================] - 16s 278ms/step - loss: 0.0844 - accuracy: 0.9946 - val_loss: 0.0800 - val_accuracy: 0.9922 - lr: 1.0000e-05\n","Epoch 279/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9954\n","Epoch 279: val_loss did not improve from 0.07845\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0817 - accuracy: 0.9954 - val_loss: 0.0816 - val_accuracy: 0.9927 - lr: 1.0000e-05\n","Epoch 280/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9957\n","Epoch 280: val_loss did not improve from 0.07845\n","56/56 [==============================] - 16s 277ms/step - loss: 0.0816 - accuracy: 0.9957 - val_loss: 0.0829 - val_accuracy: 0.9928 - lr: 1.0000e-05\n","Epoch 281/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9954\n","Epoch 281: val_loss did not improve from 0.07845\n","56/56 [==============================] - 15s 274ms/step - loss: 0.0845 - accuracy: 0.9954 - val_loss: 0.0834 - val_accuracy: 0.9929 - lr: 1.0000e-05\n","Epoch 282/1000\n","56/56 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9959\n","Epoch 282: val_loss did not improve from 0.07845\n","56/56 [==============================] - 16s 277ms/step - loss: 0.0797 - accuracy: 0.9959 - val_loss: 0.0827 - val_accuracy: 0.9931 - lr: 1.0000e-05\n","Epoch 282: early stopping\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7b6aa1d4e8d0>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Train model\n","from keras.callbacks import ReduceLROnPlateau\n","reduce_lr=ReduceLROnPlateau(monitor='val_loss',\n","                         factor=0.1,\n","                         patience=10,\n","                         verbose=1,\n","                         mode='auto',\n","                         min_delta=0.00003, #5883s\n","                         cooldown=0,\n","                         min_lr=0)\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15,verbose=1,mode='min')\n","save_model= ModelCheckpoint('daunet_hybird_4b_ndwi.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","unet2.fit(newimages_4b_ndwi, masks, validation_data=(newvalimages_4b_ndwi,val_masks), batch_size=16, epochs=1000,verbose=1,shuffle=True,callbacks=[save_model,reduce_lr,early_stop])"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:30:06.771633Z","iopub.status.busy":"2023-04-06T16:30:06.770425Z","iopub.status.idle":"2023-04-06T16:30:06.778114Z","shell.execute_reply":"2023-04-06T16:30:06.776907Z","shell.execute_reply.started":"2023-04-06T16:30:06.771569Z"},"trusted":true},"outputs":[],"source":["np.save('daunet_hybird_4b_ndwi-history.npy',unet2.history.history)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:30:09.684331Z","iopub.status.busy":"2023-04-06T16:30:09.683962Z","iopub.status.idle":"2023-04-06T16:30:09.690914Z","shell.execute_reply":"2023-04-06T16:30:09.689755Z","shell.execute_reply.started":"2023-04-06T16:30:09.684298Z"},"trusted":true},"outputs":[],"source":["model_history = np.load('daunet_hybird_4b_ndwi-history.npy', allow_pickle='TRUE').item()"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:30:12.974619Z","iopub.status.busy":"2023-04-06T16:30:12.973979Z","iopub.status.idle":"2023-04-06T16:31:07.651551Z","shell.execute_reply":"2023-04-06T16:31:07.650491Z","shell.execute_reply.started":"2023-04-06T16:30:12.974576Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<matplotlib.legend.Legend at 0x7b6a6ba953d0>"]},"execution_count":26,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#@title Plot accuracy and loss \n","from matplotlib import pyplot as plt\n","## Accuracy\n","plt.plot(model_history['accuracy'])\n","plt.plot(model_history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')\n","plt.show()\n","\n","## Loss\n","plt.plot(model_history['loss'])\n","plt.plot(model_history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper left')"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:32:47.080596Z","iopub.status.busy":"2023-04-06T16:32:47.080199Z","iopub.status.idle":"2023-04-06T16:32:47.090009Z","shell.execute_reply":"2023-04-06T16:32:47.088740Z","shell.execute_reply.started":"2023-04-06T16:32:47.080560Z"},"trusted":true},"outputs":[],"source":["del newimages_4b_ndwi,newvalimages_4b_ndwi"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:36:32.849242Z","iopub.status.busy":"2023-04-06T16:36:32.848113Z","iopub.status.idle":"2023-04-06T16:36:57.209647Z","shell.execute_reply":"2023-04-06T16:36:57.208606Z","shell.execute_reply.started":"2023-04-06T16:36:32.849195Z"},"trusted":true},"outputs":[],"source":["## Test images\n","test_dir=r'/content/gdrive/MyDrive/mudtest/'\n","test_images_list = os.listdir(r\"{}/images/\".format(test_dir))\n","test_masks_list = []\n","test_images = []\n","test_originalimages = []\n","for n in test_images_list:\n","  test_masks_list.append(n)\n","  a = (np.array(rxr.open_rasterio(r\"{}/images/{}\".format(test_dir,n))))\n","  test_originalimages.append(a)\n","  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n","  test_images.append(a)\n","\n","## Test masks\n","test_masks = []\n","for n in test_masks_list:\n","  a = (np.array(rxr.open_rasterio(r\"{}/labels/{}\".format(test_dir,n))))\n","  test_masks.append(a)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:37:10.387436Z","iopub.status.busy":"2023-04-06T16:37:10.386499Z","iopub.status.idle":"2023-04-06T16:37:10.677040Z","shell.execute_reply":"2023-04-06T16:37:10.675992Z","shell.execute_reply.started":"2023-04-06T16:37:10.387385Z"},"trusted":true},"outputs":[],"source":["for i in range(len(test_images)):\n","  test_images[i] = test_images[i].astype('float32')\n","  test_images[i] = test_images[i].T\n","\n","  test_originalimages[i] = test_originalimages[i].astype('float32')\n","  test_originalimages[i] = test_originalimages[i].T\n","\n","for i in range(len(test_masks)):\n","  test_masks[i] = test_masks[i].reshape(1,256,256,1)\n","  test_masks[i] = test_masks[i].T\n","for i in range(len(test_images)):\n","  test_images[i] = test_images[i].reshape(-1,256,256,10)\n","  test_originalimages[i] = test_originalimages[i].reshape(-1,256,256,10)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:38:18.558754Z","iopub.status.busy":"2023-04-06T16:38:18.558238Z","iopub.status.idle":"2023-04-06T16:38:18.565773Z","shell.execute_reply":"2023-04-06T16:38:18.564627Z","shell.execute_reply.started":"2023-04-06T16:38:18.558717Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(1, 256, 256, 10)"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["test_originalimages[0].shape"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:48:54.055232Z","iopub.status.busy":"2023-04-06T16:48:54.054546Z","iopub.status.idle":"2023-04-06T16:48:54.124601Z","shell.execute_reply":"2023-04-06T16:48:54.123530Z","shell.execute_reply.started":"2023-04-06T16:48:54.055197Z"},"trusted":true},"outputs":[],"source":["test_images_4b_ndwi=[]\n","for i in range(len(test_images)):\n","  test_images_4b_ndwi.append(test_images[i][...,bands])"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:49:00.853272Z","iopub.status.busy":"2023-04-06T16:49:00.852887Z","iopub.status.idle":"2023-04-06T16:49:00.860494Z","shell.execute_reply":"2023-04-06T16:49:00.859189Z","shell.execute_reply.started":"2023-04-06T16:49:00.853238Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(1, 256, 256, 4)"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["test_images_4b_ndwi[0].shape"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:51:59.593639Z","iopub.status.busy":"2023-04-06T16:51:59.592698Z","iopub.status.idle":"2023-04-06T16:51:59.711864Z","shell.execute_reply":"2023-04-06T16:51:59.710801Z","shell.execute_reply.started":"2023-04-06T16:51:59.593587Z"},"trusted":true},"outputs":[],"source":["# calculate ndwi\n","for i in range(len(test_originalimages)):\n","  epsilon=1e-8 #small value to avoid division by zero errors\n","  test_ndwi=(test_originalimages[i][:,:,:,1]-test_originalimages[i][:,:,:,6])/(test_originalimages[i][:,:,:,1]+test_originalimages[i][:,:,:,6]+epsilon)\n","  ndwi=np.expand_dims(test_ndwi,axis=-1)\n","  test_images_4b_ndwi[i]=np.concatenate([test_images_4b_ndwi[i],ndwi],axis=-1)\n"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:52:05.089189Z","iopub.status.busy":"2023-04-06T16:52:05.088467Z","iopub.status.idle":"2023-04-06T16:52:05.095565Z","shell.execute_reply":"2023-04-06T16:52:05.094459Z","shell.execute_reply.started":"2023-04-06T16:52:05.089152Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(1, 256, 256, 5)"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["test_images_4b_ndwi[0].shape"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:52:14.752943Z","iopub.status.busy":"2023-04-06T16:52:14.752331Z","iopub.status.idle":"2023-04-06T16:52:14.759563Z","shell.execute_reply":"2023-04-06T16:52:14.758545Z","shell.execute_reply.started":"2023-04-06T16:52:14.752892Z"},"trusted":true},"outputs":[{"data":{"text/plain":["266"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["len(test_images_4b_ndwi)"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:52:29.722575Z","iopub.status.busy":"2023-04-06T16:52:29.722166Z","iopub.status.idle":"2023-04-06T16:52:29.731052Z","shell.execute_reply":"2023-04-06T16:52:29.729898Z","shell.execute_reply.started":"2023-04-06T16:52:29.722541Z"},"trusted":true},"outputs":[],"source":["#@title Returns an image or array plot of mask prediction\n","\n","def reconstruct_image(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","  # Standardise between 0-1\n","  reconstruction = reconstruction/np.max(reconstruction)\n","\n","  # Round to 0-1, binary pixel-by-pixel classification \n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  # Plot reconstructed mask (prediction)\n","  plt.imshow(reconstruction) \n","'''\n","  Returns array of mask prediction, given model and image\n","'''\n","def reconstruct_array(model, image, rounded=False):\n","\n","  # Find model prediction\n","  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n","\n","  if rounded:\n","    reconstruction = np.round(reconstruction)\n","\n","  return reconstruction # Returns array"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:52:36.151051Z","iopub.status.busy":"2023-04-06T16:52:36.150682Z","iopub.status.idle":"2023-04-06T16:52:36.169767Z","shell.execute_reply":"2023-04-06T16:52:36.168017Z","shell.execute_reply.started":"2023-04-06T16:52:36.151018Z"},"trusted":true},"outputs":[],"source":["#@title Metric functions for evaluation\n","def accuracy_eval(model, image, mask): # Gives score of mask vs prediction\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","    return accuracy_score(mask.flatten(), reconstruction)\n","\n","  else: # If a list of images input, find accuracy for each\n","    accuracy = []\n","    for i in range(len(image)):\n","#         reconstruction = model.predict(image[i].reshape(1, 256, 256, 10))\n","      reconstruction = model.predict(image[i].reshape(1, 256, 256, 5))\n","      reconstruction = np.round(reconstruction).flatten()\n","      accuracy.append(accuracy_score(mask[i].flatten(), reconstruction))\n","    return accuracy\n","\n","def recall_eval(model, image, mask): # Find recall score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return recall_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    recall = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        recall.append(recall_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return recall\n","\n","def precision_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return precision_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    precision = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        precision.append(precision_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return precision\n","\n","def iou_eval(model, image, mask): # Find precision score\n","  if type(image) != list:   \n","    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n","    reconstruction = np.round(reconstruction).flatten()\n","\n","    return jaccard_score(mask.flatten(), reconstruction, average='binary')\n","\n","  else: # If a list of images input, find accuracy for each\n","    iou = []\n","    for i in range(len(image)):\n","        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n","        reconstruction = np.round(reconstruction).flatten()\n","\n","        iou.append(jaccard_score(mask[i].flatten(), reconstruction, average='binary'))\n","\n","    return iou\n","\n","def f1_score_eval_basic(precision, recall):\n","    prec = np.mean(precision)\n","    rec = np.mean(recall)\n","\n","    if prec + rec == 0:\n","        return 0\n","\n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def produce_mask(image): # Outputs rounded image (binary)\n","  return np.round(image)\n","\n"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:52:55.697603Z","iopub.status.busy":"2023-04-06T16:52:55.696677Z","iopub.status.idle":"2023-04-06T16:54:27.153492Z","shell.execute_reply":"2023-04-06T16:54:27.152497Z","shell.execute_reply.started":"2023-04-06T16:52:55.697550Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 50ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 53ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 50ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 47ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 50ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 54ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n"]}],"source":["accuracy = (accuracy_eval(unet2, test_images_4b_ndwi, test_masks))\n","precision = (precision_eval(unet2, test_images_4b_ndwi, test_masks))\n","recall = (recall_eval(unet2, test_images_4b_ndwi, test_masks))\n","iou = (iou_eval(unet2, test_images_4b_ndwi, test_masks))"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:54:44.279439Z","iopub.status.busy":"2023-04-06T16:54:44.278436Z","iopub.status.idle":"2023-04-06T16:54:44.285183Z","shell.execute_reply":"2023-04-06T16:54:44.283895Z","shell.execute_reply.started":"2023-04-06T16:54:44.279400Z"},"trusted":true},"outputs":[],"source":["f1_score = (f1_score_eval_basic(precision, recall))"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T16:54:46.729601Z","iopub.status.busy":"2023-04-06T16:54:46.729211Z","iopub.status.idle":"2023-04-06T16:54:46.737747Z","shell.execute_reply":"2023-04-06T16:54:46.736621Z","shell.execute_reply.started":"2023-04-06T16:54:46.729568Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model accuracy:  0.9602413320899906 0.04836885385098491\n","model precision:  0.9025517179728288 0.15545904713583594\n","model recall:  0.9293288045669174 0.13843184974647474\n","model F1-score:  0.9157445573586068\n","model iou:  0.8560687771263764\n"]}],"source":["\n","# Print score eval results for each model\n","print('model accuracy: ', np.mean(accuracy), np.std(accuracy))\n","# Print precision eval results for each model\n","print('model precision: ', np.mean(precision), np.std(precision))\n","# Print recall eval results for each model\n","print('model recall: ', np.mean(recall), np.std(recall))\n","# Print f1-score eval results for each model\n","print('model F1-score: ', np.mean(f1_score))\n","print('model iou: ', np.mean(iou))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
